{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PG2FbKpuqD7O"
      },
      "outputs": [],
      "source": [
        "#pat = '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'\n",
        "#!git clone https://{pat}@github.com/fatma18F/partnet.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuEIVQsJGUbQ",
        "outputId": "a24f375a-7f89-4eee-ac76-df945f7c37cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLGjzwFkGUbV",
        "outputId": "1af38b49-542b-4756-d218-d7990dd7e1f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting path.py\n",
            "  Downloading path.py-12.5.0-py3-none-any.whl (2.3 kB)\n",
            "Collecting path (from path.py)\n",
            "  Downloading path-16.6.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: path, path.py\n",
            "Successfully installed path-16.6.0 path.py-12.5.0\n"
          ]
        }
      ],
      "source": [
        "root_dir = \"/content/gdrive/My Drive\"\n",
        "\n",
        "!pip install path.py;\n",
        "from path import Path\n",
        "import sys\n",
        "sys.path.append(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LB7sxVUtGUbY"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import scipy.spatial.distance\n",
        "import math\n",
        "\n",
        "import random\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "#import utils\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data.dataset import random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAihIQ-8wFtN"
      },
      "source": [
        "## Loading Partnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(fn):\n",
        "    with open(fn, 'r') as fin:\n",
        "        return json.load(fin)\n",
        "\n",
        "def load_data(fn):\n",
        "    with h5py.File(fn, 'r') as fin:\n",
        "        pts = fin['pts'][:]\n",
        "        gt_label = fin['gt_label'][:]\n",
        "        gt_mask = fin['gt_mask'][:]\n",
        "        gt_valid = fin['gt_valid'][:]\n",
        "        #gt_other_mask = fin['gt_other_mask'][:]\n",
        "        return pts, gt_label, gt_mask, gt_valid #gt_other_mask"
      ],
      "metadata": {
        "id": "pY08FGOToqej"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_in_dir='/content/gdrive/MyDrive/Chair-3/train-01.h5'\n",
        "\n",
        "f = h5py.File(data_in_dir, 'r')\n",
        "f.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc7_CupixW4L",
        "outputId": "dbabd3c0-d8d2-4ded-922f-b982a4f59c2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KeysViewHDF5 ['data', 'data_num', 'label_seg']>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VANXkwmrI3OY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7acc29ee-e527-402f-9fa6-5bd0aa0f5752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data from  /content/gdrive/MyDrive/train-04.h5\n",
            "(393, 10000, 3)\n",
            "(393, 10000)\n",
            "(393, 200, 10000)\n",
            "(393, 200)\n"
          ]
        }
      ],
      "source": [
        "category='Chair'\n",
        "level_id=3\n",
        "#data_in_dir = f'{root_dir}/Chair-3/train-04.h5'# % (category, level_id)\n",
        "data_in_dir='/content/gdrive/MyDrive/train-04.h5'\n",
        "cur_h5_fn = os.path.join(data_in_dir)\n",
        "print('Reading data from ', data_in_dir)\n",
        "pts, gt_label, gt_mask, gt_valid = load_data(cur_h5_fn)\n",
        "\n",
        "print(pts.shape)\n",
        "print(gt_label.shape)\n",
        "print(gt_mask.shape)\n",
        "print(gt_valid.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-TZCgrjql_Ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a75b9e8-24e2-4879-c9f0-a094eac8cc68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######### Dataset class created #########\n",
            "Number of images:  393\n",
            "Sample image shape:  (10000, 3)\n",
            "Sample categories shape (10000,)\n",
            "Sample mask shape (200, 10000)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class Data(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, pts,gt_label,gt_mask,gt_valid, transform=None):\n",
        "        \n",
        "        self.pts = pts\n",
        "        self.gt_label = gt_label\n",
        "        self.gt_mask = gt_mask\n",
        "        self.gt_valid=gt_valid\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        return pts.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #if not self.valid:\n",
        "          #  theta = random.random()*360\n",
        "         #   image2 = utils.RandRotation_z()(utils.RandomNoise()(image2))\n",
        "        point_cloud_dims_min = pts[idx].min(axis=0)[:3]\n",
        "        point_cloud_dims_max = pts[idx].max(axis=0)[:3]\n",
        "\n",
        "        ret_dict={}\n",
        "        ret_dict[\"point_clouds\"] =  np.array(pts[idx], dtype=\"float32\")\n",
        "        ret_dict[\"point_cloud_dims_min\"] = point_cloud_dims_min#.astype(np.float32)\n",
        "        ret_dict[\"point_cloud_dims_max\"] = point_cloud_dims_max#.astype(np.float32)\n",
        "        ret_dict[\"category\"] = gt_label[idx].astype(int)\n",
        "        ret_dict[\"masks\"] = gt_mask[idx]\n",
        "        ret_dict[\"valid\"] = np.array(gt_valid[idx])\n",
        "\n",
        "        #return {'image': np.array(pts[idx], dtype=\"float32\"), 'category': gt_label[idx].astype(int) , 'masks':gt_mask[idx], 'valid':np.array(gt_valid[idx])}#, 'other_masks':gt_other_mask}\n",
        "        return ret_dict\n",
        "        \n",
        "dset = Data(pts , gt_label, gt_mask, gt_valid)\n",
        "train_num = int(len(dset) * 0.95)\n",
        "val_num = int(len(dset) *0.05)\n",
        "if int(len(dset)) - train_num -  val_num >0 :\n",
        "    train_num = train_num + 1\n",
        "elif int(len(dset)) - train_num -  val_num < 0:\n",
        "    train_num = train_num -1\n",
        "#train_dataset, val_dataset = random_split(dset, [3000, 118])\n",
        "train_dataset, val_dataset = random_split(dset, [train_num, val_num])\n",
        "val_dataset.valid=True\n",
        "\n",
        "print('######### Dataset class created #########')\n",
        "print('Number of images: ', len(dset))\n",
        "print('Sample image shape: ', dset[0]['point_clouds'].shape)\n",
        "print('Sample categories shape', dset[0]['category'].shape)\n",
        "print('Sample mask shape', dset[0]['masks'].shape, end='\\n\\n')\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=2,drop_last = True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=2,drop_last = True)\n",
        "\n",
        "#dataloader = torch.utils.data.DataLoader(dset, batch_size=4, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KNH5UtbWWok"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "o8D_2sRaKUZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc01cb9-4576-457b-b327-2be8112dbfd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UlCGIGvV88f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed19335d-ab6b-43b0-bf30-7d0654dad0ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "n=np.unique(gt_label.ravel()).size\n",
        "n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper scripts"
      ],
      "metadata": {
        "id": "mYbsfYC35CZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Helpers.py***"
      ],
      "metadata": {
        "id": "RA2LuJZ45llS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "\n",
        "class BatchNormDim1Swap(nn.BatchNorm1d):\n",
        "    \"\"\"\n",
        "    Used for nn.Transformer that uses a HW x N x C rep\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: HW x N x C\n",
        "        permute to N x C x HW\n",
        "        Apply BN on C\n",
        "        permute back\n",
        "        \"\"\"\n",
        "        hw, n, c = x.shape\n",
        "        x = x.permute(1, 2, 0)\n",
        "        x = super(BatchNormDim1Swap, self).forward(x)\n",
        "        # x: n x c x hw -> hw x n x c\n",
        "        x = x.permute(2, 0, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "NORM_DICT = {\n",
        "    \"bn\": BatchNormDim1Swap,\n",
        "    \"bn1d\": nn.BatchNorm1d,\n",
        "    \"id\": nn.Identity,\n",
        "    \"ln\": nn.LayerNorm,\n",
        "}\n",
        "\n",
        "ACTIVATION_DICT = {\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"gelu\": nn.GELU,\n",
        "    \"leakyrelu\": partial(nn.LeakyReLU, negative_slope=0.1),\n",
        "}\n",
        "\n",
        "WEIGHT_INIT_DICT = {\n",
        "    \"xavier_uniform\": nn.init.xavier_uniform_,\n",
        "}\n",
        "\n",
        "\n",
        "class GenericMLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dims,\n",
        "        output_dim,\n",
        "        norm_fn_name=None,\n",
        "        activation=\"relu\",\n",
        "        use_conv=False,\n",
        "        dropout=None,\n",
        "        hidden_use_bias=False,\n",
        "        output_use_bias=True,\n",
        "        output_use_activation=False,\n",
        "        output_use_norm=False,\n",
        "        weight_init_name=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        activation = ACTIVATION_DICT[activation]\n",
        "        norm = None\n",
        "        if norm_fn_name is not None:\n",
        "            norm = NORM_DICT[norm_fn_name]\n",
        "        if norm_fn_name == \"ln\" and use_conv:\n",
        "            norm = lambda x: nn.GroupNorm(1, x)  # easier way to use LayerNorm\n",
        "\n",
        "        if dropout is not None:\n",
        "            if not isinstance(dropout, list):\n",
        "                dropout = [dropout for _ in range(len(hidden_dims))]\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for idx, x in enumerate(hidden_dims):\n",
        "            if use_conv:\n",
        "                layer = nn.Conv1d(prev_dim, x, 1, bias=hidden_use_bias)\n",
        "            else:\n",
        "                layer = nn.Linear(prev_dim, x, bias=hidden_use_bias)\n",
        "            layers.append(layer)\n",
        "            if norm:\n",
        "                layers.append(norm(x))\n",
        "            layers.append(activation())\n",
        "            if dropout is not None:\n",
        "                layers.append(nn.Dropout(p=dropout[idx]))\n",
        "            prev_dim = x\n",
        "        if use_conv:\n",
        "            layer = nn.Conv1d(prev_dim, output_dim, 1, bias=output_use_bias)\n",
        "        else:\n",
        "            layer = nn.Linear(prev_dim, output_dim, bias=output_use_bias)\n",
        "        layers.append(layer)\n",
        "\n",
        "        if output_use_norm:\n",
        "            layers.append(norm(output_dim))\n",
        "\n",
        "        if output_use_activation:\n",
        "            layers.append(activation())\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        if weight_init_name is not None:\n",
        "            self.do_weight_init(weight_init_name)\n",
        "\n",
        "    def do_weight_init(self, weight_init_name):\n",
        "        func = WEIGHT_INIT_DICT[weight_init_name]\n",
        "        for (_, param) in self.named_parameters():\n",
        "            if param.dim() > 1:  # skips batchnorm/layernorm\n",
        "                func(param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layers(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "metadata": {
        "id": "HsudBIC-4-XX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Transformers.py***"
      ],
      "metadata": {
        "id": "7NdJRlTk5sSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Modified from DETR Transformer class.\n",
        "\n",
        "Copy-paste from torch.nn.Transformer with modifications:\n",
        "    * positional encodings are passed in MHattention\n",
        "    * extra LN at the end of encoder is removed\n",
        "    * decoder returns a stack of activations from all decoding layers\n",
        "\"\"\"\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "#from helpers import (ACTIVATION_DICT, NORM_DICT, WEIGHT_INIT_DICT,\n",
        "#                            get_clones)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers,\n",
        "                 norm=None, weight_init_name=\"xavier_uniform\"):\n",
        "        super().__init__()\n",
        "        self.layers = get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self._reset_parameters(weight_init_name)\n",
        "\n",
        "    def _reset_parameters(self, weight_init_name):\n",
        "        func = WEIGHT_INIT_DICT[weight_init_name]\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                func(p)\n",
        "\n",
        "    def forward(self, src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                xyz: Optional [Tensor] = None,\n",
        "                transpose_swap: Optional[bool] = False,\n",
        "                ):\n",
        "        if transpose_swap:\n",
        "            bs, c, h, w = src.shape\n",
        "            src = src.flatten(2).permute(2, 0, 1)\n",
        "            if pos is not None:\n",
        "                pos = pos.flatten(2).permute(2, 0, 1)\n",
        "        output = src\n",
        "        orig_mask = mask\n",
        "        if orig_mask is not None and isinstance(orig_mask, list):\n",
        "            assert len(orig_mask) == len(self.layers)\n",
        "        elif orig_mask is not None:\n",
        "            orig_mask = [mask for _ in range(len(self.layers))]\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if orig_mask is not None:\n",
        "                mask = orig_mask[idx]\n",
        "                # mask must be tiled to num_heads of the transformer\n",
        "                bsz, n, n = mask.shape\n",
        "                nhead = layer.nhead\n",
        "                mask = mask.unsqueeze(1)\n",
        "                mask = mask.repeat(1, nhead, 1, 1)\n",
        "                mask = mask.view(bsz * nhead, n, n)\n",
        "            output = layer(output, src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        if transpose_swap:\n",
        "            output = output.permute(1, 2, 0).view(bs, c, h, w).contiguous()\n",
        "\n",
        "        xyz_inds = None\n",
        "\n",
        "        return xyz, output, xyz_inds\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm_fn_name=\"ln\",\n",
        "                return_intermediate=False,\n",
        "                weight_init_name=\"xavier_uniform\"):\n",
        "        super().__init__()\n",
        "        self.layers = get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = None\n",
        "        if norm_fn_name is not None:\n",
        "            self.norm = NORM_DICT[norm_fn_name](self.layers[0].linear2.out_features)\n",
        "        self.return_intermediate = return_intermediate\n",
        "        self._reset_parameters(weight_init_name)\n",
        "\n",
        "    def _reset_parameters(self, weight_init_name):\n",
        "        func = WEIGHT_INIT_DICT[weight_init_name]\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                func(p)\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None,\n",
        "                transpose_swap: Optional [bool] = False,\n",
        "                return_attn_weights: Optional [bool] = False,\n",
        "                ):\n",
        "        if transpose_swap:\n",
        "            bs, c, h, w = memory.shape\n",
        "            memory = memory.flatten(2).permute(2, 0, 1) # memory: bs, c, t -> t, b, c\n",
        "            if pos is not None:\n",
        "                pos = pos.flatten(2).permute(2, 0, 1)\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "        attns = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output, attn = layer(output, memory, tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos, query_pos=query_pos,\n",
        "                           return_attn_weights=return_attn_weights)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "            if return_attn_weights:\n",
        "                attns.append(attn)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if return_attn_weights:\n",
        "            attns = torch.stack(attns)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate), attns\n",
        "\n",
        "        return output, attns\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead=4, dim_feedforward=128,\n",
        "                 dropout=0.1, dropout_attn=None,\n",
        "                 activation=\"relu\", normalize_before=True, norm_name=\"ln\",\n",
        "                 use_ffn=True,\n",
        "                 ffn_use_bias=True):\n",
        "        super().__init__()\n",
        "        if dropout_attn is None:\n",
        "            dropout_attn = dropout\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout_attn)\n",
        "        self.use_ffn = use_ffn\n",
        "        if self.use_ffn:\n",
        "            # Implementation of Feedforward model\n",
        "            self.linear1 = nn.Linear(d_model, dim_feedforward, bias=ffn_use_bias)\n",
        "            self.dropout = nn.Dropout(dropout, inplace=False)\n",
        "            self.linear2 = nn.Linear(dim_feedforward, d_model, bias=ffn_use_bias)\n",
        "            self.norm2 = NORM_DICT[norm_name](d_model)\n",
        "            self.norm2 = NORM_DICT[norm_name](d_model)\n",
        "            self.dropout2 = nn.Dropout(dropout, inplace=False)\n",
        "\n",
        "        self.norm1 = NORM_DICT[norm_name](d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout, inplace=False)\n",
        "\n",
        "        self.activation = ACTIVATION_DICT[activation]()\n",
        "        self.normalize_before = normalize_before\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        value = src\n",
        "        src2 = self.self_attn(q, k, value=value, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        if self.use_norm_fn_on_input:\n",
        "            src = self.norm1(src)\n",
        "        if self.use_ffn:\n",
        "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "            src = src + self.dropout2(src2)\n",
        "            src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self, src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    return_attn_weights: Optional [Tensor] = False):\n",
        "\n",
        "        src2 = self.norm1(src)\n",
        "        value = src2\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2, attn_weights = self.self_attn(q, k, value=value, attn_mask=src_mask,\n",
        "                            key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        if self.use_ffn:\n",
        "            src2 = self.norm2(src)\n",
        "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "            src = src + self.dropout2(src2)\n",
        "        if return_attn_weights:\n",
        "            return src, attn_weights\n",
        "        return src\n",
        "\n",
        "    def forward(self, src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                return_attn_weights: Optional [Tensor] = False):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos, return_attn_weights)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        st = \"\"\n",
        "        if hasattr(self.self_attn, \"dropout\"):\n",
        "            st += f\"attn_dr={self.self_attn.dropout}\"\n",
        "        return st\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead=4, dim_feedforward=256,\n",
        "                 dropout=0.1, dropout_attn=None,\n",
        "                 activation=\"relu\", normalize_before=True,\n",
        "                 norm_fn_name=\"ln\"):\n",
        "        super().__init__()\n",
        "        if dropout_attn is None:\n",
        "            dropout_attn = dropout\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        self.norm1 = NORM_DICT[norm_fn_name](d_model)\n",
        "        self.norm2 = NORM_DICT[norm_fn_name](d_model)\n",
        "\n",
        "        self.norm3 = NORM_DICT[norm_fn_name](d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout, inplace=False)\n",
        "        self.dropout2 = nn.Dropout(dropout, inplace=False)\n",
        "        self.dropout3 = nn.Dropout(dropout, inplace=False)\n",
        "\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout, inplace=False)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.activation =nn.ReLU()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None,\n",
        "                     return_attn_weights: Optional [bool] = False):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2, attn = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        if return_attn_weights:\n",
        "            return tgt, attn\n",
        "        return tgt, None\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None,\n",
        "                    return_attn_weights: Optional [bool] = False):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2, attn = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        if return_attn_weights:\n",
        "            return tgt, attn\n",
        "        return tgt, None\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None,\n",
        "                return_attn_weights: Optional [bool] = False):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos, return_attn_weights)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos, return_attn_weights)"
      ],
      "metadata": {
        "id": "JVE9iAra5HFZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***positional embedding.py***"
      ],
      "metadata": {
        "id": "1EQtj_Vi5wry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## positional embedding\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Various positional encodings for the transformer.\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "#from utils.pc_util import shift_scale_points\n",
        "\n",
        "class PositionEmbeddingCoordsSine(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=10000,\n",
        "        normalize=False,\n",
        "        scale=None,\n",
        "        pos_type=\"fourier\",\n",
        "        d_pos=None,\n",
        "        d_in=3,\n",
        "        gauss_scale=1.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        assert pos_type in [\"sine\", \"fourier\"]\n",
        "        self.pos_type = pos_type\n",
        "        self.scale = scale\n",
        "        if pos_type == \"fourier\":\n",
        "            assert d_pos is not None\n",
        "            assert d_pos % 2 == 0\n",
        "            # define a gaussian matrix input_ch -> output_ch\n",
        "            B = torch.empty((d_in, d_pos // 2)).normal_()\n",
        "            B *= gauss_scale\n",
        "            self.register_buffer(\"gauss_B\", B)\n",
        "            self.d_pos = d_pos\n",
        "        # ----------------------------------------\n",
        "    # Simple Point manipulations\n",
        "    # ----------------------------------------\n",
        "    def shift_scale_points(self,pred_xyz, src_range, dst_range=None):\n",
        "      \"\"\"\n",
        "      pred_xyz: B x N x 3\n",
        "      src_range: [[B x 3], [B x 3]] - min and max XYZ coords\n",
        "      dst_range: [[B x 3], [B x 3]] - min and max XYZ coords\n",
        "      \"\"\"\n",
        "      if dst_range is None:\n",
        "        dst_range = [\n",
        "            torch.zeros((src_range[0].shape[0], 3), device=src_range[0].device),\n",
        "            torch.ones((src_range[0].shape[0], 3), device=src_range[0].device),\n",
        "        ]\n",
        "\n",
        "      if pred_xyz.ndim == 4:\n",
        "        src_range = [x[:, None] for x in src_range]\n",
        "        dst_range = [x[:, None] for x in dst_range]\n",
        "\n",
        "      assert src_range[0].shape[0] == pred_xyz.shape[0]\n",
        "      assert dst_range[0].shape[0] == pred_xyz.shape[0]\n",
        "      assert src_range[0].shape[-1] == pred_xyz.shape[-1]\n",
        "      assert src_range[0].shape == src_range[1].shape\n",
        "      assert dst_range[0].shape == dst_range[1].shape\n",
        "      assert src_range[0].shape == dst_range[1].shape\n",
        "\n",
        "      src_diff = src_range[1][:, None, :] - src_range[0][:, None, :]\n",
        "      dst_diff = dst_range[1][:, None, :] - dst_range[0][:, None, :]\n",
        "      prop_xyz = (\n",
        "        ((pred_xyz - src_range[0][:, None, :]) * dst_diff) / src_diff\n",
        "      ) + dst_range[0][:, None, :]\n",
        "      return prop_xyz\n",
        "\n",
        "    def get_sine_embeddings(self, xyz, num_channels, input_range):\n",
        "        # clone coords so that shift/scale operations do not affect original tensor\n",
        "        orig_xyz = xyz\n",
        "        xyz = orig_xyz.clone()\n",
        "\n",
        "        ncoords = xyz.shape[1]\n",
        "        if self.normalize:\n",
        "            xyz = self.shift_scale_points(xyz, src_range=input_range)\n",
        "\n",
        "        ndim = num_channels // xyz.shape[2]\n",
        "        if ndim % 2 != 0:\n",
        "            ndim -= 1\n",
        "        # automatically handle remainder by assiging it to the first dim\n",
        "        rems = num_channels - (ndim * xyz.shape[2])\n",
        "\n",
        "        assert (\n",
        "            ndim % 2 == 0\n",
        "        ), f\"Cannot handle odd sized ndim={ndim} where num_channels={num_channels} and xyz={xyz.shape}\"\n",
        "\n",
        "        final_embeds = []\n",
        "        prev_dim = 0\n",
        "\n",
        "        for d in range(xyz.shape[2]):\n",
        "            cdim = ndim\n",
        "            if rems > 0:\n",
        "                # add remainder in increments of two to maintain even size\n",
        "                cdim += 2\n",
        "                rems -= 2\n",
        "\n",
        "            if cdim != prev_dim:\n",
        "                dim_t = torch.arange(cdim, dtype=torch.float32, device=xyz.device)\n",
        "                dim_t = self.temperature ** (2 * (dim_t // 2) / cdim)\n",
        "\n",
        "            # create batch x cdim x nccords embedding\n",
        "            raw_pos = xyz[:, :, d]\n",
        "            if self.scale:\n",
        "                raw_pos *= self.scale\n",
        "            pos = raw_pos[:, :, None] / dim_t\n",
        "            pos = torch.stack(\n",
        "                (pos[:, :, 0::2].sin(), pos[:, :, 1::2].cos()), dim=3\n",
        "            ).flatten(2)\n",
        "            final_embeds.append(pos)\n",
        "            prev_dim = cdim\n",
        "\n",
        "        final_embeds = torch.cat(final_embeds, dim=2).permute(0, 2, 1)\n",
        "        return final_embeds\n",
        "\n",
        "    def get_fourier_embeddings(self, xyz, num_channels=None, input_range=None):\n",
        "        # Follows - https://people.eecs.berkeley.edu/~bmild/fourfeat/index.html\n",
        "\n",
        "        if num_channels is None:\n",
        "            num_channels = self.gauss_B.shape[1] * 2\n",
        "\n",
        "        bsize, npoints = xyz.shape[0], xyz.shape[1]\n",
        "        assert num_channels > 0 and num_channels % 2 == 0\n",
        "        d_in, max_d_out = self.gauss_B.shape[0], self.gauss_B.shape[1]\n",
        "        d_out = num_channels // 2\n",
        "        assert d_out <= max_d_out\n",
        "        assert d_in == xyz.shape[-1]\n",
        "\n",
        "        # clone coords so that shift/scale operations do not affect original tensor\n",
        "        orig_xyz = xyz\n",
        "        xyz = orig_xyz.clone()\n",
        "\n",
        "        ncoords = xyz.shape[1]\n",
        "        if self.normalize:\n",
        "            xyz = self.shift_scale_points(xyz, src_range=input_range)\n",
        "\n",
        "        xyz *= 2 * np.pi\n",
        "        xyz_proj = torch.mm(xyz.view(-1, d_in), self.gauss_B[:, :d_out]).view(\n",
        "            bsize, npoints, d_out\n",
        "        )\n",
        "        final_embeds = [xyz_proj.sin(), xyz_proj.cos()]\n",
        "\n",
        "        # return batch x d_pos x npoints embedding\n",
        "        final_embeds = torch.cat(final_embeds, dim=2).permute(0, 2, 1)\n",
        "        return final_embeds\n",
        "\n",
        "    def forward(self, xyz, num_channels=None, input_range=None):\n",
        "        assert isinstance(xyz, torch.Tensor)\n",
        "        assert xyz.ndim == 3\n",
        "        # xyz is batch x npoints x 3\n",
        "        if self.pos_type == \"sine\":\n",
        "            with torch.no_grad():\n",
        "                return self.get_sine_embeddings(xyz, num_channels, input_range)\n",
        "        elif self.pos_type == \"fourier\":\n",
        "            with torch.no_grad():\n",
        "                return self.get_fourier_embeddings(xyz, num_channels, input_range)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown {self.pos_type}\")\n",
        "\n",
        "    def extra_repr(self):\n",
        "        st = f\"type={self.pos_type}, scale={self.scale}, normalize={self.normalize}\"\n",
        "        if hasattr(self, \"gauss_B\"):\n",
        "            st += (\n",
        "                f\", gaussB={self.gauss_B.shape}, gaussBsum={self.gauss_B.sum().item()}\"\n",
        "            )\n",
        "        return st"
      ],
      "metadata": {
        "id": "F4wbPYSr5YRj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***pointnet2_utils.py***"
      ],
      "metadata": {
        "id": "YOwjLQ3C54iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## pointnet2_utils\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "def timeit(tag, t):\n",
        "    print(\"{}: {}s\".format(tag, time() - t))\n",
        "    return time()\n",
        "\n",
        "def pc_normalize(pc):\n",
        "    l = pc.shape[0]\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc\n",
        "\n",
        "def square_distance(src, dst):\n",
        "    \"\"\"\n",
        "    Calculate Euclid distance between each two points.\n",
        "\n",
        "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
        "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
        "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
        "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
        "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
        "\n",
        "    Input:\n",
        "        src: source points, [B, N, C]\n",
        "        dst: target points, [B, M, C]\n",
        "    Output:\n",
        "        dist: per-point square distance, [B, N, M]\n",
        "    \"\"\"\n",
        "    B, N, _ = src.shape\n",
        "    _, M, _ = dst.shape\n",
        "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
        "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
        "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
        "    return dist\n",
        "\n",
        "\n",
        "def index_points(points, idx):\n",
        "    \"\"\"\n",
        "\n",
        "    Input:\n",
        "        points: input points data, [B, N, C]\n",
        "        idx: sample index data, [B, S]\n",
        "    Return:\n",
        "        new_points:, indexed points data, [B, S, C]\n",
        "    \"\"\"\n",
        "    device = points.device\n",
        "    B = points.shape[0]\n",
        "    view_shape = list(idx.shape)\n",
        "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
        "    repeat_shape = list(idx.shape)\n",
        "    repeat_shape[0] = 1\n",
        "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
        "    new_points = points[batch_indices, idx, :]\n",
        "    return new_points\n",
        "\n",
        "\n",
        "def index_points2(points, idx):\n",
        "    \"\"\"\n",
        "\n",
        "    Input:\n",
        "        points: input points data, [B, N, C]\n",
        "        idx: sample index data, [B, S]\n",
        "    Return:\n",
        "        new_points:, indexed points data, [B, S, C]\n",
        "    \"\"\"\n",
        "    device = points.device\n",
        "    B = points.shape[0]\n",
        "    view_shape = list(idx.shape)\n",
        "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
        "    repeat_shape = list(idx.shape)\n",
        "    repeat_shape[0] = 1\n",
        "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
        "    new_points = points[batch_indices, idx]#, :]\n",
        "    return new_points\n",
        "\n",
        "\n",
        "def farthest_point_sample(xyz, npoint):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        xyz: pointcloud data, [B, N, 3]\n",
        "        npoint: number of samples\n",
        "    Return:\n",
        "        centroids: sampled pointcloud index, [B, npoint]\n",
        "    \"\"\"\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
        "    distance = torch.ones(B, N).to(device) * 1e10\n",
        "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
        "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
        "    for i in range(npoint):\n",
        "        centroids[:, i] = farthest\n",
        "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
        "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
        "        mask = dist < distance\n",
        "        distance[mask] = dist[mask]\n",
        "        farthest = torch.max(distance, -1)[1]\n",
        "    return centroids\n",
        "\n",
        "\n",
        "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        radius: local region radius\n",
        "        nsample: max sample number in local region\n",
        "        xyz: all points, [B, N, 3]\n",
        "        new_xyz: query points, [B, S, 3]\n",
        "    Return:\n",
        "        group_idx: grouped points index, [B, S, nsample]\n",
        "    \"\"\"\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    _, S, _ = new_xyz.shape\n",
        "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
        "    sqrdists = square_distance(new_xyz, xyz)\n",
        "    group_idx[sqrdists > radius ** 2] = N\n",
        "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
        "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
        "    mask = group_idx == N\n",
        "    group_idx[mask] = group_first[mask]\n",
        "    return group_idx\n",
        "\n",
        "\n",
        "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        npoint:\n",
        "        radius:\n",
        "        nsample:\n",
        "        xyz: input points position data, [B, N, 3]\n",
        "        points: input points data, [B, N, D]\n",
        "    Return:\n",
        "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
        "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
        "    \"\"\"\n",
        "    B, N, C = xyz.shape\n",
        "    S = npoint\n",
        "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
        "    new_xyz = index_points(xyz, fps_idx)\n",
        "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
        "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
        "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
        "\n",
        "    if points is not None:\n",
        "        grouped_points = index_points(points, idx)\n",
        "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
        "    else:\n",
        "        new_points = grouped_xyz_norm\n",
        "    if returnfps:\n",
        "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
        "    else:\n",
        "        return new_xyz, new_points,fps_idx\n",
        "\n",
        "\n",
        "def sample_and_group_all(xyz, points):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        xyz: input points position data, [B, N, 3]\n",
        "        points: input points data, [B, N, D]\n",
        "    Return:\n",
        "        new_xyz: sampled points position data, [B, 1, 3]\n",
        "        new_points: sampled points data, [B, 1, N, 3+D]\n",
        "    \"\"\"\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
        "    grouped_xyz = xyz.view(B, 1, N, C)\n",
        "    if points is not None:\n",
        "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
        "    else:\n",
        "        new_points = grouped_xyz\n",
        "    return new_xyz, new_points\n",
        "\n",
        "\n",
        "class PointNetSetAbstraction(nn.Module):\n",
        "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
        "        super(PointNetSetAbstraction, self).__init__()\n",
        "        self.npoint = npoint\n",
        "        self.radius = radius\n",
        "        self.nsample = nsample\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = in_channel\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
        "            last_channel = out_channel\n",
        "        self.group_all = group_all\n",
        "\n",
        "    def forward(self, xyz, points):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            xyz: input points position data, [B, C, N]\n",
        "            points: input points data, [B, D, N]\n",
        "        Return:\n",
        "            new_xyz: sampled points position data, [B, C, S]\n",
        "            new_points_concat: sample points feature data, [B, D', S]\n",
        "        \"\"\"\n",
        "        #xyz = xyz.permute(0, 2, 1)\n",
        "        #if points is not None:\n",
        "        #    points = points.permute(0, 2, 1)\n",
        "        fps_idx=[]\n",
        "        if self.group_all:\n",
        "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
        "        else:\n",
        "            new_xyz, new_points,fps_idx = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
        "        # new_xyz: sampled points position data, [B, npoint, C]\n",
        "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
        "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
        "        for i, conv in enumerate(self.mlp_convs):\n",
        "            bn = self.mlp_bns[i]\n",
        "            new_points =  F.relu(bn(conv(new_points)))\n",
        "\n",
        "        new_points = torch.max(new_points, 2)[0]\n",
        "        new_xyz = new_xyz.permute(0, 2, 1)\n",
        "        return new_xyz, new_points, fps_idx\n",
        "\n",
        "\n",
        "\n",
        "class PointNetFeaturePropagation(nn.Module):\n",
        "    def __init__(self, in_channel, mlp):\n",
        "        super(PointNetFeaturePropagation, self).__init__()\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = in_channel\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1))\n",
        "            self.mlp_bns.append(nn.BatchNorm1d(out_channel))\n",
        "            last_channel = out_channel\n",
        "\n",
        "    def forward(self, xyz1, xyz2, points1, points2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            xyz1: input points position data, [B, C, N]\n",
        "            xyz2: sampled input points position data, [B, C, S]\n",
        "            points1: input points data, [B, D, N]\n",
        "            points2: input points data, [B, D, S]\n",
        "        Return:\n",
        "            new_points: upsampled points data, [B, D', N]\n",
        "        \"\"\"\n",
        "        xyz1 = xyz1.permute(0, 2, 1)\n",
        "        xyz2 = xyz2.permute(0, 2, 1)\n",
        "\n",
        "        points2 = points2.permute(0, 2, 1)\n",
        "        B, N, C = xyz1.shape\n",
        "        _, S, _ = xyz2.shape\n",
        "\n",
        "        if S == 1:\n",
        "            interpolated_points = points2.repeat(1, N, 1)\n",
        "        else:\n",
        "            dists = square_distance(xyz1, xyz2)\n",
        "            dists, idx = dists.sort(dim=-1)\n",
        "            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
        "\n",
        "            dist_recip = 1.0 / (dists + 1e-8)\n",
        "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
        "            weight = dist_recip / norm\n",
        "            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n",
        "\n",
        "        if points1 is not None:\n",
        "            points1 = points1.permute(0, 2, 1)\n",
        "            new_points = torch.cat([points1, interpolated_points], dim=-1)\n",
        "        else:\n",
        "            new_points = interpolated_points\n",
        "\n",
        "        new_points = new_points.permute(0, 2, 1)\n",
        "        for i, conv in enumerate(self.mlp_convs):\n",
        "            bn = self.mlp_bns[i]\n",
        "            new_points = F.relu(bn(conv(new_points)))\n",
        "        return new_points\n"
      ],
      "metadata": {
        "id": "ABh2Svl9sPsm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Encoder + Trasnformer Decoder"
      ],
      "metadata": {
        "id": "9ICo3gTk4xQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformer import (MaskedTransformerEncoder, TransformerDecoder,\n",
        "#                                TransformerDecoderLayer, TransformerEncoder,\n",
        "#                                TransformerEncoderLayer)\n",
        "#from helpers import GenericMLP\n",
        "\n",
        "class Model3DETR(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pre_encoder,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        encoder_dim=256,\n",
        "        decoder_dim=256,\n",
        "        position_embedding=\"fourier\",\n",
        "        mlp_dropout=0.3,\n",
        "        num_queries=200,\n",
        "        num_classes=39,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_queries=num_queries\n",
        "        self.num_classes=num_classes\n",
        "        self.pre_encoder = pre_encoder\n",
        "        self.encoder = encoder\n",
        "        hidden_dims = [encoder_dim, encoder_dim]\n",
        "        self.encoder_to_decoder_projection = GenericMLP(\n",
        "            input_dim=encoder_dim,\n",
        "            hidden_dims=hidden_dims,\n",
        "            output_dim=decoder_dim,\n",
        "            norm_fn_name=\"bn1d\",\n",
        "            activation=\"relu\",\n",
        "            use_conv=True,\n",
        "            output_use_activation=True,\n",
        "            output_use_norm=True,\n",
        "            output_use_bias=False,\n",
        "        )\n",
        "        self.pos_embedding = PositionEmbeddingCoordsSine(\n",
        "            d_pos=decoder_dim, pos_type=position_embedding, normalize=True\n",
        "        )\n",
        "        self.query_projection = GenericMLP(\n",
        "            input_dim=decoder_dim,\n",
        "            hidden_dims=[decoder_dim],\n",
        "            output_dim=decoder_dim,\n",
        "            use_conv=True,\n",
        "            output_use_activation=True,\n",
        "            hidden_use_bias=True,\n",
        "        )\n",
        "        self.decoder = decoder\n",
        "        #self.build_mlp_heads(decoder_dim, mlp_dropout)\n",
        "\n",
        "        self.num_queries = num_queries\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.conv_ = nn.Conv1d(459, 256, 1)\n",
        "        self.conv_0 = nn.Conv1d(256, 128, 1)\n",
        "        self.conv_F = nn.Conv1d(128, 200, 1)\n",
        "        self.lrelu=nn.LeakyReLU(0.2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        self.sa0 = PointNetSetAbstraction(npoint=1024, radius=0.1, nsample=32, in_channel=6, mlp=[16, 16, 32], group_all=False)\n",
        "        self.sa05 = PointNetSetAbstraction(npoint=1024, radius=0.1, nsample=32, in_channel=32+3, mlp=[32, 32, 32], group_all=False)\n",
        "        self.sa1 = PointNetSetAbstraction(npoint=1024, radius=0.1, nsample=32, in_channel=32+3, mlp=[32, 32, 64], group_all=False)\n",
        "        self.sa2 = PointNetSetAbstraction(npoint=256, radius=0.2, nsample=32, in_channel=64 + 3, mlp=[64, 64, 128], group_all=False)\n",
        "        self.sa3 = PointNetSetAbstraction(npoint=64, radius=0.4, nsample=32, in_channel=128 + 3, mlp=[128, 128,256], group_all=False)\n",
        "        self.sa4 = PointNetSetAbstraction(npoint=16, radius=0.8, nsample=32, in_channel=256 + 3, mlp=[256, 256,512], group_all=False)\n",
        "        self.sa5 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=512 + 3, mlp=[512, 512,512], group_all=True)\n",
        "\n",
        "        self.fp4 = PointNetFeaturePropagation(in_channel=768, mlp=[256, 256])\n",
        "        self.fp3 = PointNetFeaturePropagation(in_channel=384, mlp=[256, 256])\n",
        "        self.fp2 = PointNetFeaturePropagation(in_channel=320, mlp=[256, 128])\n",
        "        self.fp1 = PointNetFeaturePropagation(in_channel=160, mlp=[256, 128, 128])\n",
        "        self.fp05 = PointNetFeaturePropagation(in_channel=160, mlp=[256, 128, 64])\n",
        "        self.fp0 = PointNetFeaturePropagation(in_channel=67, mlp=[128, 128, 512])\n",
        "\n",
        "        ##seg\n",
        "        self.conv1 = nn.Conv1d(512, 128, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        #self.conv2 = nn.Conv1d(128, num_classes, 1)\n",
        "        self.conv2 = nn.Conv1d(128, self.num_classes, 1)\n",
        "\n",
        "        ##nmask\n",
        "        self.fc1_= nn.Linear(in_features=512,out_features=256).to(device)\n",
        "        self.conv2_ = nn.Conv2d(512, 256, 1).to(device)\n",
        "        self.conv3_ = nn.Conv2d(512, 128, 1).to(device)\n",
        "        self.conv4_ = nn.Conv2d(128, 128, 1).to(device)\n",
        "        self.conv5_ = nn.Conv2d(129, 64, 1).to(device)\n",
        "        self.conv6_ = nn.Conv2d(64, 32, 1).to(device)\n",
        "        self.conv7_ = nn.Conv2d(32, 1, 1).to(device)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        " \n",
        "    def get_query_embeddings(self, encoder_xyz, point_cloud_dims):\n",
        "        encoder_xyz_= encoder_xyz.permute(0,2,1)\n",
        "        query_inds = farthest_point_sample(encoder_xyz_, self.num_queries)\n",
        "\n",
        "        query_inds = query_inds.long()\n",
        "        query_xyz = [torch.gather(encoder_xyz_[..., x], 1, query_inds) for x in range(3)]\n",
        "        query_xyz = torch.stack(query_xyz)\n",
        "        query_xyz = query_xyz.permute(1, 2, 0)\n",
        "\n",
        "        pos_embed = self.pos_embedding(query_xyz, input_range=point_cloud_dims)\n",
        "        query_embed = self.query_projection(pos_embed)\n",
        "        return query_xyz, query_embed\n",
        "\n",
        "    def run_encoder(self, point_clouds):\n",
        "        #xyz, features = point_clouds,point_clouds\n",
        "        #xyz,features=point_clouds.permute(0,2,1),point_clouds.permute(0,2,1)\n",
        "        pre_enc_xyz, pre_enc_features, pre_enc_inds = self.pre_encoder(point_clouds.permute(0,2,1), point_clouds.permute(0,2,1))\n",
        "        # xyz: batch x npoints x 3\n",
        "        # features: batch x channel x npoints\n",
        "        # inds: batch x npoints\n",
        "\n",
        "        # nn.MultiHeadAttention in encoder expects npoints x batch x channel features\n",
        "        pre_enc_features = pre_enc_features.permute(2, 0, 1)\n",
        "\n",
        "        # xyz points are in batch x npointx channel order\n",
        "        enc_xyz, enc_features, enc_inds = self.encoder(\n",
        "            pre_enc_features, xyz=pre_enc_xyz\n",
        "        )\n",
        "        if enc_inds is None:\n",
        "            # encoder does not perform any downsampling\n",
        "            enc_inds = pre_enc_inds\n",
        "        else:\n",
        "            # use gather here to ensure that it works for both FPS and random sampling\n",
        "            enc_inds = torch.gather(pre_enc_inds, 1, enc_inds.type(torch.int64))\n",
        "        return enc_xyz, enc_features, enc_inds,pre_enc_xyz,pre_enc_features\n",
        "\n",
        "    def xxlu(self,x,label):\n",
        "        relu = nn.ReLU()\n",
        "        lrelu = nn.LeakyReLU(0.2)\n",
        "        if label =='relu':\n",
        "            return  relu(x)\n",
        "        if label =='lrelu':\n",
        "            return  lrelu(x)\n",
        "            \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        xyz = inputs[\"point_clouds\"].to(device)\n",
        "        xyz = xyz.permute(0,2,1)\n",
        "\n",
        "        enc_xyz, enc_features, enc_inds,pre_enc_xyz ,pre_enc_features= self.run_encoder(xyz)\n",
        "        \n",
        "        #Pointnet2\n",
        "        # Set Abstraction layers\n",
        "        B,C,N = xyz.shape\n",
        "        l00_points = pre_enc_xyz#.permute(0,2,1)\n",
        "        l00_xyz = pre_enc_xyz#.permute(0,2,1)\n",
        "\n",
        "        l0_xyz, l0_points, fps_idx = self.sa0(l00_xyz.permute(0,2,1), l00_points.permute(0,2,1))\n",
        "        l05_xyz, l05_points, fps_idx = self.sa05(l0_xyz.permute(0,2,1), l0_points.permute(0,2,1))\n",
        "        l1_xyz, l1_points, fps_idx = self.sa1(l05_xyz.permute(0,2,1), l05_points.permute(0,2,1))\n",
        "        l2_xyz, l2_points, fps_idx = self.sa2(l1_xyz.permute(0,2,1), l1_points.permute(0,2,1))\n",
        "        l3_xyz, l3_points, fps_idx = self.sa3(l2_xyz.permute(0,2,1), l2_points.permute(0,2,1)) \n",
        "        l4_xyz, l4_points,fps_idx  = self.sa4(l3_xyz.permute(0,2,1), l3_points.permute(0,2,1))\n",
        "        l5_xyz, l5_points,fps_idx  = self.sa5(l4_xyz.permute(0,2,1), l4_points.permute(0,2,1)) # [4, 1, 512]\n",
        "        # Feature Propagation layers\n",
        "        l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n",
        "        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n",
        "        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n",
        "        l05_points= self.fp1(l05_xyz, l1_xyz, l05_points, l1_points)\n",
        "        l0_points = self.fp05(l0_xyz, l05_xyz, l0_points, l05_points)\n",
        "        l0_points = self.fp0(l00_xyz, l0_xyz, l00_points, l0_points)\n",
        " \n",
        "        global_features = l5_points.permute(0,2,1)\n",
        "        point_features = l0_points\n",
        "        \n",
        "        #Seg\n",
        "        x = self.drop1(F.relu(self.bn1(self.conv1(l0_points))))\n",
        "        sem_seg = self.conv2(x)\n",
        "        #sem_seg = torch.sigmoid(x)\n",
        "\n",
        "\n",
        "        #Transformer encoder\n",
        "        #enc_xyz, enc_features, enc_inds,pre_enc_xyz ,pre_enc_features= self.run_encoder(xyz)\n",
        "        gt_mask=index_points(inputs[\"masks\"].permute(0,2,1).to(device),enc_inds)\n",
        "        labels=index_points2(inputs[\"category\"].to(device),enc_inds)\n",
        "\n",
        "        enc_features = self.encoder_to_decoder_projection(\n",
        "            enc_features.permute(1, 2, 0)\n",
        "        ).permute(2, 0, 1)\n",
        "        # encoder features: npoints x batch x channel\n",
        "        # encoder xyz: npoints x batch x 3\n",
        "\n",
        "        point_cloud_dims = [\n",
        "            inputs[\"point_cloud_dims_min\"].to(device),\n",
        "            inputs[\"point_cloud_dims_max\"].to(device),\n",
        "        ]\n",
        "        query_xyz, query_embed = self.get_query_embeddings(enc_xyz, point_cloud_dims)\n",
        "        enc_xyz=enc_xyz.permute(0,2,1)\n",
        "        # query_embed: batch x channel x npoint\n",
        "        enc_pos = self.pos_embedding(enc_xyz, input_range=point_cloud_dims)\n",
        "\n",
        "        # decoder expects: npoints x batch x channel\n",
        "        enc_pos = enc_pos.permute(2, 0, 1)\n",
        "        query_embed = query_embed.permute(2, 0, 1)\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        box_features = self.decoder(\n",
        "            tgt, enc_features, query_pos=query_embed, pos=enc_pos\n",
        "        )[0]\n",
        "        #box_features=self.sigmoid(box_features)\n",
        "        box_features = box_features.permute(0, 2, 3, 1)\n",
        "        num_layers, batch, channel, num_queries = (\n",
        "            box_features.shape[0],\n",
        "            box_features.shape[1],\n",
        "            box_features.shape[2],\n",
        "            box_features.shape[3],\n",
        "        )\n",
        "        box_features = box_features.reshape(batch,num_queries,num_layers* channel)\n",
        "        pre_enc_features=pre_enc_features.permute(1,2,0)\n",
        "        ins_net= torch.concat([box_features, pre_enc_xyz,pre_enc_features], 1 )\n",
        "        ins_net = self.lrelu(self.conv_(ins_net)) \n",
        "        ins_net = self.lrelu(self.conv_0(ins_net))\n",
        "        ins_net=self.conv_F(ins_net)\n",
        "        #mask_pred=self.softmax(ins_net)\n",
        "\n",
        "\n",
        "        # Mask genration branch\n",
        "        \n",
        "        point_features=point_features.permute(0,2,1)\n",
        "        p_f_num = int(point_features.shape[-1])\n",
        "        p_num = point_features.shape[1]\n",
        "        bb_num = int(box_features.shape[1])\n",
        "        #print(p_f_num,p_num,bb_num)\n",
        "\n",
        "        #1. both the point and global features are compressed to be 256 dimensional vectors through fully connected layers, \n",
        "        #before being concatenated and further compressed to be 128 dimensional mixed point features Fl\n",
        "        global_features_=(self.xxlu(self.fc1_(global_features),'lrelu')[:,None,:]).repeat( [1, p_num, 1, 1])\n",
        "        point_features_=point_features[:,:,:,None].permute(0,2,1,3)\n",
        "        point_features_=self.xxlu(self.conv2_(point_features_),label='lrelu').permute(0,1,3,2)\n",
        "        Fl=torch.cat([point_features_.permute(0,3,2,1),global_features_],-1)\n",
        "        Fl=Fl.permute(0,3,2,1)\n",
        "        Fl=self.xxlu(self.conv3_(Fl),label='lrelu')#.permute(0,1,3,2)\n",
        "        Fl=self.xxlu(self.conv4_(Fl),label='lrelu')#.permute(0,1,3,2)\n",
        "        Fl = torch.squeeze(Fl,-2)\n",
        "\n",
        "        #2. For the ith predicted bounding box Bi, the estimated vertices are fused with features Fl through concatenation, \n",
        "        # producing box-aware features Fbl\n",
        "        pmask0 = Fl[:,None,:,:].repeat([1, bb_num, 1, 1])\n",
        "        box_features=box_features[:,:,None,:]#.shape\n",
        "        pmask0 = torch.concat([pmask0, box_features], axis=2)\n",
        "        pmask0_=pmask0.permute(0,2,1,3)\n",
        "        pmask1=self.xxlu(self.conv5_(pmask0_),label='lrelu')#.permute(0,1,3,2)\n",
        "        pmask2=self.xxlu(self.conv6_(pmask1),label='lrelu')#.permute(0,1,3,2)\n",
        "        pmask3=self.xxlu(self.conv7_(pmask2),label='lrelu')#.permute(0,1,3,2)\n",
        "        pmask3 = torch.reshape(pmask3, [-1, bb_num, p_num])\n",
        "        y_pmask_pred = self.softmax(pmask3)\n",
        "\n",
        "\n",
        "        return sem_seg,labels,y_pmask_pred,gt_mask.permute(0,2,1)"
      ],
      "metadata": {
        "id": "RHP8R2mFcqO_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_preencoder():\n",
        "    #mlp_dims = [3 * int(args.use_color), 64, 128, args.enc_dim]\n",
        "    preencoder = PointNetSetAbstraction ( \n",
        "        npoint=2048,\n",
        "        radius=0.2, \n",
        "        nsample=32, \n",
        "        in_channel=6,\n",
        "        mlp=[64, 64, 128,256],\n",
        "        group_all=False\n",
        "    )\n",
        "    return preencoder\n",
        "\n",
        "def build_encoder():\n",
        "        #enc_type == \"vanilla\":\n",
        "        encoder_layer = TransformerEncoderLayer(\n",
        "            d_model=256,\n",
        "            nhead=4,\n",
        "            dim_feedforward=128,\n",
        "            dropout=0.1,\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "        encoder = TransformerEncoder(\n",
        "            encoder_layer=encoder_layer, num_layers=3\n",
        "        )\n",
        "        return encoder\n",
        "args={}\n",
        "args[\"dec_dim\"]=256\n",
        "args[\"dec_nhead\"]=4\n",
        "args[\"dec_ffn_dim\"]=256\n",
        "args[\"dec_dropout\"]=0.1\n",
        "args[\"dec_nlayers\"]=8\n",
        "\n",
        "def build_decoder(args):\n",
        "    decoder_layer = TransformerDecoderLayer(\n",
        "        d_model=args[\"dec_dim\"],\n",
        "        nhead=args[\"dec_nhead\"],\n",
        "        dim_feedforward=args[\"dec_ffn_dim\"],\n",
        "        dropout=args[\"dec_dropout\"],\n",
        "    )\n",
        "    decoder = TransformerDecoder(\n",
        "        decoder_layer, num_layers=args[\"dec_nlayers\"], return_intermediate=True\n",
        "    )\n",
        "    return decoder\n",
        "\n",
        "def build_3detr():\n",
        "    pre_encoder = build_preencoder()\n",
        "    encoder = build_encoder()\n",
        "    decoder = build_decoder(args)\n",
        "    model = Model3DETR(\n",
        "        pre_encoder,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        encoder_dim=256,\n",
        "        decoder_dim=256,\n",
        "        mlp_dropout= 0.3 ,\n",
        "        num_queries=200,\n",
        "        num_classes=39\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "enuqY-ebc26M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model= build_3detr()\n",
        "#model.to(device)\n",
        "#print(\"model_created\")"
      ],
      "metadata": {
        "id": "V6EngXWEdgcS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## for testing\n",
        "for (idx, batch) in enumerate(train_loader):\n",
        "    print('Batch index: ', idx)\n",
        "    inputs, labels,gt_mask_pl,gt_valid_pl =batch['point_clouds'].to(device),batch['category'].to(device), batch['masks'].to(device), batch['valid'].to(device)\n",
        "    #gt_mask_pl=gt_mask_pl.permute(0,2,1)\n",
        "    print(inputs.shape)\n",
        "    print(labels.shape)\n",
        "    print(gt_mask_pl.shape)\n",
        "    break\n",
        "\n",
        "pts2048,outputs ,labels, masks_pred,gt_masks = model(batch)\n",
        "print(\"gt_masks\",gt_masks.shape)\n",
        "print(\"masks_pred\",masks_pred.shape)\n",
        "print(\"labels\",labels.shape)\n",
        "print(\"outputs\",outputs.shape)\n",
        "'''"
      ],
      "metadata": {
        "id": "Hz2yfK3tN8EF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863e022b-c3bd-4649-ee74-5dafce51584c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## for testing\\nfor (idx, batch) in enumerate(train_loader):\\n    print(\\'Batch index: \\', idx)\\n    inputs, labels,gt_mask_pl,gt_valid_pl =batch[\\'point_clouds\\'].to(device),batch[\\'category\\'].to(device), batch[\\'masks\\'].to(device), batch[\\'valid\\'].to(device)\\n    #gt_mask_pl=gt_mask_pl.permute(0,2,1)\\n    print(inputs.shape)\\n    print(labels.shape)\\n    print(gt_mask_pl.shape)\\n    break\\n\\npts2048,outputs ,labels, masks_pred,gt_masks = model(batch)\\nprint(\"gt_masks\",gt_masks.shape)\\nprint(\"masks_pred\",masks_pred.shape)\\nprint(\"labels\",labels.shape)\\nprint(\"outputs\",outputs.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVGdMPy4DZBx"
      },
      "source": [
        "## Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_seg_loss (outputs, labels, end_points, alpha = 0.0001): \n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs=outputs.size(0)\n",
        "    #id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n",
        "    #id128x128 = torch.eye(128, requires_grad=True).repeat(bs,1,1)\n",
        "    #if outputs.is_cuda:\n",
        "     #   id3x3=id3x3.cuda()\n",
        "     #   id128x128=id128x128.cuda()\n",
        "    #diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n",
        "    #diff128x128 = id128x128-torch.bmm(m128x128,m128x128.transpose(1,2))\n",
        "    logsoftmax = nn.LogSoftmax(dim=1)\n",
        "    per_point_loss= criterion(logsoftmax(outputs), labels) #+ alpha * (torch.norm(diff3x3)+torch.norm(diff128x128)) / float(bs)\n",
        "    end_points['per_point_seg_loss'] = per_point_loss\n",
        "    per_shape_loss = torch.mean(per_point_loss, -1)\n",
        "    end_points['per_shape_seg_loss'] = per_shape_loss\n",
        "   # loss = torch.mean(per_shape_loss)\n",
        "    return per_shape_loss, end_points "
      ],
      "metadata": {
        "id": "KygfuXOSbgJQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def hungarian_matching(pred_x, gt_x, curnmasks):\n",
        "    \"\"\" pred_x, gt_x: B x nmask x n_point\n",
        "            curnmasks: B\n",
        "            return matching_idx: B x nmask x 2 \"\"\"\n",
        "    pred_x = pred_x.cpu().detach().numpy()\n",
        "    gt_x = gt_x.cpu().detach().numpy()\n",
        "    curnmasks = curnmasks.cpu().detach().numpy()\n",
        "\n",
        "    batch_size = gt_x.shape[0]\n",
        "    nmask = gt_x.shape[1]\n",
        "    matching_score = np.matmul(gt_x, np.transpose(pred_x, axes=[0, 2, 1]))  # B x nmask x nmask\n",
        "    matching_score = 1 - np.divide(matching_score, np.maximum(\n",
        "        np.expand_dims(np.sum(pred_x, 2), 1) + np.sum(gt_x, 2, keepdims=True) - matching_score, 1e-8))\n",
        "    matching_idx = np.zeros((batch_size, nmask, 2)).astype('int32')\n",
        "    curnmasks = curnmasks.astype('int32')\n",
        "    for i, curnmask in enumerate(curnmasks):\n",
        "        row_ind, col_ind = linear_sum_assignment(matching_score[i, :curnmask, :])\n",
        "        matching_idx[i, :curnmask, 0] = row_ind\n",
        "        matching_idx[i, :curnmask, 1] = col_ind\n",
        "    return matching_idx\n",
        "\n",
        "def iou_inst(pred_x, gt_x, gt_conf, n_point, nmask, k_inst=50):\n",
        "    matching_idx = hungarian_matching(pred_x, gt_x, torch.sum(gt_conf, -1)) # B x nmask x 2\n",
        "    matching_idx = torch.from_numpy(matching_idx).to(pred_x.device)\n",
        "\n",
        "    matching_idx_row = matching_idx[:, :, 0]\n",
        "    idx = torch.where(torch.greater_equal(matching_idx_row, 0))\n",
        "    matching_idx_row = torch.cat(\n",
        "        (torch.unsqueeze(idx[0].type(torch.int32), -1), torch.reshape(matching_idx_row, [-1, 1])), 1)\n",
        "    # gt_x_matched = torch.reshape(gt_x.masked_select(matching_idx_row), [-1, nmask, n_point])\n",
        "    gt_x_matched = torch.reshape(gt_x[matching_idx_row[:, 0].long(), matching_idx_row[:,1].long()], [-1, nmask, n_point])\n",
        "\n",
        "\n",
        "    matching_idx_column = matching_idx[:, :, 1]\n",
        "    idx = torch.where(torch.greater_equal(matching_idx_column, 0))\n",
        "    matching_idx_column = torch.cat(\n",
        "        (torch.unsqueeze(idx[0].type(torch.int32), -1), torch.reshape(matching_idx_column, [-1, 1])), 1)\n",
        "    # pred_x_matched = torch.reshape(pred_x.masked_select(matching_idx_column), [-1, nmask, n_point])\n",
        "    pred_x_matched = torch.reshape(pred_x[matching_idx_column[:, 0].long(), matching_idx_column[:, 1].long()],\n",
        "                                 [-1, nmask, n_point])\n",
        "\n",
        "    # mask number of instances, set values above k_inst to 0/-inf etc\n",
        "    #gt_x_matched = gt_x_matched[:, :k_inst, :]\n",
        "    #pred_x_matched = pred_x_matched[:, :k_inst, :]\n",
        "\n",
        "    # compute meaniou\n",
        "    matching_score = torch.sum(torch.multiply(gt_x_matched, pred_x_matched), 2)\n",
        "    iou_all = torch.divide(matching_score,\n",
        "                        torch.sum(gt_x_matched, 2) + torch.sum(pred_x_matched, 2) - matching_score + 1e-8)\n",
        "\n",
        "    meaniou = torch.divide(torch.sum(torch.multiply(iou_all, gt_conf), 1), torch.sum(gt_conf, -1) + 1e-8)  # B\n",
        "    return meaniou\n",
        "\n",
        "def get_ins_loss(mask_pred, mask_gt, gt_valid,end_points):\n",
        "    \"\"\" Input:  mask_pred   B x K x N\n",
        "                mask_gt     B x K x N\n",
        "                gt_valid    B x K\n",
        "    \"\"\"\n",
        "    #num_ins = mask_pred.get_shape()[1].value\n",
        "    #num_point = mask_pred.get_shape()[2].value\n",
        "    num_ins = mask_pred.shape[1]\n",
        "    num_point = mask_pred.shape[2]\n",
        "    meaniou = iou_inst(mask_pred, mask_gt, gt_valid, num_point, num_ins)\n",
        "    loss = - torch.mean(meaniou)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "xiXcKDMwds1E"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg9RjG7awgVK"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cqXW9-oJwEPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878b1c23-dc16-4cf2-ab7c-c86437f822c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_created\n"
          ]
        }
      ],
      "source": [
        "model= build_3detr()\n",
        "model.to(device)\n",
        "print(\"model_created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JV09EA4_wJnR"
      },
      "outputs": [],
      "source": [
        "LR = 0.0001\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=1e-3, \n",
        "                                              step_size_up=1000, cycle_momentum=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kR9oqW_nr8O4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "203a44f2-f07a-4b75-b9d5-972e43e11c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Log in to your W&B account\n",
        "!pip install wandb -Uq\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CJv-cBeusYGD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c7c97c26-130c-48f4-f7a0-15bf5d4dd70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfatmaa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230516_231515-qfyoru3b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fatmaa/3detr_3D-Bonet-1dataset-75ep/runs/qfyoru3b' target=\"_blank\">jolly-totem-1</a></strong> to <a href='https://wandb.ai/fatmaa/3detr_3D-Bonet-1dataset-75ep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/fatmaa/3detr_3D-Bonet-1dataset-75ep' target=\"_blank\">https://wandb.ai/fatmaa/3detr_3D-Bonet-1dataset-75ep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/fatmaa/3detr_3D-Bonet-1dataset-75ep/runs/qfyoru3b' target=\"_blank\">https://wandb.ai/fatmaa/3detr_3D-Bonet-1dataset-75ep/runs/qfyoru3b</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 🐝 initialise a wandb run\n",
        "wandb.init(\n",
        "        project=\"3detr_3D-Bonet-1dataset-75ep\",\n",
        "        config={\n",
        "            \"epochs\": 75,\n",
        "            \"lr\": 1e-3,\n",
        "            })\n",
        "    \n",
        "# Copy your config \n",
        "config = wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "import time\n",
        "\n",
        "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
        "mcc_metric = MulticlassMatthewsCorrCoef(num_classes=39).to(device)\n",
        "# store best validation iou\n",
        "best_iou = 0.6\n",
        "best_mcc = 0.6\n",
        "\n",
        "def compute_iou(targets, predictions):\n",
        "\n",
        "    targets = targets.reshape(-1)\n",
        "    predictions = predictions.reshape(-1)\n",
        "\n",
        "    intersection = torch.sum(predictions == targets) # true positives\n",
        "    union = len(predictions) + len(targets) - intersection\n",
        "\n",
        "    return intersection / union \n",
        "    \n",
        "# lists to store metrics\n",
        "train_loss = []\n",
        "train_seg_loss=[]\n",
        "train_ins_loss=[]\n",
        "train_accuracy = []\n",
        "train_mcc = []\n",
        "train_iou = []\n",
        "\n",
        "valid_loss = []\n",
        "valid_ins_loss = []\n",
        "valid_seg_loss = []\n",
        "valid_accuracy = []\n",
        "valid_mcc = []\n",
        "valid_iou = []"
      ],
      "metadata": {
        "id": "pFLg3bdWRU_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"best_checkpoint\"\n",
        "checkpoint = torch.load( FILE ,map_location=torch.device('cpu')) \n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch_ = checkpoint['epoch']+1\n",
        "loss_=checkpoint['loss']"
      ],
      "metadata": {
        "id": "cUestysyTmWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CgaPisZFwVzh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "def train(model, train_loader, val_loader=None,  epochs=5, save=True):\n",
        "    end_points={}\n",
        "    BATCH_SIZE=2\n",
        "    NUM_TRAIN_POINTS=2048\n",
        "    for epoch in tqdm(range(0,config.epochs)): \n",
        "      #with torch.autograd.detect_anomaly():\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        _train_loss = []\n",
        "        _train_seg_loss=[]\n",
        "        _train_ins_loss=[]\n",
        "        _train_accuracy = []\n",
        "        _train_mcc = []\n",
        "        _train_iou = []\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            gt_valid_pl = data['valid'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs ,labels, mask_pred,gt_masks = model(data)\n",
        "            ins_loss = get_ins_loss(mask_pred, gt_masks, gt_valid_pl, end_points)\n",
        "            seg_loss ,end_points = get_seg_loss(outputs, labels,end_points)\n",
        "            loss=ins_loss+seg_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #scheduler.step() # update learning rate\n",
        "        \n",
        "            # get class predictions\n",
        "            pred_choice = torch.softmax(outputs, dim=1).argmax(dim=1)\n",
        "            # get metrics\n",
        "            correct = pred_choice.eq(labels.data).cpu().sum()\n",
        "            accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
        "            mcc = mcc_metric(outputs, labels)\n",
        "            iou = compute_iou(labels, pred_choice)\n",
        "\n",
        "            # update epoch loss and accuracy\n",
        "            _train_loss.append(loss.item())\n",
        "            _train_ins_loss.append(ins_loss.item())\n",
        "            _train_seg_loss.append(seg_loss.item())\n",
        "            _train_accuracy.append(accuracy)\n",
        "            _train_mcc.append(mcc.item())\n",
        "            _train_iou.append(iou.item())\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()           \n",
        "            if i % 100== 0:    # print every 10 mini-batches\n",
        "                print(f'\\t [Epoch {epoch}: Batch {i}] ' \\\n",
        "                  + f'train loss: {loss.item():.4f} ' \\\n",
        "                  + f'train seg loss: {seg_loss.item():.4f} ' \\\n",
        "                  + f'train ins loss: {ins_loss.item():.4f} ' \\\n",
        "                  + f'Seg_accuracy: {accuracy:.4f} ' \\\n",
        "                  + f'Seg_mcc: {mcc:.4f} ' \\\n",
        "                  + f'Seg_iou: {iou:.4f}')\n",
        "                wandb.log({\"epoch\": epoch, \"loss\": loss , \"ins_loss\":ins_loss ,\"seg_loss\":seg_loss,\"Seg_accuracy\":accuracy })\n",
        "       \n",
        "        train_loss.append(np.mean(_train_loss))\n",
        "        train_seg_loss.append(np.mean(_train_seg_loss))\n",
        "        train_ins_loss.append(np.mean(_train_ins_loss))\n",
        "        train_accuracy.append(np.mean(_train_accuracy))\n",
        "        train_mcc.append(np.mean(_train_mcc))\n",
        "        train_iou.append(np.mean(_train_iou))\n",
        "        print(f'Epoch: {epoch} - Train Loss: {train_loss[-1]:.4f} ' \\\n",
        "          + f'- Train SEG Loss: {train_seg_loss[-1]:.4f} ' \\\n",
        "          + f'- Train INS Loss: {train_ins_loss[-1]:.4f} ' \\\n",
        "          + f'- Train Accuracy: {train_accuracy[-1]:.4f} ' \\\n",
        "          + f'- Train MCC: {train_mcc[-1]:.4f} ' \\\n",
        "          + f'- Train IOU: {train_iou[-1]:.4f}')\n",
        "        # pause to cool down\n",
        "        time.sleep(4)  \n",
        "        # get test results after each epoch\n",
        "        if val_loader:\n",
        "         with torch.no_grad():\n",
        "\n",
        "            # place model in evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            _valid_loss = []\n",
        "            _valid_seg_loss = []\n",
        "            _valid_ins_loss = []\n",
        "            _valid_accuracy = []\n",
        "            _valid_mcc = []\n",
        "            _valid_iou = []\n",
        " \n",
        "            for i,data in enumerate(val_loader,0):\n",
        "                    gt_valid_pl = data['valid'].to(device)\n",
        "                    outputs ,labels, mask_pred,gt_masks = model(data)\n",
        "                    seg_loss ,end_points = get_seg_loss(outputs, labels,end_points)\n",
        "                    ins_loss = get_ins_loss(mask_pred, gt_masks, gt_valid_pl, end_points)\n",
        "                    loss=ins_loss+seg_loss\n",
        "                    \n",
        "                    # get class predictions\n",
        "                    pred_choice = torch.softmax(outputs, dim=1).argmax(dim=1)\n",
        "                    # get metrics\n",
        "                    correct = pred_choice.eq(labels.data).cpu().sum()\n",
        "                    accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
        "                    mcc = mcc_metric(outputs, labels)\n",
        "                    iou = compute_iou(labels, pred_choice)\n",
        "\n",
        "                    # update epoch loss and accuracy\n",
        "                    _valid_ins_loss.append(ins_loss.item())\n",
        "                    _valid_seg_loss.append(seg_loss.item())\n",
        "                    _valid_loss.append(loss.item())\n",
        "                    _valid_accuracy.append(accuracy)\n",
        "                    _valid_mcc.append(mcc.item())\n",
        "                    _valid_iou.append(iou.item())\n",
        "\n",
        "                    # print statistics\n",
        "                    if i % 100== 0:    # print every 10 mini-batches\n",
        "                     print(f'\\t [Epoch {epoch}: Batch {i}] ' \\\n",
        "                      + f'valid loss: {loss.item():.4f} ' \\\n",
        "                      + f'valid seg loss: {seg_loss.item():.4f} ' \\\n",
        "                      + f'valid ins loss: {ins_loss.item():.4f} ' \\\n",
        "                      + f'valid Seg_accuracy: {accuracy:.4f} ' \\\n",
        "                      + f'valid Seg_mcc: {mcc:.4f} ' \\\n",
        "                      + f'valid Seg_iou: {iou:.4f}')\n",
        "                    wandb.log({\"epoch\": epoch, \"valid loss\": loss , \"valid ins_loss\":ins_loss ,\"valid seg_loss\":seg_loss,\"valid Seg_accuracy\":accuracy })\n",
        "       \n",
        "            valid_loss.append(np.mean(_valid_loss))\n",
        "            valid_seg_loss.append(np.mean(_valid_seg_loss))\n",
        "            valid_ins_loss.append(np.mean(_valid_ins_loss))\n",
        "            valid_accuracy.append(np.mean(_valid_accuracy))\n",
        "            valid_mcc.append(np.mean(_valid_mcc))\n",
        "            valid_iou.append(np.mean(_valid_iou))\n",
        "            print(f'Epoch: {epoch} - Validation Loss: {valid_loss[-1]:.4f} ' \\\n",
        "               + f'- Validation SEG Loss: {valid_seg_loss[-1]:.4f} ' \\\n",
        "               + f'- Validation INS Loss: {valid_ins_loss[-1]:.4f} ' \\\n",
        "               + f'- Validation Accuracy: {valid_accuracy[-1]:.4f} ' \\\n",
        "               + f'- Validation MCC: {valid_mcc[-1]:.4f} ' \\\n",
        "               + f'- Validation IOU: {valid_iou[-1]:.4f}')\n",
        "            #print('[Validation Epoch %03d, Batch %03d]=  %f (ins_loss)' \\\n",
        "            #        % (epoch+1, i+1, ins_loss))\n",
        "            wandb.log({\"epoch\": epoch, \"valid loss\": loss , \"valid ins_loss\":ins_loss ,\"valid seg_loss\":seg_loss,\"valid Seg_accuracy\":accuracy })\n",
        "\n",
        "            # save the model\n",
        "            if epoch>20:\n",
        "              save_path=\"/content/gdrive/MyDrive/trans_losses_2048/3dert_3D-Bonet_loss\"+str(epoch)+\"_\"+str(valid_seg_loss[-1])+\".pth\"\n",
        "              #torch.save(model.state_dict(),save_path )\n",
        "              torch.save({\n",
        "                 'epoch': epoch,\n",
        "                 'model_state_dict': model.state_dict(),\n",
        "                 'optimizer_state_dict': optimizer.state_dict(),\n",
        "                 'loss': loss,\n",
        "              }, save_path)\n",
        "              files.download(save_path) \n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU_2lHvTWW76"
      },
      "outputs": [],
      "source": [
        "train(model, train_loader, val_loader,  save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Downsampling"
      ],
      "metadata": {
        "id": "pC7Q5LmzlY4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,z=np.array(pre_enc_xyz[1].permute(1,0)).T\n",
        "c = np.array(masks_pl[1][0].float().cpu()).T\n",
        "#c = np.array(pred_np[0]).T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, \n",
        "                                   mode='markers',\n",
        "                                   marker=dict(\n",
        "        size=30,\n",
        "        color=c,                # set color to an array/list of desired values\n",
        "        colorscale='Viridis',   # choose a colorscale\n",
        "        opacity=1.0\n",
        "    ))])\n",
        "fig.update_traces(marker=dict(size=2,\n",
        "                              line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBGjqSV_lYJy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "2ede1787-93ba-441d-e171-f4d36255d4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"361fdc6f-0e7e-43d8-ac40-5a44ce0d7ccc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"361fdc6f-0e7e-43d8-ac40-5a44ce0d7ccc\")) {                    Plotly.newPlot(                        \"361fdc6f-0e7e-43d8-ac40-5a44ce0d7ccc\",                        [{\"marker\":{\"color\":[0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"opacity\":1.0,\"size\":2,\"line\":{\"color\":\"DarkSlateGrey\",\"width\":2}},\"mode\":\"markers\",\"x\":[0.24030862748622894,-0.2590193450450897,-0.2746244966983795,-0.2748759984970093,0.2454184740781784,0.274836927652359,0.274836927652359,-0.26320624351501465,-0.18374355137348175,0.24584338068962097,-0.2746836245059967,0.251726895570755,0.2674863338470459,-0.22951070964336395,0.2593059539794922,-0.22053465247154236,0.24802900850772858,-0.012338696978986263,0.015095540322363377,0.036628250032663345,-0.2671877145767212,0.01870720647275448,-0.026090890169143677,-0.2700347602367401,0.09398701786994934,-0.0244063138961792,0.22870700061321259,-0.22874608635902405,-0.03631860390305519,0.2192068248987198,0.26627275347709656,-0.0012100731255486608,0.25136709213256836,-0.2432548850774765,-0.2748759984970093,-0.23813802003860474,0.2832898795604706,0.19965332746505737,-0.2671877145767212,0.2311832457780838,0.20104527473449707,-0.1476619690656662,-0.228851318359375,-0.1930292695760727,0.12941020727157593,-0.11942902952432632,0.10790656507015228,0.12564623355865479,-0.055668678134679794,0.08676969259977341,0.26714861392974854,-0.13195061683654785,-0.12814953923225403,0.11576424539089203,-0.017191998660564423,-0.2572145164012909,-0.28230077028274536,-0.03368300572037697,0.08247056603431702,-0.09254796802997589,-0.07861638814210892,-0.1467580497264862,0.1004527360200882,-0.23590828478336334,-0.28287699818611145,-0.07777661085128784,0.09888942539691925,0.22793936729431152,0.1993967741727829,0.26714861392974854,0.07980290055274963,0.10192485898733139,0.2794627547264099,-0.12737490236759186,-0.2496885508298874,0.2682008445262909,-0.22874608635902405,0.24350641667842865,0.2450035959482193,-0.05688726529479027,-0.2515154182910919,-0.014905145391821861,0.17080608010292053,0.23953698575496674,0.27401816844940186,-0.11558085680007935,0.025488603860139847,-0.16504888236522675,-0.0058759781531989574,0.26240652799606323,0.22870700061321259,-0.2592238187789917,-0.28754791617393494,0.29013538360595703,-0.2748759984970093,-0.22874608635902405,-0.26999062299728394,0.26307594776153564,0.05258813127875328,-0.2544967532157898,-0.002583989640697837,-0.09012983739376068,-0.0866534486413002,-0.2671877145767212,0.03931595757603645,0.1791798621416092,-0.18058085441589355,0.09600930660963058,-0.13295674324035645,-0.21451087296009064,-0.2804778814315796,-0.17957070469856262,0.12051532417535782,-0.24321380257606506,0.11579631268978119,0.06276272982358932,0.20391537249088287,-0.06776335090398788,0.16738882660865784,-0.05931040644645691,-0.24609792232513428,0.24966548383235931,0.1925853192806244,0.24681143462657928,-0.162262961268425,-0.08371321856975555,0.2903288006782532,0.16376513242721558,-1.9545792383723892e-05,-0.19000786542892456,0.14596736431121826,-0.24625827372074127,0.08420424163341522,-0.24799494445323944,-0.12653110921382904,0.2058795541524887,0.19808800518512726,-0.054486166685819626,-0.03692789375782013,-0.18971022963523865,0.04010763764381409,0.2700507640838623,-0.07649288326501846,0.08157466351985931,0.07466398924589157,0.28523901104927063,-0.14743147790431976,-0.09267523884773254,0.18843449652194977,0.050706133246421814,0.26059970259666443,0.10452939569950104,0.00406211894005537,-0.20249135792255402,-0.00018289254512637854,-0.04128314182162285,0.20963652431964874,-0.1785304844379425,0.13393481075763702,-0.05970524623990059,-0.19857604801654816,-0.19684436917304993,0.06662392616271973,-0.24759510159492493,-0.2533092200756073,0.012183358892798424,0.21683278679847717,0.24752695858478546,-0.10626107454299927,-0.016604753211140633,0.19686339795589447,0.19288796186447144,-0.2326764315366745,0.10296507179737091,-0.22438383102416992,0.08156363666057587,0.22870700061321259,-0.24649778008460999,0.22961893677711487,0.18265122175216675,0.2504100799560547,0.04276427626609802,0.26552414894104004,-0.22874608635902405,-0.2487625926733017,-0.2849504053592682,-0.10872030258178711,-0.07515103369951248,0.01947483792901039,0.1996513158082962,0.05313829705119133,-0.26220113039016724,-0.29025065898895264,-0.22932732105255127,-0.09876416623592377,-0.06617999076843262,0.23530100286006927,0.12611623108386993,0.07439642399549484,-0.2748759984970093,-0.25334930419921875,-0.24180079996585846,0.2727384567260742,-0.1072642058134079,0.23711785674095154,0.25506094098091125,-0.01980254240334034,-0.2905683219432831,-0.2734459936618805,0.20348045229911804,-0.08985525369644165,-0.2616930305957794,0.13167902827262878,-0.06618499755859375,-0.2087065428495407,0.2554207146167755,-0.23875835537910461,0.25710225105285645,-0.20664216578006744,0.26392677426338196,-0.22891244292259216,-0.2574891149997711,-0.2748759984970093,-0.21785196661949158,0.274836927652359,0.03161561116576195,0.209150493144989,0.04029102623462677,0.24772337079048157,-0.21737495064735413,0.24612095952033997,0.25104042887687683,0.2474958747625351,-0.14108599722385406,-0.11530426889657974,-0.23431290686130524,-0.2550429105758667,-0.08367713540792465,0.14889056980609894,0.26726284623146057,0.2658197581768036,-0.024041540920734406,-0.23962418735027313,0.2301410436630249,-0.004306646529585123,0.2675805389881134,0.274836927652359,0.17707538604736328,0.026442628353834152,0.034129947423934937,0.23449328541755676,-0.03040805459022522,0.2384095937013626,0.22100363671779633,0.19618697464466095,0.05273043364286423,-0.14998789131641388,-0.10830642282962799,-0.2748759984970093,0.1067982092499733,-0.11609896272420883,0.016874317079782486,-0.13405808806419373,-0.024981535971164703,-0.26185837388038635,0.2460598349571228,-0.08546794205904007,0.26714861392974854,-0.16616222262382507,-0.062006134539842606,-0.2735251486301422,0.2623434066772461,-0.1797330528497696,0.2253689169883728,0.26150161027908325,0.270699143409729,0.27181050181388855,-0.2621901035308838,0.25275102257728577,-0.2633334994316101,-0.2350284308195114,-0.22874608635902405,0.15699276328086853,0.24341821670532227,0.195118710398674,0.26117992401123047,0.09916701167821884,-0.2906906008720398,-0.23102793097496033,-0.03189621493220329,0.08750525116920471,0.2460598349571228,0.09188354760408401,-0.2053854912519455,0.22898559272289276,-0.13142649829387665,0.274836927652359,0.1486961543560028,0.14532901346683502,-0.20900116860866547,-0.008861315436661243,-0.23543429374694824,-0.12208767235279083,0.2460598349571228,-0.2550429105758667,-0.06215444579720497,0.23094473779201508,-0.1467871069908142,-0.23032544553279877,0.22870700061321259,0.24679338932037354,-0.17659136652946472,-0.26562535762786865,-0.26749834418296814,0.0590708926320076,0.029943060129880905,-0.2075130194425583,-0.26180729269981384,-0.24204231798648834,-0.2553325295448303,0.14870718121528625,-0.20840592682361603,0.22877314686775208,0.0008422837127000093,0.2764323055744171,-0.2160952389240265,0.027466801926493645,-0.2143104523420334,0.0440189391374588,-0.2469557374715805,0.1620514988899231,0.017689047381281853,-0.036640286445617676,0.058266181498765945,-0.24609792232513428,-0.05695641413331032,0.061918940395116806,0.2460598349571228,-0.24786368012428284,-0.1963954120874405,0.19950200617313385,0.05239572003483772,0.21802231669425964,0.02323281392455101,0.20207245647907257,0.27233460545539856,0.26729390025138855,0.01552946213632822,0.21896031498908997,-0.17254579067230225,0.22363223135471344,-0.19862915575504303,-0.2553134858608246,0.22090142965316772,-0.26659443974494934,0.2550860047340393,0.066851407289505,-0.28331291675567627,-0.033985648304224014,-0.08994143456220627,0.22329050302505493,-0.016778122633695602,0.274836927652359,-0.08999955654144287,0.16374309360980988,0.03892813250422478,0.267466276884079,-0.24609792232513428,0.2683311104774475,0.1305636614561081,0.281963050365448,0.24119751155376434,0.03439050167798996,-1.9545792383723892e-05,0.1042337641119957,-0.2578108012676239,-0.2671877145767212,-0.13514138758182526,0.23236075043678284,-0.054619450122117996,-0.26400694251060486,-0.2024272233247757,-0.2775056064128876,-0.26798540353775024,0.2433270364999771,-0.07937400043010712,-0.19387206435203552,0.13779902458190918,0.049822259694337845,-0.23168933391571045,0.0738823413848877,-0.0009936135029420257,0.26909974217414856,-0.07793594896793365,-0.15546151995658875,0.0948057472705841,-0.25316593050956726,0.1455935537815094,0.2627853453159332,-0.23955804109573364,0.2905522882938385,0.027869656682014465,-0.2345844805240631,-0.07040095329284668,-0.26992252469062805,-0.23409044742584229,0.14905792474746704,-0.1330329179763794,0.07399357110261917,0.13705243170261383,-0.2522880434989929,0.10967030376195908,0.28895488381385803,0.14217530190944672,-0.25350263714790344,0.14587515592575073,-0.2748759984970093,-0.01713888719677925,0.15453355014324188,0.27369049191474915,-0.21288742125034332,-0.036827683448791504,0.21074587106704712,-0.24068042635917664,0.17804445326328278,0.25523629784584045,-0.02388019859790802,-0.23058199882507324,-0.14357829093933105,-0.11937692016363144,-0.08539177477359772,0.26714861392974854,0.07960748672485352,-0.20140805840492249,0.23977850377559662,0.014238721691071987,-0.24807211756706238,-0.25403377413749695,-0.2304898053407669,-0.26616454124450684,0.08477946370840073,0.21350473165512085,-0.25694793462753296,-0.23888562619686127,-0.08398378640413284,-0.1972832977771759,0.09926922619342804,-0.06907914578914642,0.2511315941810608,-0.08764355629682541,-0.21331533789634705,0.27332472801208496,0.09865592420101166,0.2673790752887726,-0.26792725920677185,-0.2748759984970093,0.22998671233654022,0.22870700061321259,-0.2734109163284302,0.06298419833183289,0.11866138875484467,0.17698420584201813,0.03316088765859604,0.07270282506942749,-0.24609792232513428,-0.11223074793815613,0.07801510393619537,-0.05002770572900772,-0.10594841837882996,0.22344282269477844,-0.22874608635902405,0.051923718303442,-0.27886849641799927,0.034298308193683624,0.2340022474527359,0.22870700061321259,-0.08270206302404404,0.23488612473011017,0.028790613636374474,0.16214269399642944,-0.22874608635902405,-0.22874608635902405,-0.1684330552816391,-0.00793334562331438,0.1742544025182724,0.274836927652359,0.2324289083480835,0.10264238715171814,-0.28382501006126404,0.09558841586112976,0.2735642194747925,0.27361834049224854,-0.0333893857896328,0.04696018621325493,0.23416860401630402,0.26928213238716125,-0.21389155089855194,0.00439983606338501,-0.029585309326648712,0.07590662688016891,-0.2748759984970093,-0.289409875869751,0.016480479389429092,-0.03881390020251274,-0.2734670341014862,-0.2031698077917099,0.22870700061321259,-0.27152591943740845,0.274836927652359,-0.23026330769062042,-0.22874608635902405,-0.15717315673828125,0.25017958879470825,-0.13656240701675415,-0.2717333436012268,-0.2748759984970093,-0.28155818581581116,-0.2748759984970093,-0.24365974962711334,0.2381991446018219,-0.13643613457679749,0.22870700061321259,0.07487443834543228,0.24577723443508148,-0.19165335595607758,-0.09989956766366959,-0.0072899796068668365,-0.26087430119514465,-0.264534056186676,0.07551480084657669,-0.2748759984970093,0.0014986770693212748,-0.14054083824157715,-0.22874608635902405,0.13518346846103668,0.24366675317287445,-0.18042652308940887,0.19271259009838104,-0.030510272830724716,-0.04077807068824768,-0.019305486232042313,0.19644249975681305,0.13823093473911285,0.22870700061321259,0.2606668472290039,-0.08628267049789429,0.2643596827983856,-0.13362115621566772,-0.16719341278076172,0.0914255753159523,-0.16961155831813812,0.27108898758888245,0.2217331975698471,0.051661163568496704,-0.050416529178619385,-0.055038340389728546,-0.22604034841060638,0.22870700061321259,0.022783860564231873,-0.23303718864917755,0.22870700061321259,0.08064669370651245,-0.28395429253578186,-0.02551065944135189,-0.06596453487873077,0.2179020643234253,0.24459172785282135,0.21352876722812653,-0.2748759984970093,0.24413074553012848,0.13838525116443634,0.23771211504936218,0.274836927652359,-0.22794638574123383,0.28715208172798157,-0.023345062509179115,0.24279189109802246,0.012263529002666473,-0.06797180324792862,-0.22874608635902405,0.2246343493461609,0.267739862203598,-0.08300571143627167,-0.2369164377450943,-0.22882525622844696,0.23251508176326752,-0.07417396456003189,-0.13469043374061584,0.22870700061321259,-0.24725638329982758,0.2354302704334259,0.08954358100891113,0.07682958990335464,0.274836927652359,-0.029520170763134956,0.28893986344337463,-0.17062070965766907,0.26714861392974854,0.05278655141592026,-0.28153616189956665,-0.2172456830739975,-0.24207638204097748,-0.2641783058643341,0.274836927652359,-0.07340232282876968,-0.2671877145767212,-0.2748759984970093,0.274836927652359,-0.23075637221336365,0.289306640625,0.09891447424888611,0.21247054636478424,0.2336023896932602,0.274836927652359,-0.19946794211864471,0.02726737968623638,-0.07233506441116333,0.21081502735614777,0.274836927652359,-0.26437172293663025,0.25562813878059387,0.26714861392974854,-0.231183260679245,-0.13095450401306152,-0.1898956298828125,0.274836927652359,-0.22874608635902405,0.24617408215999603,0.274836927652359,0.2284294068813324,-0.2671877145767212,-0.2748759984970093,-0.27349308133125305,0.01739642396569252,-0.24746383726596832,-0.22874608635902405,-0.11290618032217026,-0.10692348331212997,0.06235887482762337,-0.2660492956638336,-0.26695922017097473,0.274836927652359,-0.20649786293506622,-0.22874608635902405,0.274836927652359,-0.019982924684882164,-0.22533786296844482,0.26714861392974854,-0.2748759984970093,0.14519771933555603,-0.22874608635902405,0.274836927652359,0.2694013714790344,0.22870700061321259,0.22870700061321259,-0.27894866466522217,0.26714861392974854,0.08627364039421082,-0.08756138384342194,-0.22874608635902405,0.24816931784152985,0.0993373692035675,-0.22874608635902405,-0.23552848398685455,-0.23957808315753937,-0.2226080596446991,-0.2748759984970093,0.23554952442646027,0.13775892555713654,-0.22767682373523712,-0.056405242532491684,-0.2748759984970093,-0.1945504993200302,0.04643506929278374,0.16757923364639282,-0.2748759984970093,0.2146661877632141,-0.22874608635902405,0.059269312769174576,0.09487289190292358,-0.0805695429444313,-0.15755096077919006,-0.08088821917772293,-0.059802453964948654,0.2306511402130127,0.24745279550552368,0.23005184531211853,0.2672748863697052,0.2460598349571228,-0.2696268856525421,-0.022285813465714455,-0.26979321241378784,-0.041311200708150864,-0.11175373941659927,0.17422935366630554,0.2663208544254303,0.2460598349571228,-0.23265036940574646,-0.27921122312545776,-0.22874608635902405,-0.24609792232513428,-0.17325930297374725,-0.22240763902664185,-0.04010363668203354,0.06888473033905029,0.21480047702789307,0.11881371587514877,0.18676497042179108,0.010670145973563194,-0.2748759984970093,0.2279694378376007,0.27177441120147705,0.1588386744260788,-0.0907401293516159,0.27674800157546997,-0.11871851980686188,-0.2504010796546936,-0.24654388427734375,0.16964763402938843,0.2297922968864441,0.035774439573287964,0.29081985354423523,0.22870700061321259,0.22870700061321259,-0.23558060824871063,-0.2799818515777588,0.26714861392974854,0.21972793340682983,-0.28949806094169617,-0.04717564955353737,0.22325442731380463,-0.23876334726810455,-0.09732811897993088,-0.16741789877414703,-0.014115468598902225,-0.2652726173400879,-0.2865878939628601,-0.0834326222538948,0.13718871772289276,0.034434594213962555,0.16700702905654907,-0.2748759984970093,0.2350233942270279,0.2701379656791687,0.19011907279491425,-0.1512024700641632,-0.08128806948661804,0.26842230558395386,-0.2671877145767212,-0.24609792232513428,-0.24344028532505035,-0.013900011777877808,0.22892345488071442,-0.2748759984970093,0.22870700061321259,0.13396187126636505,0.22989551723003387,-0.16659817099571228,0.09184647351503372,0.056051481515169144,0.22996367514133453,0.20041394233703613,-0.22874608635902405,0.14891061186790466,0.24274379014968872,-0.07228795439004898,-0.07066752016544342,-0.24609792232513428,0.2273360788822174,0.274836927652359,-0.037644416093826294,-0.2363341897726059,0.2296099066734314,-0.011802558787167072,0.274836927652359,-0.23674306273460388,-0.111569344997406,-0.24609792232513428,0.10477592051029205,-0.05945771932601929,-0.1838337481021881,-0.1302790641784668,-0.23269547522068024,-0.24305547773838043,0.28222861886024475,0.22870700061321259,0.26714861392974854,0.274836927652359,-0.02943098172545433,-0.22874608635902405,-0.26357904076576233,0.2097778171300888,-0.015567551366984844,-0.27327460050582886,0.2233947217464447,0.2579931616783142,0.22870700061321259,0.03304263949394226,-0.2671877145767212,0.274836927652359,0.04438471421599388,0.06917333602905273,-0.1423647105693817,0.22870700061321259,-0.25287729501724243,0.274836927652359,-0.1476840078830719,0.19443926215171814,-0.08788606524467468,0.18770495057106018,0.03734176605939865,-0.1614833027124405,-0.1293380707502365,-0.1501091718673706,0.05646035075187683,-0.23084554076194763,0.10299915075302124,0.0872797742486,0.13880915939807892,0.25171786546707153,0.22870700061321259,-0.2748759984970093,-0.01236374955624342,0.21241642534732819,-0.03010140359401703,-0.21910563111305237,-0.2514282464981079,-0.07784876227378845,0.2741945683956146,0.24926665425300598,0.26714861392974854,-0.2748759984970093,0.2700207233428955,0.2168307900428772,-0.12849026918411255,0.23631715774536133,0.274836927652359,-0.0666479840874672,0.26569753885269165,0.057380303740501404,0.24576520919799805,-0.11723737418651581,0.24701987206935883,0.15712504088878632,-0.22874608635902405,-0.2671877145767212,0.27334076166152954,-0.1824367791414261,0.22870700061321259,-0.04018080234527588,-0.24609792232513428,-0.2748759984970093,0.18424460291862488,0.02863428182899952,0.022519299760460854,0.045928992331027985,-0.09638110548257828,-0.1500881165266037,0.274836927652359,0.2585693895816803,-0.044934894889593124,-0.2057202160358429,-0.23300111293792725,-0.24252234399318695,0.2460598349571228,0.2460598349571228,-0.07212962210178375,0.025969624519348145,0.2313646376132965,0.22870700061321259,-0.13747534155845642,0.1424739509820938,-0.28178468346595764,0.2576363980770111,-0.16521623730659485,-0.24841885268688202,0.02626725658774376,-0.22874608635902405,0.26714861392974854,-0.2748759984970093,-0.2743208408355713,0.2737245559692383,-0.22086235880851746,0.22870700061321259,-0.2823067903518677,-0.20451365411281586,0.22550921142101288,0.018102925270795822,0.13555625081062317,0.19593141973018646,-0.2573728561401367,-0.1880386769771576,-0.2359473705291748,-0.005472120828926563,-0.21164779365062714,0.11558987200260162,0.274836927652359,0.039358045905828476,0.06327281147241592,0.1743856817483902,-0.2789817452430725,-0.13955575227737427,0.22870700061321259,0.10522586852312088,0.027656204998493195,-0.23021219670772552,-0.058646999299526215,-0.02497151494026184,0.06721718609333038,0.026837466284632683,0.015893233940005302,-0.10286185890436172,0.22870700061321259,0.274836927652359,0.011799543164670467,-0.2671877145767212,-0.017936579883098602,-0.231125146150589,0.0034037211444228888,-0.29084891080856323,0.035256337374448776,0.053521107882261276,0.2053704559803009,0.22870700061321259,-0.01677611656486988,-0.21590082347393036,-0.22874608635902405,0.18079128861427307,0.24114741384983063,-0.0513044148683548,-0.020889850333333015,0.10972742736339569,-0.10911913961172104,0.09225333482027054,0.05736827850341797,-0.28385308384895325,-0.10667996108531952,0.28397130966186523,0.26714861392974854,-0.2748759984970093,-0.260223925113678,0.04994051158428192,0.023449275642633438,0.07795197516679764,-0.2510644495487213,0.1715737134218216,-0.2313786745071411,-0.20421601831912994,0.24781957268714905,-0.12647098302841187,0.052357640117406845,-0.045874886214733124,0.19165533781051636,0.009293223731219769,-0.20260559022426605,0.16012340784072876,0.09699039161205292,-0.27760881185531616,0.2815852463245392,-0.2851698696613312,0.23424875736236572,-0.24754701554775238,-0.24609792232513428,0.05055481567978859,-0.21583768725395203,-0.1711057424545288,-0.12089213728904724,-0.20539051294326782,-0.2657396197319031,0.28240299224853516,-0.011299491859972477,-0.04061872884631157,-0.2839212119579315,0.26714861392974854,-0.14488306641578674,-0.03631860390305519,-0.22874608635902405,-0.22874608635902405,0.057831261307001114,0.2632693648338318,0.07585752755403519,0.006771871354430914,-0.2889368534088135,0.006073388271033764,0.05178442597389221,-0.003573089139536023,0.1725638061761856,-0.27310726046562195,0.14746354520320892,0.11402254551649094,0.24182985723018646,-0.08571546524763107,-0.10170440375804901,0.1565137505531311,0.24025149643421173,-0.24320478737354279,0.26714861392974854,-0.1066889837384224,0.28064224123954773,0.00710056908428669,-0.2748759984970093,-0.23894673585891724,-0.26407307386398315,-0.2748759984970093,-0.2748759984970093,-0.049214981496334076,-0.26597511768341064,-0.22874608635902405,-0.038469165563583374,0.05821707844734192,-0.23213627934455872,-0.03779273107647896,-0.19074241816997528,0.03137008845806122,0.2698122560977936,-0.22874608635902405,0.0740206316113472,-0.26848745346069336,0.26965993642807007,0.18552233278751373,-0.05554741993546486,0.13035421073436737,-0.23437905311584473,0.22248780727386475,0.2747798264026642,0.23758183419704437,-0.23476286232471466,0.23961515724658966,0.0943547934293747,-0.22386972606182098,0.1889415830373764,0.2207290679216385,0.2400420606136322,0.018082883208990097,0.025396408513188362,-0.2578278183937073,0.21755832433700562,0.18481482565402985,0.056707873940467834,-0.2555239498615265,-0.2748759984970093,0.03748106211423874,-0.19151605665683746,0.274836927652359,-0.22796142101287842,-0.03148433938622475,-0.2748759984970093,0.0013974623288959265,0.11638756096363068,-0.08570543676614761,-0.050211090594530106,-0.2569639980792999,-0.25584161281585693,0.24355551600456238,0.2460598349571228,-0.22303396463394165,0.1432395726442337,0.17953060567378998,-0.2511165738105774,-0.17199161648750305,0.04290156811475754,0.27637219429016113,0.274836927652359,0.07715427875518799,-0.21021775901317596,-0.189146026968956,0.22870700061321259,0.25299155712127686,0.09015186876058578,-0.22937943041324615,-0.2102448046207428,-0.1667424738407135,-0.20875665545463562,-0.02175368368625641,-0.17297670245170593,0.274836927652359,-0.2009190171957016,-0.20899516344070435,0.2630138099193573,-0.10579608380794525,-0.16029077768325806,0.14818206429481506,-0.22978027164936066,0.11111236363649368,-1.9545792383723892e-05,-0.22874608635902405,-0.19096289575099945,-0.06351533532142639,-0.04419131577014923,0.1903766244649887,0.2740362286567688,-0.2748759984970093,-0.07316382229328156,0.1994829624891281,-0.2739039361476898,0.14268839359283447,0.20624832808971405,-0.10859302431344986,0.2703503966331482,-0.2792071998119354,-0.09938347339630127,-0.2748759984970093,0.28514280915260315,0.0032093084882944822,-0.25438353419303894,-0.24609792232513428,0.028309592977166176,0.233076274394989,-0.15437822043895721,0.274836927652359,-0.20300345122814178,0.2460598349571228,0.26040929555892944,-0.2312474101781845,0.10507054626941681,0.2723957598209381,-0.2671877145767212,-0.052364662289619446,0.026282288134098053,-0.12554201483726501,0.06886468082666397,0.2702261209487915,0.13248874247074127,-0.23521080613136292,0.13549913465976715,-0.2426425963640213,0.2460598349571228,-0.1292869597673416,-0.06197807565331459,0.22870700061321259,0.27377966046333313,-0.05068910866975784,-0.21950146555900574,-0.13892842829227448,0.2651824355125427,-0.21929703652858734,0.07774753868579865,-0.2702201306819916,-0.09430068731307983,0.18028821051120758,0.28988587856292725,0.24989697337150574,0.07274191826581955,0.26714861392974854,-0.02730146050453186,-0.09996972233057022,-0.26335257291793823,0.08885512501001358,-0.23124338686466217,-0.054715655744075775,0.10495830327272415,0.22870700061321259,-0.22959588468074799,-0.15322677791118622,-0.26222214102745056,0.231460839509964,0.05061594396829605,0.23301613330841064,-0.18330763280391693,0.08223506063222885,-0.2696208655834198,-0.03761635720729828,-0.24609792232513428,0.0014605963369831443,-0.04803146421909332,-0.27282264828681946,0.0836590901017189,-0.2813377380371094,0.11903218179941177,0.16498172283172607,-0.1632320135831833,-0.18737025558948517,-0.13065186142921448,-0.24273578822612762,0.2503158748149872,-0.14456839859485626,0.19652067124843597,-0.24609792232513428,-0.05241677537560463,0.22935837507247925,-0.07104231417179108,-0.06520693004131317,-0.11592960357666016,0.2553405463695526,-0.22019194066524506,-0.2425123155117035,0.1621467024087906,-0.047093477100133896,0.08867473155260086,-0.21621748805046082,0.13541696965694427,-0.015567551366984844,-0.2748759984970093,0.10385295748710632,-0.25803425908088684,-0.22874608635902405,-0.22114194929599762,-0.26125410199165344,-0.037583284080028534,0.2719968855381012,-0.2817426025867462,-0.2204504758119583,0.21617038547992706,0.05965513363480568,-0.23501941561698914,-0.2748759984970093,0.2430073618888855,-0.2671877145767212,0.022201625630259514,0.2170492559671402,0.056424275040626526,0.25495171546936035,0.036625247448682785,-0.2640199661254883,-0.1584218144416809,-0.0896698608994484,0.274836927652359,0.23832741379737854,-0.22299186885356903,-0.19559872150421143,-0.1772327423095703,-0.271281361579895,-0.23949390649795532,-0.10576401650905609,-0.03072773478925228,-0.2748759984970093,-0.06415770202875137,0.2522610127925873,0.19945590198040009,0.2460598349571228,0.12011147290468216,0.26714861392974854,0.015463321469724178,0.004319665487855673,0.1451917141675949,0.28426793217658997,0.20895005762577057,-0.06906812638044357,0.03213471174240112,0.22870700061321259,0.0465322770178318,-0.19925348460674286,-0.21099340915679932,-0.247269406914711,0.17240747809410095,0.274836927652359,-0.2254410684108734,0.221070796251297,-0.10721810907125473,-0.11245622485876083,-0.06336401402950287,-0.2748759984970093,0.27359527349472046,-0.21416012942790985,-0.014068368822336197,-0.11859625577926636,-0.23536613583564758,-0.23526392877101898,0.14751464128494263,0.2549126148223877,-0.23143678903579712,-0.11812727153301239,0.12434445321559906,0.061524100601673126,-0.22874608635902405,-0.23653362691402435,-0.0976978987455368,-0.2639438211917877,0.1137920618057251,-0.2598952054977417,-0.2748759984970093,-0.0556827075779438,0.23699258267879486,0.001720147323794663,0.2460598349571228,0.1672024428844452,0.25286829471588135,-0.0510127954185009,0.11191607266664505,-0.03792601451277733,0.2359483689069748,0.19155412912368774,0.12375420331954956,-0.05291082710027695,0.22978328168392181,-0.2195405513048172,0.22527270019054413,0.23526892066001892,-0.20832374691963196,0.2746044397354126,-0.2476041316986084,0.24133780598640442,-0.11305349320173264,0.2821684777736664,-0.08131111413240433,0.2632022202014923,-0.18910594284534454,0.26714861392974854,0.22870700061321259,0.13025599718093872,-0.27498823404312134,0.2573087215423584,-0.20016241073608398,0.01552946213632822,-0.13718271255493164,-0.2748759984970093,0.06300925463438034,-0.22106076776981354,0.055338967591524124,-0.22874608635902405,0.022176573053002357,-0.01202202495187521,0.146700918674469,-0.2671877145767212,-0.02778749167919159,0.0799001082777977,-0.23968230187892914,-0.2671877145767212,-0.0742320865392685,-0.25325915217399597,-0.2796090841293335,-0.24846094846725464,0.28639546036720276,0.271689236164093,-0.09995468705892563,0.2223394811153412,-0.2671877145767212,0.02152518928050995,-0.02999217063188553,-0.2831736207008362,-0.2524193525314331,-0.24391929805278778,-0.08292854577302933,-0.23979955911636353,0.2666766047477722,0.06811509281396866,0.274836927652359,0.06464071571826935,0.25916263461112976,-0.2748759984970093,0.06360652297735214,-0.021322768181562424,0.26714861392974854,-0.2748759984970093,0.050273217260837555,0.22870700061321259,-0.06763006746768951,0.2514171898365021,0.07886090129613876,-0.2724127769470215,-0.24315668642520905,0.22870700061321259,-0.2490241378545761,0.2335863560438156,0.20780663192272186,0.09531883895397186,0.012270543724298477,-0.14746254682540894,0.023260874673724174,0.26714861392974854,-0.10363450646400452,0.09663262963294983,-0.18519966304302216,-0.1964014172554016,-0.2748759984970093,0.2515975832939148,-0.2654299736022949,-0.06873441487550735,-0.24751393496990204,-0.22874608635902405,0.25913459062576294,-0.21394968032836914,0.19814713299274445,-0.2155109941959381,0.04128413274884224,-0.03472020849585533,-0.24245017766952515,0.06824637204408646,-0.20238913595676422,0.26653629541397095,-0.11766228079795837,0.17826992273330688,0.20874463021755219,0.11134686321020126,0.100321464240551,0.2600395381450653,0.10312341153621674,0.24181783199310303,-0.056330081075429916,-1.9545792383723892e-05,0.08407697081565857,0.06833255290985107,0.2189352661371231,0.05875522270798683,-0.20604589581489563,0.03373410552740097,-0.2836456298828125,0.274836927652359,-0.23094776272773743,-0.2648557424545288,0.04622061178088188,-0.2679022252559662,-0.2748759984970093,-0.22874608635902405,0.23187772929668427,0.19212134182453156,0.1990841180086136,-0.16495968401432037,-0.013741674832999706,-0.16067257523536682,0.26571956276893616,-0.17605023086071014,0.22127622365951538,-0.2636892795562744,0.209617480635643,0.07296839356422424,-0.17600414156913757,0.07509791851043701,-0.0880664512515068,0.22870700061321259,0.00848149973899126,0.2670654356479645,-0.2671877145767212,0.0447765477001667,0.22020795941352844,-0.13411922752857208,-0.25224095582962036,-0.08403690159320831,0.22870700061321259,-0.2239338755607605,-0.28568094968795776,-0.25724056363105774,0.2618383467197418,0.2646142244338989,0.11971663683652878,0.229406476020813,0.025142868980765343,-0.013605386018753052,-0.22874608635902405,-0.199228435754776,0.08523844182491302,0.08369617164134979,-0.2487335205078125,-0.028001947328448296,-0.06676523387432098,0.0677623450756073,0.22001755237579346,0.26714861392974854,0.22870700061321259,-0.233047217130661,0.24588747322559357,-0.03271094337105751,0.017624910920858383,-0.06434109061956406,-0.22874608635902405,0.15565091371536255,-0.24240408837795258,-0.10145387053489685,0.08933313190937042,-0.057191912084817886,-0.0065163373947143555,-0.04024694114923477,-0.015606634318828583,0.274836927652359,-0.12860049307346344,-0.00566653348505497,0.22870700061321259,0.1542639583349228,0.10817613452672958,0.015401190146803856,-0.2639097273349762,-0.0016560197109356523,0.2779795825481415,-0.08082708716392517,-0.05137255787849426,-0.2671877145767212,-0.25641685724258423,0.2732255160808563,0.12274305522441864,0.22870700061321259,-0.09758666157722473,0.19617795944213867,-0.24859322607517242,0.244966521859169,-0.2577677071094513,0.2379365861415863,0.2324068546295166,0.10195893794298172,-0.20098115503787994,-0.22874608635902405,0.2509361803531647,0.008349219337105751,-0.02624722197651863,-0.09677894413471222,-0.2539806663990021,-0.016823217272758484,-0.02895597368478775,-0.22874608635902405,-0.24609792232513428,0.26714861392974854,0.2908168435096741,-0.17441175878047943,0.28351736068725586,0.016378263011574745,0.2475820779800415,0.18675394356250763,0.24060726165771484,0.08927500993013382,-0.2201298028230667,0.14358730614185333,0.274836927652359,0.19212134182453156,-0.11925866454839706,-0.035359565168619156,0.20794492959976196,-0.07937199622392654,-0.06842074543237686,-0.1464604139328003,-0.03915061429142952,0.04396382346749306,-0.2748759984970093,0.16744594275951385,-0.2229728400707245,0.22870700061321259,0.1378060281276703,-0.2054576426744461,-0.05738331750035286,-0.2671877145767212,0.11991104483604431,-0.2395830899477005,0.039784952998161316,-0.24609792232513428,0.2415131777524948,0.07166863977909088,-0.03895620256662369,0.20220473408699036,0.2747637629508972,0.016266025602817535,0.16490155458450317,-0.051708269864320755,-0.08093030750751495,0.24530723690986633,-0.027838600799441338,-0.2248508185148239,0.03885898366570473,0.24078765511512756,0.22870700061321259,-0.24845995008945465,-0.09052466601133347,0.26714861392974854,0.24293319880962372,0.2799517810344696,-0.2748759984970093,0.11354152113199234,0.21174098551273346,-0.23171940445899963,0.16021059453487396,-0.25915366411209106,-0.24609792232513428,0.22870700061321259,0.005247635766863823,-0.1751803755760193,0.26869288086891174,-0.2250622659921646,-0.1633703112602234,-0.2415492683649063,-0.04054056480526924,0.2007225900888443,-0.021480102092027664,-0.2748759984970093,-0.2671877145767212,-0.07743187993764877,-0.023307982832193375,-0.27434489130973816,0.23038354516029358,-0.2031257003545761,0.037180423736572266,-0.25520625710487366,-0.2438531517982483,-0.0006859604618512094,-0.09225234389305115,-0.18755966424942017,0.2716100811958313,0.25822165608406067,0.274836927652359,-0.2748759984970093,-0.06171451136469841,0.2582196593284607,-0.2612580955028534,0.0210040844976902,-0.08906857669353485,0.01552946213632822,0.2460598349571228,0.22870700061321259,0.02521001175045967,0.253740131855011,0.04352288693189621,0.2823147773742676,0.24840080738067627,-0.27896371483802795,0.2140979766845703,0.20978783071041107,0.0656568706035614,0.02231486700475216,0.26749536395072937,-0.009344340302050114,0.23398521542549133,0.21477040648460388,-0.2748759984970093,-0.2518611550331116,0.12178301811218262,0.26714861392974854,0.2460598349571228,0.274836927652359,0.2822917699813843,0.14878834784030914,-0.1066007986664772,0.20390334725379944,0.274836927652359,0.1974065601825714,0.01552946213632822,-0.06885567307472229,0.2042851448059082,-0.04339361935853958,0.05135250836610794,-0.08062364906072617,-0.22874608635902405,0.2210788130760193,-0.2613683342933655,0.22870700061321259,-0.0760459378361702,0.11618813872337341,0.2679022252559662,-0.26610037684440613,-0.26999062299728394,0.14254911243915558,0.21637383103370667,0.23016609251499176,-0.20010828971862793,0.2535116672515869,0.2460598349571228,-0.06464072316884995,0.12043917179107666,0.16785483062267303,0.05310121923685074,-0.16796807944774628,-0.0468599796295166,0.22870700061321259,-0.24609792232513428,0.2374916523694992,0.15130369365215302,-0.24609792232513428,0.1943701058626175,-0.22874608635902405,-0.24710404872894287,-0.2808897793292999,-0.07929583638906479,-0.1615344136953354,0.1896771341562271,0.12191930413246155,0.2493307888507843,0.274836927652359,0.00017787329852581024,-0.05989064276218414,-0.0892690122127533,-0.05205701291561127,0.075550876557827,0.054398976266384125,-0.10768108814954758,-0.21219795942306519,0.06437515467405319,-0.056172747164964676,0.1204070970416069,-0.16659115254878998,0.274836927652359,-0.25344952940940857,0.020236453041434288,0.28577715158462524,0.12407989799976349,0.22870700061321259,0.042907580733299255,0.23523084819316864,0.2563556730747223,0.25635868310928345,-0.25224798917770386,-0.22874608635902405,0.26502910256385803,-0.2113220989704132,0.2460598349571228,0.07641270756721497,0.2460598349571228,-0.09098264575004578,-0.03980099409818649,0.26206180453300476,0.26714861392974854,-0.24609792232513428,0.25007838010787964,0.10078243911266327,-0.2746916115283966,0.16005927324295044,-0.2421034425497055,0.05787034332752228,-0.2642444372177124,0.24905018508434296,0.0887158215045929,0.26857665181159973,0.014661619439721107,0.05592120438814163,0.16451071202754974,-0.24735058844089508,0.052941881120204926,-0.2517368793487549,-0.24966049194335938,-0.25703415274620056,0.2489159107208252,-0.24609792232513428,-0.16421310603618622,-0.07283110916614532,-0.21960768103599548,0.26128917932510376,0.21791109442710876,-0.22194766998291016,0.18985353410243988,0.20433124899864197,-0.029853878542780876,0.274836927652359,0.2322675585746765,0.014911149628460407,-0.16256259381771088,-1.9545792383723892e-05,0.2362319827079773,-0.2565370798110962,-0.239183247089386,0.274836927652359,0.274836927652359,0.26442381739616394,-0.054360903799533844,-0.2748759984970093,0.2509031295776367,0.22658750414848328,0.02511380799114704,0.12368806451559067,-0.25141221284866333,0.23598845303058624,0.019229315221309662,-0.010531861335039139,-0.2615106403827667,0.25651001930236816,-0.14346204698085785,-0.2671877145767212,0.22626681625843048,0.26714861392974854,-0.009662014432251453,0.07861939072608948,-0.06729135662317276,0.1379573494195938,-0.030072342604398727,-0.2291058450937271,0.06788961589336395,0.09042845666408539,-0.2584751844406128,0.25560107827186584,-0.14214524626731873,0.26605427265167236,-0.22874608635902405,0.24297527968883514,-0.2817075252532959,0.22870700061321259,-0.004455963149666786,-0.22247076034545898,-0.03807833790779114,0.0726146399974823,-0.015567551366984844,-0.2671877145767212,-0.01916218176484108,0.26714861392974854,-0.24609792232513428,-0.19253121316432953,-0.16052627563476562,0.08496686816215515,-0.23008893430233002,-0.18306010961532593,-0.22604836523532867,-0.024743029847741127,0.012191376648843288,0.25051429867744446,-0.2748759984970093,0.019730379804968834,-0.019392671063542366,-0.10589329898357391,-0.07343639433383942,0.0412941537797451,0.10331480950117111,-0.18710669875144958,0.2462722808122635,0.274836927652359,0.03386037424206734,0.2596396505832672,0.23737139999866486,-0.2671877145767212,0.27743643522262573,0.0841982364654541,0.2460598349571228,-0.03094719909131527,-0.10222651064395905,0.23733831942081451,-0.19875642657279968,-0.24662603437900543,-0.27264127135276794,0.2300167828798294,0.2460598349571228,0.2880770266056061,-0.24609792232513428,-0.14723004400730133,0.1720537394285202,-0.2726232409477234,-0.0428895503282547,0.15292011201381683,-0.09950272738933563,-0.2748759984970093,-0.24609792232513428,-0.11015834659337997,-0.2496003657579422,-0.2622893154621124,0.0699259340763092,-0.07664521038532257,-0.07276497036218643,0.07415090501308441,0.26397886872291565,-0.21164779365062714,-0.1075047180056572,0.2030545473098755,0.2460598349571228,-0.26413121819496155,-0.14390398561954498,-0.08210880309343338,-0.1412944495677948,0.2460598349571228,-0.22141051292419434,0.042007673531770706,0.08788806200027466,-0.2321954220533371,0.24436023831367493,0.03123380057513714,0.2655201554298401,0.24778048694133759,-0.2541840970516205,0.2040005475282669,-0.21989630162715912,-0.2536660134792328,0.1969205141067505,-0.06065426021814346,0.16696593165397644,0.2146160900592804,-0.2715319097042084,0.023183710873126984,-0.13716568052768707,0.2514803409576416,0.08757741004228592,0.26714861392974854,-0.062262676656246185,0.2460598349571228,-0.22874608635902405,0.23065613210201263,-0.02995409071445465,0.12624047696590424,0.24305245280265808,0.0887378677725792,-0.19866523146629333,0.09183143824338913,0.06544943153858185,-0.23436400294303894,-0.2671877145767212,-0.19881856441497803,0.24913637340068817,-0.22874608635902405,-0.06282787770032883,-0.2748759984970093,0.08273211866617203,0.2514171898365021,-0.1236049011349678,0.050671059638261795,0.23819012939929962,0.20227989554405212,0.12834495306015015,0.22986845672130585,-0.05642227828502655,0.24219362437725067,0.2460598349571228,0.0936262458562851,0.09771092236042023,-0.17412914335727692,-0.04569350183010101,0.22870700061321259,-0.21373723447322845,0.274809867143631,0.09432473033666611,0.26972708106040955,-0.2600976526737213,0.2552964389324188,0.04019983112812042,-0.10907505452632904,-0.05912401154637337,0.27777916193008423,0.274836927652359,0.008253015577793121,0.22870700061321259,-0.1167282983660698,0.2305779755115509,0.08731184154748917,0.2540868818759918,0.16907942295074463,-0.2501886188983917,-0.25530847907066345,-0.02158232033252716,0.274836927652359,0.10241289436817169,-0.04156874865293503,0.03708622232079506,-0.11465690284967422,0.07825662195682526,-0.052815619856119156,0.274836927652359,0.15375488996505737,-0.24609792232513428,-0.17875495553016663,0.23942676186561584,0.10099789500236511,-0.2722434401512146,0.07846105098724365,-0.17374834418296814,0.002674172632396221,0.24015630781650543,0.20243923366069794,0.06388010084629059,0.22870700061321259,-0.22874608635902405,-0.28351137042045593,-0.23046374320983887,-0.23308229446411133,-0.15268763899803162,-0.08699718117713928,-0.23337692022323608,0.02608587220311165,-0.2905873954296112,-0.07767640054225922,-0.2648847997188568,-0.24351945519447327,-0.036002933979034424,-0.2069077342748642,0.06128759682178497,0.026183076202869415,-0.14320750534534454,0.22351397573947906,0.26674774289131165,-0.16354267299175262,-0.08229520916938782,0.22346988320350647,0.025876427069306374,-0.07539454847574234,0.0593985877931118,0.26769477128982544,0.2662998139858246,0.21732082962989807,0.01858093962073326,0.23921430110931396,-0.2748759984970093,-0.02406959980726242,0.2658037543296814,-0.23537015914916992,0.14949184656143188,-0.06231779232621193,-0.22672781348228455,0.2542863190174103,-0.2748759984970093,-0.11456470936536789,-0.11748690158128738,0.23343104124069214,0.2114102840423584,-0.18016496300697327,-0.24609792232513428,-0.2558285892009735,0.016164811328053474,0.22870700061321259,-0.22944557666778564,-0.22874608635902405,-0.24473802745342255,-0.09189558029174805,-0.08463316410779953,0.0633108988404274,-0.2422858327627182,-0.16176390647888184,0.11530927568674088,-0.0914326012134552,0.02908223122358322,-0.26670968532562256,0.24174167215824127,0.15607081353664398,0.23791053891181946,-0.043557967990636826,0.010760338045656681,0.2670043110847473,0.11637253314256668,0.018104929476976395,-0.2748759984970093,-0.2671877145767212,-0.2671877145767212,-0.04345575347542763,-0.018098924309015274,-0.2797844409942627,0.23457244038581848,-0.27760082483291626,-0.21778181195259094,-0.2748759984970093,-0.28627723455429077,0.2179591804742813,0.24832163751125336,-0.07162655889987946,-0.28787660598754883,0.13852055370807648,-0.23895175755023956,0.057438429445028305,-0.24879464507102966,0.017814312130212784,0.2686888873577118,0.08796422928571701,-0.23269246518611908,0.19649261236190796,0.028225412592291832,-0.1352335810661316,-0.28194302320480347,-0.012442918494343758,0.26883718371391296,-0.2094060331583023,-0.08185426890850067,0.24951216578483582,0.10614783316850662,0.26115289330482483,0.21101443469524384,-0.22874608635902405,-0.06161830946803093,-0.031194724142551422,-0.115693099796772,0.049576740711927414,-0.21802634000778198,-0.06623110175132751,0.2460598349571228,-0.0047074975445866585],\"y\":[-0.15818028151988983,0.862852156162262,-0.8807020783424377,0.017567794770002365,-0.8878222107887268,-0.4425469636917114,0.4429227411746979,-0.5279371738433838,-0.07418297976255417,-0.8858780860900879,-0.44072309136390686,-0.0014395576436072588,0.8663305640220642,0.45273756980895996,-0.5175511837005615,-0.8859522342681885,-0.23309531807899475,-0.2186516523361206,0.2300678938627243,0.6669402718544006,-0.2055237889289856,-0.07267678529024124,-0.22884029150009155,-0.27038148045539856,-0.065889373421669,0.8822423219680786,-0.6724138855934143,-0.6682670712471008,0.00772489607334137,-0.22248178720474243,0.21631669998168945,0.4518887996673584,0.6477705836296082,0.2364073544740677,0.6619336605072021,-0.2250562608242035,-0.12567928433418274,-0.698139488697052,-0.23072628676891327,-0.34626156091690063,-0.07953032851219177,-0.06696765869855881,-0.7137135863304138,-0.11124465614557266,-0.23309531807899475,-0.1598518341779709,-0.0923284962773323,0.8083504438400269,-0.07583648711442947,-0.1175580620765686,-0.20490747690200806,-0.23298609256744385,-0.11365476995706558,-0.15874749422073364,-0.15934476256370544,-0.09748142957687378,-0.10843870043754578,-0.16010136902332306,0.09993865340948105,0.3273513913154602,0.5793603658676147,0.8012614250183105,0.35423848032951355,-0.4065525531768799,-0.1490529179573059,0.13315817713737488,-0.16159051656723022,-0.08704027533531189,-0.0699770525097847,-0.23006990551948547,0.5459133982658386,-0.23309531807899475,-0.12266389280557632,-0.22814881801605225,-0.5495681166648865,-0.7876555323600769,0.1257183849811554,-0.5508488416671753,0.7636045217514038,-0.06964635103940964,-0.7652309536933899,0.7698908448219299,-0.15773333609104156,0.3319201171398163,-0.3222305178642273,0.4540604054927826,-0.1602075845003128,-0.2240150421857834,-0.10112015902996063,0.1003575325012207,0.5480669140815735,0.7742370367050171,-0.173364520072937,-0.1474284678697586,-0.32644549012184143,0.5601506233215332,0.35394588112831116,-0.23309531807899475,-0.06594449281692505,-0.09316026419401169,-0.15743571519851685,-0.07418698817491531,-0.12981507182121277,-0.22161094844341278,-0.07862841337919235,-0.16391345858573914,-0.15833061933517456,-0.23309531807899475,-0.1622258871793747,-0.6194143891334534,-0.1291997730731964,-0.15985986590385437,-0.07304956763982773,-0.08041120320558548,0.4529159367084503,0.00017386703984811902,-0.7967308163642883,-0.0787997841835022,-0.07052120566368103,0.6770597696304321,-0.20113947987556458,-0.44488590955734253,-0.16093112528324127,-0.6134858131408691,-0.07912546396255493,-0.07032178342342377,-0.13932126760482788,-0.15737858414649963,0.34838607907295227,-0.15621310472488403,-0.06702277809381485,-0.8073663711547852,0.8845261931419373,-0.23309531807899475,-0.018317386507987976,-0.08010154217481613,-0.2898007333278656,-0.15821535885334015,-0.23309531807899475,-0.07365986704826355,-0.16209961473941803,-0.11176376044750214,0.2351967692375183,-0.15980374813079834,-0.15703485906124115,-0.1580369770526886,-0.07555990666151047,-0.16356171667575836,-0.1625174880027771,-0.0777796134352684,-0.2644649147987366,-0.0679006427526474,-0.06879553943872452,-0.1555446982383728,-0.14559657871723175,-0.07972173392772675,-0.07948824018239975,-0.0781874805688858,-0.1664588749408722,-0.15621310472488403,-0.16258364915847778,-0.27922824025154114,0.7477027177810669,-0.09014085680246353,-0.35186946392059326,0.14325359463691711,-0.1581091284751892,-0.23309531807899475,0.8729445934295654,0.5244778394699097,0.847166895866394,-0.22151975333690643,-0.8343557119369507,-0.15621310472488403,-0.16501981019973755,0.2762589454650879,-0.3786032199859619,-0.44735315442085266,0.15770027041435242,-0.09781514108181,-0.23309531807899475,-0.23309531807899475,-0.717003583908081,-0.3860490024089813,0.3027281165122986,-0.14965319633483887,-0.15840476751327515,-0.16175487637519836,-0.15972456336021423,-0.15934476256370544,-0.16921672224998474,-0.15621310472488403,-0.1623561531305313,0.7133508324623108,-0.23309531807899475,-0.159551203250885,0.38424918055534363,-0.08030998706817627,0.19005194306373596,0.5015822649002075,-0.09242971241474152,-0.31657353043556213,-0.6060961484909058,-0.15970852971076965,-0.3954770267009735,-0.15840476751327515,0.07503177225589752,-0.1677546203136444,-0.6056692600250244,-0.5576342344284058,0.0649503767490387,0.1902383416891098,-0.021843871101737022,-0.058273203670978546,0.8505871891975403,-0.5120053887367249,-0.03200043365359306,0.7961836457252502,-0.23309531807899475,0.6088670492172241,-0.08466523140668869,-0.5130716562271118,-0.7268123626708984,-0.0765409842133522,-0.06694361567497253,-0.2330331951379776,-0.081698939204216,0.841301441192627,-0.7522283792495728,-0.4931022822856903,0.4819926619529724,0.26270920038223267,-0.08562326431274414,-0.15621310472488403,-0.18227243423461914,-0.2261555939912796,-0.16813743114471436,0.7762282490730286,-0.16372105479240417,-0.8567101359367371,-0.33307555317878723,-0.09153782576322556,0.05724802613258362,-0.8128079175949097,-0.06429498642683029,-0.10567884147167206,-0.2643316388130188,-0.22941049933433533,-0.1572873890399933,0.5937469005584717,0.03860144689679146,0.28805097937583923,0.6963908076286316,-0.08490974456071854,-0.23309531807899475,-0.1606164574623108,-0.1638352870941162,0.3909373879432678,0.599084198474884,-0.1630556434392929,-0.08839114010334015,-0.044459883123636246,-0.0676240548491478,-0.15621310472488403,-0.11340925097465515,-0.2055598646402359,0.5143172740936279,-0.19278575479984283,-0.23032042384147644,-0.23110811412334442,-0.26268312335014343,-0.09802860021591187,-0.07446859031915665,-0.4590439796447754,-0.1024780422449112,-0.11046800762414932,0.7164433598518372,0.09258905053138733,-0.15621310472488403,0.4170658588409424,0.8037316799163818,-0.4674137532711029,-0.06726228445768356,-0.1589990258216858,-0.14275053143501282,0.29792293906211853,-0.07456979900598526,-0.16847915947437286,0.3790651857852936,0.19104906916618347,-0.06544943898916245,-0.16631655395030975,0.6443713903427124,-0.5592567324638367,-0.4755249619483948,-0.08384649455547333,-0.20671632885932922,-0.23309531807899475,-0.16116963326931,-0.27763986587524414,-0.08985424786806107,-0.29278305172920227,-0.15881364047527313,-0.16848015785217285,-0.6296451091766357,-0.15792375802993774,-0.6110756993293762,-0.16789692640304565,0.6206931471824646,-0.2812996506690979,0.07618321478366852,-0.16093212366104126,-0.861046314239502,0.8326420783996582,-0.15833361446857452,0.297553151845932,-0.7704119086265564,0.1570448875427246,-0.6503681540489197,-0.5873884558677673,-0.23024627566337585,-0.07891201972961426,-0.7407609820365906,-0.06673416495323181,-0.3855699896812439,-0.1656711995601654,-0.1609281301498413,-0.1597265750169754,0.054828889667987823,-0.15621310472488403,-0.18148276209831238,-0.08087217807769775,0.6164701581001282,-0.06984677910804749,-0.22629688680171967,0.8432495594024658,-0.23309531807899475,-0.20349249243736267,-0.1487332433462143,-0.21680773794651031,-0.6297673583030701,-0.08707133680582047,-0.23309531807899475,-0.1586933732032776,-0.0798049122095108,0.8344859480857849,0.5068835616111755,0.3994755148887634,-0.08432450890541077,-0.23309531807899475,-0.15621310472488403,-0.076890729367733,-0.6890702843666077,-0.4979325234889984,-0.8228392004966736,-0.8219302892684937,-0.21275314688682556,-0.1453240066766739,-0.15621310472488403,-0.0707346647977829,-0.18678000569343567,-0.08882105350494385,-0.6629127860069275,-0.07428519427776337,-0.07582646608352661,-0.09365732222795486,0.3816035985946655,-0.17185533046722412,-0.3898240327835083,-0.1578676402568817,-0.16229502856731415,-0.31567060947418213,-0.23309531807899475,-0.1567111760377884,-0.07029272615909576,-0.23309531807899475,-0.21522940695285797,-0.06769821792840958,-0.019337551668286324,0.7334885597229004,-0.45817112922668457,-0.6812296509742737,-0.362554132938385,-0.10001180320978165,-0.6696911454200745,-0.23309531807899475,-0.08370218425989151,-0.16000016033649445,-0.11367481201887131,0.6562185287475586,-0.07626239210367203,-0.0705893486738205,0.16099926829338074,-0.18248789012432098,0.8450784087181091,0.0454179085791111,0.24650777876377106,-0.06469984352588654,0.580432653427124,-0.08980013430118561,-0.14000670611858368,-0.15915235877037048,-0.16119569540023804,-0.11591657251119614,-0.7865381836891174,-0.1025572195649147,-0.1128520667552948,-0.07260362803936005,0.49804070591926575,0.8512194752693176,0.3026449382305145,-0.15621310472488403,-0.15094392001628876,-0.06403043121099472,0.7166948914527893,-0.16273295879364014,-0.37996211647987366,-0.20135995745658875,-0.08538475632667542,0.5288461446762085,-0.3519456088542938,-0.07066350430250168,-0.8477561473846436,-0.5647543668746948,0.7975585460662842,-0.5702500343322754,-0.23309531807899475,-0.17223411798477173,-0.08007949590682983,-0.16080786287784576,-0.06727631390094757,-0.19915226101875305,-0.15759003162384033,-0.08106658607721329,-0.5629124641418457,-0.21572846174240112,0.5090541243553162,-0.21276114881038666,-0.8853389024734497,-0.23309531807899475,-0.16982901096343994,-0.30322620272636414,-0.23309531807899475,-0.08007247745990753,0.6355897784233093,-0.15621310472488403,0.3984433114528656,-0.16066455841064453,-0.2762860059738159,-0.20031172037124634,-0.8209663033485413,-0.5065387487411499,-0.23309531807899475,0.203395277261734,0.7457516193389893,-0.043811507523059845,-0.427933931350708,0.43708935379981995,-0.5036075711250305,-0.0505247563123703,-0.1565718799829483,-0.1598949283361435,0.8855363130569458,-0.19207023084163666,-0.15786662697792053,-0.07694684714078903,0.7953779101371765,-0.06995700299739838,0.8304674029350281,-0.39516738057136536,-0.7449539303779602,-0.06766814738512039,-0.3169403076171875,0.5267215967178345,-0.7558830976486206,-0.8863189816474915,0.01016207318753004,0.7533737421035767,0.7167660593986511,-0.06911822408437729,-0.34190431237220764,0.4833765923976898,-0.1803453415632248,-0.16122575104236603,-0.18914802372455597,0.34478041529655457,-0.1635085940361023,-0.06904106587171555,-0.12257269769906998,-0.13956275582313538,0.007588607259094715,-0.7005235552787781,0.12265987694263458,0.34856441617012024,0.25344452261924744,-0.43971994519233704,-0.4415578544139862,-0.0036432372871786356,0.5680834054946899,0.5974818468093872,0.577228844165802,-0.1605052351951599,0.09677192568778992,-0.07161452621221542,-0.3993081748485565,-0.16364187002182007,0.8077201247215271,0.4060294032096863,0.12797215580940247,-0.001543777878396213,0.1900930404663086,-0.15829454362392426,-0.09732410311698914,-0.12286432087421417,-0.41358447074890137,0.1401289701461792,-0.10640738904476166,0.4686533808708191,-0.2682449519634247,-0.10924641042947769,-0.16432131826877594,0.6166184544563293,-0.16051927208900452,-0.04908469691872597,0.7997311353683472,-0.1890067309141159,0.845782995223999,-0.09882328659296036,-0.10173346102237701,-0.1619713306427002,-0.6608834266662598,-0.07942710816860199,-0.07003918290138245,-0.6048835515975952,-0.09823103249073029,-0.22062285244464874,-0.07214565575122833,-0.1458040028810501,0.24049903452396393,-0.1751282662153244,-0.1602506786584854,-0.16183705627918243,-0.2119935303926468,0.19858404994010925,-0.10371667891740799,-0.08925697952508926,0.6577247381210327,-0.06504758447408676,-0.12286331504583359,-0.19652970135211945,-0.17160479724407196,-0.32293200492858887,-0.351596862077713,-0.06583525985479355,0.3319842219352722,-0.15900202095508575,-0.31182044744491577,-0.35119199752807617,0.7891136407852173,-0.49393102526664734,0.1130194142460823,-0.0694669708609581,-0.119613416492939,-0.17920492589473724,0.8878793120384216,-0.7470443248748779,-0.08652517944574356,-0.5164167881011963,0.09202886372804642,-0.7095457315444946,-0.1893133819103241,0.4178745746612549,-0.8094387650489807,-0.16184405982494354,-0.15459366142749786,-0.15816625952720642,-0.09248983860015869,0.1878552883863449,-0.08837911486625671,-0.41921743750572205,-0.15796783566474915,-0.49039652943611145,0.28507766127586365,-0.0865442156791687,-0.858430802822113,0.6940077543258667,0.18123723566532135,-0.15871642529964447,0.8492934107780457,-0.7595208287239075,-0.6823049187660217,0.3157377541065216,0.14273951947689056,0.057208940386772156,-0.07088498026132584,-0.15082569420337677,-0.16364187002182007,-0.18426565825939178,0.22938543558120728,-0.17369422316551208,-0.23309531807899475,0.05258011817932129,0.34990429878234863,-0.3445218503475189,-0.07374705374240875,-0.19714400172233582,-0.8512715697288513,0.4139161705970764,-0.39118191599845886,-0.15008610486984253,0.8485388159751892,-0.07698893547058105,-0.8281444907188416,-0.7432703375816345,-0.727210283279419,-0.18465648591518402,-0.23309531807899475,-0.2573728859424591,-0.4018746316432953,0.5562312602996826,-0.11310159415006638,-0.18706761300563812,0.6034515500068665,-0.19380490481853485,-0.19253621995449066,0.5666343569755554,-0.2561121881008148,-0.5340260863304138,0.7632968425750732,-0.20351353287696838,-0.22090543806552887,0.26469239592552185,-0.5861978530883789,0.4911210834980011,-0.29746195673942566,0.27230754494667053,-0.14712682366371155,-0.07775656133890152,-0.07806020975112915,-0.34431442618370056,0.0010336936684325337,-0.8577232956886292,-0.10857899487018585,0.7563239932060242,0.3077227473258972,-0.15812015533447266,-0.08302976191043854,-0.18844853341579437,-0.19930359721183777,-0.06975558400154114,-0.18409529328346252,-0.10981061309576035,-0.2615978419780731,0.4702407419681549,-0.5205094814300537,-0.1066078171133995,-0.20220375061035156,-0.08713948726654053,-0.03088807314634323,-0.5270102620124817,-0.15621310472488403,0.0020037509966641665,0.393852561712265,0.6767882108688354,-0.5516505837440491,-0.15866431593894958,0.6849334836006165,-0.4332802891731262,-0.0641126036643982,-0.09547819197177887,-0.16066856682300568,0.21769261360168457,-0.16488952934741974,-0.1311028152704239,-0.07284814864397049,0.7924156785011292,-0.27331268787384033,0.3364056348800659,0.631013035774231,-0.15930669009685516,0.48224419355392456,-0.0909515768289566,-0.15794478356838226,0.08763452619314194,-0.26145756244659424,-0.47127294540405273,-0.873163104057312,-0.2466059923171997,-0.19987981021404266,-0.4901890754699707,-0.194026380777359,-0.4621756374835968,-0.139851376414299,-0.23066015541553497,-0.08000834286212921,-0.10490117967128754,-0.1571911871433258,0.5248656868934631,-0.1311238557100296,-0.7851903438568115,-0.2025374472141266,-0.07366788387298584,-0.6134828329086304,-0.06763407588005066,-0.1580289751291275,-0.435192346572876,-0.15621310472488403,-0.16322901844978333,-0.06371275335550308,-0.5430322885513306,-0.16031582653522491,-0.6069850325584412,-0.15885071456432343,-0.0737440437078476,-0.2957322895526886,-0.23309531807899475,-0.18238165974617004,0.8663305640220642,-0.15698575973510742,-0.6623806357383728,-0.19801786541938782,-0.1420370191335678,0.3396936058998108,-0.15348030626773834,0.8059894442558289,-0.1744207739830017,-0.17101754248142242,-0.23309531807899475,-0.17208781838417053,0.04678281024098396,-0.568953275680542,-0.2668319344520569,-0.1577383428812027,-0.16238521039485931,-0.15977166593074799,-0.06369972229003906,-0.15750786662101746,-0.15928763151168823,-0.1573515236377716,-0.15826848149299622,-0.15621310472488403,-0.2657656669616699,-0.6199876070022583,0.7323872447013855,-0.1125464215874672,-0.1592014580965042,-0.1592675894498825,0.04272820055484772,-0.19438613951206207,-0.18749050796031952,-0.14185863733291626,0.4849890470504761,-0.5971952080726624,0.5375956892967224,-0.8535253405570984,-0.0642508938908577,0.15495343506336212,-0.23309531807899475,-0.03950035572052002,-0.1745099574327469,-0.7886075377464294,-0.15859316289424896,0.09043146669864655,-0.1619112193584442,0.8608719706535339,-0.15621310472488403,-0.19297514855861664,-0.1904638111591339,-0.3164653182029724,0.6938043236732483,-0.03177294880151749,-0.358884334564209,0.37086477875709534,-0.07576433569192886,-0.6544778347015381,-0.7068530321121216,-0.16300953924655914,-0.19760097563266754,-0.20226988196372986,-0.23309531807899475,-0.067507803440094,-0.23309531807899475,0.1311318725347519,-0.6894370317459106,-0.16337832808494568,0.6553968191146851,-0.2008959800004959,-0.5436335802078247,0.710206151008606,-0.8169257044792175,-0.7291744351387024,-0.18466050922870636,0.40541812777519226,0.6257187724113464,-0.08574753254652023,-0.15621310472488403,0.5092204809188843,0.7589446306228638,-0.19437512755393982,0.2686798572540283,-0.08045127987861633,-0.23309531807899475,-0.15871743857860565,0.30022183060646057,0.4537016451358795,-0.03075278364121914,-0.1469123661518097,-0.15616200864315033,0.36462852358818054,-0.183448925614357,0.270668089389801,-0.15621310472488403,-0.07607699930667877,-0.16117866337299347,-0.15982478857040405,-0.15621310472488403,-0.23309531807899475,-0.06777537614107132,-0.23309531807899475,-0.20752404630184174,0.03194931894540787,0.8313693404197693,0.3735053837299347,-0.07685565203428268,-0.13538891077041626,-0.856378436088562,-0.8600462675094604,-0.15621310472488403,-0.112283855676651,-0.23309531807899475,-0.23303619027137756,0.04610336571931839,-0.08693605661392212,-0.1640838235616684,-0.16142918169498444,-0.05677201971411705,0.6293585300445557,0.5464244484901428,-0.23309531807899475,0.6824792623519897,-0.15621310472488403,-0.07157544046640396,-0.0943908765912056,-0.12442964315414429,0.22224728763103485,-0.1970728486776352,0.1647322028875351,-0.08678774535655975,0.07255451381206512,-0.23309531807899475,-0.15892888605594635,-0.1257254034280777,-0.25521525740623474,-0.15621310472488403,-0.0684548169374466,-0.06423386186361313,-0.1609060913324356,-0.20908837020397186,-0.7723209857940674,-0.47513914108276367,-0.1581742763519287,-0.3137866258621216,-0.42188310623168945,-0.45862507820129395,-0.23044569790363312,-0.22924816608428955,-0.21903447806835175,0.026400543749332428,-0.08790110051631927,0.7263924479484558,-0.19648660719394684,-0.1633402407169342,-0.12481345981359482,-0.8573133945465088,-0.23309531807899475,-0.08861061185598373,-0.2242836058139801,-0.06617899239063263,-0.20106132328510284,0.3238028585910797,-0.8879965543746948,0.7924988269805908,-0.5285845994949341,0.23447924852371216,-0.14098580181598663,-0.21009249985218048,-0.18548424541950226,-0.09331057965755463,-0.07540056109428406,-0.07821353524923325,-0.16553589701652527,-0.1591223031282425,-0.6483598351478577,-0.23309531807899475,-0.5278089642524719,-0.15709398686885834,-0.8877741098403931,-0.07825662940740585,0.8497353196144104,-0.07206347584724426,-0.1295495182275772,-0.23309531807899475,-0.792267382144928,-0.16059942543506622,0.5604171752929688,-0.6559980511665344,-0.07623133063316345,-0.06680531799793243,-0.23309531807899475,-0.1574838012456894,0.8522516489028931,-0.07583347707986832,-0.13097955286502838,0.4627588391304016,-0.23309531807899475,-0.23050682246685028,0.16098524630069733,0.33491647243499756,-0.16210061311721802,-0.16584154963493347,-0.005505189765244722,-0.1608709990978241,-0.656960129737854,-0.5795086622238159,-0.04652126133441925,-0.6805661916732788,-0.48935332894325256,-0.07410881668329239,0.5507205724716187,0.15484119951725006,-0.15888679027557373,-0.1765492707490921,0.41409754753112793,-0.07098118215799332,0.3171968460083008,-0.11467794328927994,-0.1622479259967804,-0.12720555067062378,-0.19450139999389648,-0.48373639583587646,-0.31211405992507935,-0.15964139997959137,-0.08466923981904984,0.36372262239456177,-0.6331345438957214,-0.07108940929174423,-0.3849516808986664,-0.19374176859855652,-0.2971843481063843,-0.07910742610692978,-0.15705791115760803,-0.10205814987421036,-0.07923770695924759,-0.16118668019771576,-0.16205450892448425,-0.16083091497421265,-0.1066308543086052,-0.1375073939561844,-0.16377116739749908,-0.17269007861614227,-0.2443411946296692,-0.23309531807899475,-0.17509619891643524,0.16728061437606812,-0.753510057926178,0.8308292031288147,-0.06623710691928864,-0.23309531807899475,-0.14631108939647675,-0.12400072813034058,-0.06871336698532104,-0.0706915631890297,-0.11969759315252304,-0.17860564589500427,-0.07276797294616699,-0.15911327302455902,0.16364388167858124,-0.04950860142707825,-0.2013198733329773,-0.47282421588897705,0.23679718375205994,-0.23309531807899475,-0.17384956777095795,0.8113979697227478,0.5536919236183167,-0.16456083953380585,0.8433327078819275,-0.4247491657733917,-0.07247135043144226,-0.16252350807189941,-0.8538260459899902,-0.15789368748664856,-0.05167720466852188,-0.23309531807899475,-0.7088763117790222,0.5905070304870605,-0.19658181071281433,-0.10519982874393463,-0.3475031852722168,-0.17582876980304718,0.05955692380666733,-0.7273485660552979,-0.014379027299582958,-0.07820351421833038,0.4359709918498993,-0.1625455617904663,0.6305781006813049,-0.7144131064414978,0.26140642166137695,-0.14582706987857819,-0.15621310472488403,0.6504061818122864,-0.0773286521434784,-0.06563083082437515,-0.40285366773605347,0.009828360751271248,-0.23309531807899475,-0.28136980533599854,-0.5727112293243408,-0.15731646120548248,0.8624853491783142,-0.06456255167722702,-0.4994317293167114,-0.12055943161249161,0.8663305640220642,-0.2071843147277832,-0.5584339499473572,-0.6298365592956543,-0.08427239954471588,-0.12305472791194916,-0.23309531807899475,-0.1627860814332962,-0.09404513984918594,0.07316380739212036,-0.1396559625864029,-0.09684709459543228,-0.7201592326164246,-0.07519212365150452,0.1142369955778122,-0.17602717876434326,0.3900494873523712,0.6348090767860413,-0.16076777875423431,-0.4788690507411957,-0.23309531807899475,-0.08526450395584106,0.7220152020454407,-0.10397221893072128,-0.1334768384695053,-0.08571746200323105,-0.10401029884815216,-0.08707234263420105,0.2051880806684494,0.5731080770492554,-0.18476873636245728,-0.08266898989677429,-0.09140053391456604,-0.07376309484243393,-0.6208494901657104,-0.17469936609268188,-0.23309531807899475,-0.11814931035041809,0.24240608513355255,0.03197036683559418,-0.798965573310852,-0.14467863738536835,-0.642855167388916,-0.3662189245223999,-0.15874548256397247,-0.5898877382278442,-0.07893005758523941,-0.07130687683820724,-0.07910843938589096,0.33926570415496826,-0.07565409690141678,0.49878832697868347,-0.1599450409412384,-0.6488578915596008,0.47444668412208557,-0.15895693004131317,-0.08080704510211945,0.8200823664665222,-0.1595892757177353,-0.19280077517032623,-0.19395524263381958,-0.5719786882400513,-0.0726577416062355,0.19910617172718048,0.308789998292923,-0.19252519309520721,-0.29730862379074097,-0.7578081488609314,0.10828937590122223,-0.1862889677286148,-0.448158860206604,-0.13586491346359253,0.8228522539138794,0.7978331446647644,-0.16966065764427185,-0.10576602071523666,-0.1646369993686676,-0.23470273613929749,-0.1303582340478897,-0.09081128239631653,0.7771923542022705,-0.19551654160022736,-0.08560923486948013,-0.08865570276975632,-0.07144816964864731,0.09512142837047577,-0.5919541120529175,-0.1938009113073349,-0.3526601493358612,-0.16336829960346222,0.815946638584137,-0.5674901604652405,-0.2127060443162918,-0.16357873380184174,-0.06644254922866821,0.8560868501663208,-0.07801411300897598,0.8260660767555237,-0.04362811520695686,-0.8868581056594849,-0.15664604306221008,-0.513806164264679,-0.17030301690101624,-0.044708412140607834,-0.07056128978729248,0.5867961645126343,-0.1720036417245865,0.5993808507919312,-0.07622732222080231,-0.1629624366760254,-0.023543480783700943,-0.14152494072914124,-0.17187334597110748,-0.33204135298728943,-0.23309531807899475,-0.15621310472488403,-0.14785335958003998,-0.38020461797714233,-0.09140553325414658,-0.21753229200839996,0.5079377293586731,-0.11159038543701172,-0.16277404129505157,0.47034797072410583,-0.08514424413442612,0.3498291075229645,-0.07221881300210953,-0.7152037620544434,0.41109615564346313,-0.06528007984161377,-0.38790997862815857,-0.09682804346084595,0.19773626327514648,-0.7754175662994385,-0.13460823893547058,-0.07598180323839188,-0.10271754860877991,0.7501329183578491,-0.2319268435239792,-0.2038542479276657,-0.20898714661598206,0.12264686077833176,-0.15827548503875732,-0.11390931159257889,0.41875144839286804,-0.15827348828315735,-0.10195192694664001,-0.07632151991128922,-0.16487149894237518,-0.5992175340652466,-0.23309531807899475,-0.16010938584804535,-0.0783187597990036,-0.2162555754184723,-0.08705028891563416,-0.38317692279815674,0.052933864295482635,-0.07092105597257614,-0.07210957258939743,-0.8295745253562927,-0.38340941071510315,-0.11230089515447617,-0.15822939574718475,-0.16131094098091125,-0.1596975177526474,-0.17455604672431946,-0.07053223252296448,0.42534440755844116,-0.3001967668533325,-0.19810603559017181,-0.41408854722976685,0.44094452261924744,-0.2227092683315277,-0.15621310472488403,0.09070003032684326,-0.8256502151489258,-0.14534404873847961,-0.4637559652328491,-0.3809211254119873,0.271937757730484,-0.35417434573173523,-0.622298538684845,-0.4266511797904968,-0.21552503108978271,-0.06870835274457932,-0.7591330409049988,0.7273605465888977,-0.41410356760025024,-0.07638465613126755,-0.6916507482528687,-0.07739680260419846,0.8463972210884094,0.1928609013557434,-0.47399473190307617,-0.293881356716156,-0.2614695429801941,-0.15835265815258026,-0.6638677716255188,0.8335950374603271,-0.23309531807899475,-0.23134559392929077,0.6956391930580139,-0.15997309982776642,0.35742223262786865,-0.1643233299255371,-0.16235215961933136,-0.06492232531309128,-0.2224627435207367,0.35791027545928955,-0.15824943780899048,-0.2031838297843933,-0.1279410868883133,-0.5956799983978271,-0.07011333853006363,-0.15822339057922363,-0.023948339745402336,-0.060013897716999054,-0.29389238357543945,0.8274871110916138,-0.19500647485256195,-0.21025682985782623,0.6633597016334534,-0.33831867575645447,-0.15661296248435974,-0.006901150569319725,-0.06793471425771713,-0.01304719876497984,-0.16106440126895905,-0.4317760765552521,-0.4116723835468292,0.21998950839042664,-0.07631450891494751,-0.8878281712532043,0.836431086063385,-0.15621310472488403,0.5351384878158569,-0.45955005288124084,-0.16019758582115173,-0.15621310472488403,-0.16280010342597961,0.7250356078147888,-0.08640993386507034,0.03621738404035568,-0.10153504461050034,-0.1372869312763214,-0.23309531807899475,-0.8147761225700378,-0.18884237110614777,-0.09078822284936905,0.8255379796028137,-0.20277796685695648,-0.22314520180225372,-0.7918674945831299,0.018684159964323044,-0.1601504683494568,-0.16028174757957458,-0.4950033128261566,-0.08495484292507172,-0.23309531807899475,0.531809389591217,-0.4959523379802704,-0.08141834288835526,-0.0948498547077179,-0.23309531807899475,-0.5034131407737732,-0.23378178477287292,-0.3433704078197479,-0.351625919342041,-0.07728656381368637,-0.1248365044593811,-0.08100045472383499,-0.13985738158226013,-0.22195768356323242,-0.21440865099430084,-0.40759074687957764,-0.18553034961223602,-0.16786184906959534,0.1303451955318451,-0.24672424793243408,0.4276132583618164,-0.1617077738046646,0.8655939698219299,0.7756731510162354,0.8570207953453064,-0.15769827365875244,0.48681390285491943,-0.2020273655653,0.7921731472015381,-0.15947002172470093,-0.1677776724100113,-0.06598958373069763,-0.07335522025823593,-0.23309531807899475,-0.20752905309200287,0.03339238464832306,0.37819933891296387,-0.12701714038848877,0.10346313565969467,-0.1559024602174759,-0.17434661090373993,-0.20232300460338593,-0.16466304659843445,-0.19216443598270416,-0.07362479716539383,-0.19602462649345398,-0.1178206130862236,-0.8248475193977356,-0.5909920334815979,-0.16054029762744904,-0.08270206302404404,-0.20140604674816132,0.07910040766000748,-0.3705922067165375,0.5264861583709717,-0.23217937350273132,0.18240170180797577,-0.15914833545684814,-0.0688987672328949,-0.15720520913600922,0.7508955001831055,-0.2055748999118805,0.7739624977111816,0.7510347962379456,0.22858074307441711,0.573582112789154,-0.10172845423221588,0.6822087168693542,0.3959750831127167,-0.4178875982761383,0.2850506007671356,-0.20638862252235413,-0.013605386018753052,0.8728864789009094,0.8168926239013672,0.3311314284801483,-0.1669679433107376,-0.16161257028579712,-0.23086757957935333,0.8419438004493713,-0.16935300827026367,0.29357168078422546,0.29235410690307617,-0.38457587361335754,0.6145300269126892,-0.25754621624946594,0.5359121561050415,-0.10118129104375839,-0.08062165230512619,-0.7260868549346924,-0.12294749915599823,-0.16568322479724884,-0.16059041023254395,0.7502902746200562,0.8829989433288574,-0.1665831208229065,-0.23309531807899475,0.43664342164993286,-0.23309531807899475,-0.48696625232696533,0.8687717318534851,-0.0938657596707344,-0.43619945645332336,-0.0671791136264801,0.1317211240530014,-0.06873240321874619,-0.06963131576776505,-0.07457581162452698,-0.021679524332284927,-0.8154445290565491,-0.06645657867193222,-0.15621310472488403,0.25386741757392883,-0.1500149518251419,0.3859718441963196,0.5723294615745544,-0.25414401292800903,-0.16159753501415253,0.271411657333374,-0.7023985981941223,-0.2893346846103668,-0.5409367680549622,-0.07387632876634598,-0.23309531807899475,-0.06686444580554962,0.11224177479743958,-0.19422580301761627,-0.10268749296665192,-0.16326206922531128,-0.16191823780536652,-0.09537697583436966,-0.3307977318763733,-0.07037990540266037,-0.15621310472488403,-0.0683014914393425,-0.16103436052799225,-0.3221924602985382,-0.15917140245437622,-0.6348121166229248,-0.20058931410312653,-0.02778748981654644,-0.8172323703765869,-0.21085210144519806,-0.814911425113678,-0.2044154405593872,-0.06555767357349396,-0.08089923858642578,-0.17219604551792145,-0.795560359954834,0.35351693630218506,-0.14043161273002625,-0.20285913348197937,-0.26646214723587036,-0.12133106589317322,-0.13557830452919006,0.3653580844402313,-0.09228441119194031,0.6645672917366028,-0.1471729278564453,-0.24115943908691406,-0.15731744468212128,-0.22258000075817108,-0.19899293780326843,-0.16115762293338776,-0.18339480459690094,-0.29693683981895447,0.19111721217632294,-0.5185742974281311,-0.15621310472488403,-0.08683384209871292,-0.21731781959533691,-0.8560296893119812,0.8373790979385376,-0.7858767509460449,-0.07265373319387436,-0.21992436051368713,0.6444776058197021,-0.23309531807899475,0.6894891262054443,0.8518628478050232,-0.7458678483963013,-0.19076445698738098,-0.08252468705177307,-0.4552970230579376,-0.23309531807899475,0.3858025074005127,0.11077465116977692,-0.15621310472488403,-0.0868067815899849,-0.16988714039325714,-0.07240018993616104,-0.16951735317707062,-0.17590491473674774,-0.8537458181381226,-0.8818936347961426,-0.23309531807899475,0.8359851837158203,-0.0672081708908081,-0.07822655886411667,-0.7538698315620422,0.6301562190055847,0.26832109689712524,-0.8617197871208191,0.21752125024795532,0.4766252636909485,-0.6521508693695068,-0.01816706918179989,0.4540874660015106,0.5026485323905945,0.7360941171646118,0.4858308434486389,0.4282536208629608,-0.1616496592760086,-0.0037324237637221813,0.04800039902329445,-0.19722817838191986,-0.21916474401950836,-0.14178046584129333,-0.08180715888738632,-0.12634770572185516,-0.16032986342906952,0.41228270530700684,-0.06962429732084274,0.18356618285179138,-0.15713006258010864,-0.20132187008857727,-0.15755595266819,0.40771299600601196,-0.07714126259088516,0.8250649571418762,0.8753026127815247,-0.08125700056552887,0.3565143346786499,0.31030622124671936,-0.09840940684080124,-0.08537573367357254,-0.07167765498161316,-0.5776036381721497,-0.07053723186254501,-0.15621310472488403,-0.55024653673172,-0.06392320245504379,-0.1835351139307022,0.7069331407546997,-0.21584570407867432,-0.16121172904968262,-0.5368180871009827,0.7416608929634094,-0.21311290562152863,0.007003366015851498,0.06014217063784599,-0.07985702902078629,0.8040122985839844,-0.5319036245346069,0.21825379133224487,-0.07746093720197678,-0.13689309358596802,-0.07153134793043137,-0.09366734325885773,-0.07682458311319351,-0.432233065366745,0.023839103057980537,-0.2515895962715149,0.09957487136125565,0.020114198327064514,-0.059154074639081955,-0.17305687069892883,-0.646833598613739,-0.12034296989440918,-0.03786187246441841,-0.15800991654396057,-0.46256041526794434,0.7757683396339417,-0.18891853094100952,-0.08957866579294205,-0.1791858822107315,0.007243874948471785,-0.16091008484363556,-0.16664326190948486,-0.3606831431388855,-0.0851161852478981,-0.15985684096813202,-0.8491671085357666,0.5733235478401184,-0.7681651711463928,0.14815600216388702,0.3758513629436493,-0.1932918131351471,0.8630215525627136,0.036760538816452026,-0.2994632124900818,-0.0847083255648613,0.8235377073287964,0.09296685457229614,-0.565427839756012,-0.32763200998306274,-0.0155635392293334,-0.07443951815366745,-0.08121791481971741,0.689093291759491,-0.5499539971351624,0.5580060482025146,0.4630143940448761,-0.1897583305835724,0.6071333289146423,-0.16100329160690308,0.19890174269676208,0.6618664860725403,0.4640055000782013,-0.17479054629802704,-0.181086927652359,0.1261472851037979,-0.3822559714317322,0.5988687872886658,-0.1252453774213791,-0.09598926454782486,-0.3463316857814789,-0.5230969190597534,-0.08200658112764359,0.6567075848579407,-0.06434008479118347,-0.11028261482715607,-0.09339877218008041,-0.17569947242736816,-0.0891307070851326,-0.44031018018722534,-0.8879594802856445,-0.08417919278144836,-0.23025628924369812,-0.23007290065288544,0.013165445066988468,-0.12544779479503632,-0.06783550977706909,-0.13588595390319824,-0.1364852488040924,0.5375837087631226,-0.26280540227890015,0.3728038966655731,0.2107769399881363,-0.16024768352508545,-0.15983881056308746,-0.07033480703830719,-0.15833662450313568,0.7869119644165039,-0.8783180117607117,-0.5011563897132874,-0.3358955383300781,0.15141792595386505,-0.08407998830080032,-0.17388564348220825,-0.15621310472488403,-0.37293216586112976,-0.08591988682746887,-0.6795410513877869,-0.42535245418548584,-0.1826612651348114,-0.7552908658981323,-0.17357195913791656,0.2539956867694855,-0.07342036068439484,-0.15621310472488403,0.026732247322797775,-0.15912029147148132,-0.1576792299747467,0.680711567401886,-0.2260984629392624,0.44483277201652527,-0.2131730318069458,-0.17608530819416046,-0.1645057052373886,-0.642057478427887,-0.23309531807899475,-0.11090894043445587,-0.09726697951555252,-0.16516511142253876,-0.15757200121879578,0.834563136100769,-0.24947910010814667,-0.1984718143939972,-0.15784558653831482,0.27492812275886536,-0.1656200885772705,-0.23309531807899475,-0.06711196899414062,-0.08737397938966751,-0.1633913666009903,-0.23309531807899475,-0.15621310472488403,0.8893283605575562,-0.159829780459404,-0.11477013677358627,-0.41622206568717957,-0.0941794291138649,-0.18805070221424103,-0.15679535269737244,-0.11246825009584427,0.48228129744529724,-0.20048509538173676,-0.5364643335342407,0.8151869773864746,-0.18723395466804504,0.6420564651489258,0.29869556427001953,-0.11056921631097794,-0.16094717383384705,-0.2009330540895462,-0.1208149716258049,-0.2025945782661438,-0.014379027299582958,-0.08282332122325897,-0.43648308515548706,-0.22289668023586273,-0.23273354768753052,-0.30946144461631775,-0.06593246012926102,-0.7777214646339417,-0.07290225476026535,0.561578631401062,0.7128387689590454,-0.4738544225692749,-0.4111352562904358,0.5025633573532104,-0.2882203459739685,-0.15921248495578766,0.6985032558441162,-0.16331319510936737,-0.6764224171638489,0.13265912234783173,0.49115511775016785,-0.736368715763092,-0.5242864489555359,-0.15621310472488403,-0.19835758209228516,-0.0709821805357933,0.7637448310852051,-0.16212067008018494,0.7472718358039856,-0.7095367312431335,-0.2863062918186188,-0.10259128361940384,-0.1573224663734436,-0.1124371886253357,-0.16861042380332947,-0.2116868793964386,-0.21823276579380035,-0.08393968641757965,-0.13216406106948853,0.7266841530799866,-0.04884118586778641,0.7384711503982544,0.3191460072994232,0.06747272610664368,0.031849104911088943,0.12115269154310226,0.24199523031711578,-0.6057053208351135,-0.5613651871681213,-0.047787945717573166,-0.06812311708927155,0.022179584950208664,0.5253857970237732,0.251522421836853,-0.040024466812610626,-0.1299213021993637,-0.7295191287994385,-0.13428056240081787,-0.1870044767856598,-0.6297032833099365,-0.17738205194473267,0.480637788772583,0.3422720730304718,0.17043429613113403,-0.15621310472488403,-0.0703779011964798,0.25390350818634033,0.6278863549232483,-0.17908266186714172,0.5265582799911499,-0.5825852155685425,-0.17303180694580078,-0.28279784321784973,0.6947864294052124,-0.1329597383737564,-0.14711681008338928,0.8167954087257385,-0.1718783676624298,-0.16360579431056976,-0.15972557663917542,-0.09989657253026962,0.4595610797405243,-0.2303965836763382,0.2647415101528168,-0.21740101277828217,-0.2161303013563156,-0.23309531807899475,-0.16101230680942535,-0.19287092983722687,-0.7753373980522156,-0.07256955653429031,-0.854308009147644,-0.11974169313907623,0.16800916194915771,-0.8068603277206421,-0.3507671058177948,-0.15823540091514587,0.06733443588018417,-0.18659061193466187,0.5131558179855347,-0.1603098213672638,0.4320867657661438,-0.15919344127178192,-0.23309531807899475,-0.7157468795776367,0.3057916462421417,-0.09000356495380402,0.18705858290195465,-0.20150426030158997,-0.37747883796691895,0.014664629474282265,-0.17992344498634338,-0.15915335714817047,0.4681242108345032,0.516061007976532,-0.16443657875061035,-0.4800976514816284,-0.8841633796691895,-0.783193051815033,-0.2201458364725113,-0.15310853719711304,-0.20176981389522552,-0.0716526061296463,-0.16263073682785034,0.3124106824398041,0.06414065510034561,-0.1749669313430786,0.38758426904678345,-0.1957530528306961,-0.22476965188980103,-0.1404937505722046,0.16756922006607056,-0.15621310472488403,-0.08392065018415451,-0.158678337931633,-0.07149627804756165,0.25184011459350586,-0.5174359083175659,-0.15856610238552094,-0.15621310472488403,-0.08587279170751572,-0.1863841563463211,0.8079847097396851,-0.23309531807899475,0.10245900601148605,-0.04045337811112404,-0.21328526735305786,-0.8219162821769714,-0.03455786034464836,0.7821037769317627,-0.1643614023923874,-0.24017333984375,0.16047315299510956,0.6830103993415833,-0.11602079123258591,0.584801971912384,-0.6598742604255676,-0.25301459431648254,-0.4847475588321686,-0.11883176118135452,-0.13318723440170288,-0.10067421197891235,-0.4129721522331238,-0.169284850358963,-0.1615404188632965,-0.08346568793058395,-0.6385771036148071,-0.06477801501750946,-0.20280902087688446,-0.08267199993133545,-0.21770966053009033,-0.4481738805770874,-0.5949354767799377,0.31687217950820923,-0.06936274468898773,-0.8284160494804382,0.8502093553543091,-0.07898717373609543,-0.14195285737514496,-0.1624142825603485,-0.7860932350158691,-0.19911418855190277,-0.15621310472488403,-0.45120730996131897,-0.6795660853385925,0.2890681326389313,-0.5518459677696228,-0.21699915826320648,-0.09825307130813599,-0.0684548169374466,-0.14182457327842712,-0.08464118093252182,-0.6049476861953735,-0.01808389276266098,0.05831829458475113,0.2498999983072281,-0.16404172778129578,-0.17530864477157593,-0.1573004275560379,-0.16418102383613586,-0.2063325047492981,-0.1944572925567627,-0.7546945810317993,-0.09430168569087982,-0.8879594802856445,-0.23309531807899475,-0.10602858662605286,0.7216805219650269,-0.15621310472488403,0.05217825993895531,-0.20675040781497955,-0.01413851510733366,-0.1674860417842865,0.7972278594970703,0.3976696729660034,-0.25900134444236755,-0.09128929674625397,0.5867279767990112,-0.04430856183171272,-0.7385814189910889,-0.1653004139661789,-0.5962021350860596,-0.0946383997797966,0.793593168258667,-0.6216741800308228,-0.15892186760902405,-0.049920473247766495,-0.002056867117062211,-0.1803663820028305,0.12768754363059998,0.22931328415870667,-0.7654153108596802,0.7992762327194214,-0.16825568675994873,-0.17682887613773346,-0.683977484703064,0.026172056794166565,-0.09935341775417328,-0.1615915298461914,-0.07398856431245804,-0.15621310472488403,0.7890365123748779,-0.23309531807899475,0.8491991758346558,0.25935307145118713,-0.8098607063293457,-0.1234375387430191,-0.3259384334087372,0.8663305640220642,-0.1541757881641388,0.07763530313968658,-0.19315554201602936,-0.14481191337108612,-0.16263775527477264,-0.15621310472488403,0.22468245029449463,-0.23309531807899475,-0.07411583513021469,-0.7249564528465271,0.6708225011825562,-0.15733148157596588,-0.07052922248840332,-0.15847191214561462,-0.30808451771736145,-0.21420320868492126,0.8390436768531799,-0.1590651571750641,-0.06930362433195114,-0.21029290556907654,0.08961774408817291,-0.10825831443071365,-0.10450734943151474,-0.20881778001785278,-0.15621310472488403,-0.2680475115776062,0.5049253702163696,0.29452770948410034,-0.15621310472488403,-0.08628065884113312,-0.20109137892723083,0.6477094292640686,-0.707230806350708,0.3213426470756531,0.5572283864021301,-0.21554507315158844,0.8510871529579163,0.7228139042854309,-0.5490971207618713,-0.24799194931983948,-0.21371117234230042,-0.26496195793151855,-0.16186010837554932,0.41963228583335876,-0.6319229602813721,-0.1643233299255371,-0.516862690448761,0.461855947971344,-0.23309531807899475,0.5917366743087769,-0.8333595395088196,0.7977710366249084,-0.06539633125066757,0.8057358860969543,-0.10071129351854324,0.7361181378364563,-0.5891571640968323,-0.06757896393537521,0.49186766147613525,0.6656375527381897,0.7786263823509216,-0.15621310472488403,-0.040844209492206573,-0.17591693997383118,-0.8556950092315674,-0.2088819146156311,-0.20077672600746155,-0.23309531807899475,0.24380604922771454,-0.10844270139932632,0.6272830963134766,-0.13232842087745667,-0.08243249356746674,-0.5189761519432068,-0.15324783325195312,-0.35430261492729187,-0.6983489990234375,0.5472021102905273,-0.1727893054485321,-0.07295536994934082,-0.16026070713996887,-0.16834485530853271,-0.23309531807899475,-0.0686873123049736,-0.05615871399641037,0.0731748417019844,-0.7522293925285339,-0.16139110922813416,-0.08736797422170639,0.8222560286521912,-0.13965797424316406,0.13433466851711273,-0.46024954319000244,-0.47024574875831604,-0.07021354883909225,-0.7248021364212036,-0.17744016647338867,-0.4702407419681549,-0.17052750289440155,0.4294711947441101,0.07856327295303345,-0.1583275943994522,-0.16401167213916779,-0.15720321238040924,-0.21197950839996338,-0.0746910572052002,-0.18120115995407104,-0.16027873754501343],\"z\":[0.3789254128932953,-0.3787590563297272,-0.37115588784217834,-0.26957327127456665,0.3201526403427124,-0.27252453565597534,-0.2602825164794922,0.3083505630493164,0.18600787222385406,-0.37926408648490906,-0.254714697599411,-0.22331205010414124,-0.3762447237968445,-0.25839653611183167,0.29983648657798767,0.34908100962638855,0.03833036869764328,-0.27014046907424927,-0.26849594712257385,-0.29298293590545654,-0.07096264511346817,-0.016790639609098434,0.3594881296157837,0.3183528184890747,0.21012908220291138,-0.35860925912857056,-0.2885324954986572,-0.2894805073738098,-0.2374931424856186,-0.27036193013191223,-0.28293561935424805,-0.2616424262523651,-0.33622974157333374,-0.2859610617160797,-0.2973431944847107,-0.2825157344341278,0.19410806894302368,0.30211734771728516,0.11823098361492157,0.37002047896385193,-0.05853024870157242,-0.10582965612411499,0.28009358048439026,0.3656732738018036,0.257802277803421,0.046147968620061874,-0.23418213427066803,-0.3191986382007599,0.28528860211372375,0.37272021174430847,-0.12368255853652954,0.24658043682575226,-0.2646067142486572,0.06712249666452408,0.16288478672504425,0.04221261292695999,-0.18796201050281525,-0.13446544110774994,-0.24711757898330688,-0.2699620723724365,-0.2878350019454956,-0.31885188817977905,-0.2523547112941742,0.3703712224960327,0.2520290017127991,-0.2619951665401459,-0.11398597061634064,0.07864496111869812,0.28799039125442505,0.26863226294517517,-0.29669684171676636,-0.22151723504066467,-0.22092197835445404,-0.21734438836574554,-0.3169918954372406,-0.3061348795890808,-0.2350369393825531,-0.31638461351394653,-0.30259835720062256,0.09304352104663849,-0.35404959321022034,-0.32090824842453003,0.17166340351104736,-0.2409094125032425,-0.24295775592327118,-0.28402093052864075,0.27117764949798584,0.3594881296157837,-0.26436522603034973,-0.2789261043071747,-0.28525254130363464,-0.3049834370613098,0.3741542398929596,-0.02087230607867241,-0.2520189583301544,-0.2903413474559784,-0.2919357419013977,0.14738187193870544,0.09151928126811981,-0.06707137823104858,0.047810494899749756,-0.013604877516627312,0.37486475706100464,-0.17796076834201813,-0.14763841032981873,-0.1851891130208969,-0.1500294804573059,0.35908329486846924,0.1585686206817627,0.3266564607620239,0.13884976506233215,-0.038644034415483475,0.01007639616727829,-0.27867957949638367,-0.2740948796272278,-0.2600129544734955,0.3171432614326477,-0.1806735396385193,-0.15149961411952972,-0.3140276074409485,0.026640553027391434,0.3654899001121521,0.0012165838852524757,0.28653424978256226,0.08032151311635971,0.1865890920162201,0.2940211594104767,0.31027165055274963,-0.2522173821926117,0.27693086862564087,0.12490013986825943,0.29212111234664917,-0.35999220609664917,0.22255845367908478,-0.2627718150615692,-0.2639613449573517,0.29536303877830505,-0.04039975628256798,0.24881017208099365,-0.19074591994285583,-0.19188334047794342,-0.12409242242574692,-0.2541474997997284,-0.02295873686671257,0.16498222947120667,0.08888769149780273,0.2724123001098633,0.2896779179573059,-0.09551376104354858,0.29254603385925293,0.3527568280696869,-0.07675192505121231,0.19674064218997955,-0.24168004095554352,0.3787219524383545,-0.08562877029180527,0.19889721274375916,-0.011991453357040882,-0.2671240270137787,-0.22863034904003143,0.08539928495883942,0.27518215775489807,-0.3247573971748352,0.30128055810928345,0.3063403069972992,-0.2566027045249939,0.24658043682575226,-0.044446349143981934,-0.35551270842552185,-0.2955884635448456,-0.3274661600589752,0.3594881296157837,-0.31750601530075073,0.2499716579914093,0.18389137089252472,-0.24856264889240265,-0.285969078540802,0.30250415205955505,-0.23407189548015594,0.3602317273616791,-0.19567438960075378,0.2953740060329437,-0.3419288396835327,-0.28757548332214355,-0.23630663752555847,-0.017097292467951775,-0.09579635411500931,0.21437408030033112,-0.06889624893665314,0.10032597929239273,-0.2672834098339081,-0.1312836855649948,-0.2417181134223938,-0.3403855562210083,0.31974276900291443,0.10488565266132355,-0.2957167625427246,0.31279805302619934,-0.24347685277462006,-0.30369067192077637,0.209443598985672,0.3719435930252075,-0.27549582719802856,-0.1713046431541443,0.31118056178092957,-0.28103959560394287,-0.26309651136398315,0.07514653354883194,-0.27586662769317627,0.3250930905342102,-0.23980404436588287,-0.2303239405155182,-0.2628399729728699,-0.26354146003723145,-0.3281947076320648,-0.2565676271915436,-0.22581034898757935,-0.3648545444011688,-0.21932056546211243,-0.2754497230052948,0.11963897943496704,-0.256986528635025,-0.2997332811355591,-0.12470772862434387,-0.2665608525276184,-0.21734438836574554,0.011246878653764725,-0.32662540674209595,0.28561627864837646,0.34808194637298584,-0.3104640245437622,-0.23319703340530396,0.3379954993724823,0.339094877243042,-0.26803797483444214,0.3594881296157837,0.3132770359516144,-0.3319877088069916,0.377460241317749,-0.32042622566223145,0.3111194670200348,0.3439521789550781,-0.22637054324150085,-0.3574768602848053,0.04694465920329094,0.13081370294094086,-0.27881789207458496,-0.21734438836574554,0.11067794263362885,-0.28077906370162964,-0.27519917488098145,-0.24698631465435028,-0.29110997915267944,-0.19827090203762054,0.24779704213142395,0.3324717879295349,0.22364777326583862,-0.25581100583076477,-0.3200664520263672,-0.1937422901391983,0.3321961760520935,-0.23848827183246613,0.14277708530426025,0.3184640407562256,0.36655914783477783,0.2044159471988678,-0.2926121652126312,0.3240959942340851,-0.2708148956298828,-0.21734438836574554,-0.2385624349117279,0.249118834733963,-0.2626104950904846,0.30680227279663086,-0.05944819748401642,0.036271002143621445,-0.3426142930984497,-0.2788459360599518,-0.17319364845752716,-0.3007614314556122,-0.36298254132270813,-0.2913013696670532,0.23306076228618622,0.15640702843666077,-0.26575416326522827,-0.2899254262447357,-0.16746649146080017,0.18684464693069458,-0.24950766563415527,-0.2539490759372711,0.14773963391780853,-0.061098698526620865,-0.31168967485427856,0.3123480975627899,-0.2997473180294037,-0.2094866931438446,-0.24755249917507172,0.3178076446056366,-0.05135601758956909,0.3356515169143677,-0.2038046419620514,-0.28530561923980713,-0.015192248858511448,0.030395524576306343,-0.3291807770729065,0.02038828283548355,-0.3199862539768219,-0.21734438836574554,-0.28212690353393555,-0.24259799718856812,-0.2251439392566681,0.025035150349140167,0.31344541907310486,-0.3200574517250061,0.045249056071043015,-0.2723962366580963,0.32065367698669434,-0.2810235321521759,0.28243058919906616,0.29288774728775024,-0.27081090211868286,0.25662174820899963,-0.3002132773399353,0.13642962276935577,0.35591357946395874,0.3774372637271881,-0.13320676982402802,-0.09527625143527985,-0.23792606592178345,-0.20108987390995026,0.24658043682575226,-0.07808375358581543,-0.30760398507118225,0.032511018216609955,-0.12243691086769104,-0.3279711902141571,-0.26829251646995544,0.08964429050683975,-0.28074997663497925,0.24658043682575226,0.31803613901138306,-0.21453942358493805,0.3026414215564728,-0.005158948712050915,0.13720928132534027,-0.32306283712387085,-0.2618488371372223,-0.28114479780197144,-0.11777500808238983,0.3012595474720001,0.31156638264656067,-0.06845030933618546,-0.33947762846946716,0.3498275876045227,-0.3633353114128113,0.3059234321117401,0.24658043682575226,-0.07834531366825104,0.26122555136680603,-0.11567655950784683,-0.21734438836574554,-0.1385050117969513,-0.3241090178489685,0.044093601405620575,0.07079429179430008,0.35186898708343506,-0.24700534343719482,0.11606739461421967,-0.24612846970558167,0.1309630125761032,0.23797519505023956,-0.28923898935317993,0.35186898708343506,-0.250926673412323,0.06833105534315109,-0.02224121242761612,0.2840901017189026,0.21449333429336548,-0.27332621812820435,-0.3040213882923126,0.35498857498168945,0.3177325129508972,0.35601577162742615,-0.13287606835365295,0.2818743586540222,-0.27034488320350647,0.3146759867668152,0.011334063485264778,-0.2646077275276184,-0.3320408761501312,-0.02267613634467125,0.25208014249801636,-0.2785022258758545,0.3594881296157837,-0.3518368899822235,-0.2625263035297394,-0.23230011761188507,-0.03864102438092232,-0.3229205012321472,-0.009641464799642563,0.3650980591773987,0.21606966853141785,-0.042535290122032166,-0.26465079188346863,-0.3067782521247864,-0.22917649149894714,-0.26459166407585144,-0.04742066189646721,-0.27134501934051514,-0.35058820247650146,-0.2915489077568054,0.30871933698654175,-0.09206744283437729,0.17869234085083008,-0.2922924757003784,-0.14287230372428894,-0.2579425573348999,-0.21734438836574554,-0.20823904871940613,-0.3103969097137451,0.3416011333465576,-0.018186604604125023,0.33185744285583496,-0.2651338279247284,-0.33397796750068665,0.3166291415691376,0.3004177510738373,0.24658043682575226,0.03060196340084076,0.09919557720422745,0.2422863394021988,-0.008854794315993786,0.10571842640638351,0.03560759499669075,-0.26512083411216736,0.24658043682575226,-0.25908297300338745,-0.229171484708786,-0.3297620117664337,0.06304984539747238,0.374342679977417,0.3435192406177521,0.16937856376171112,-0.1745305061340332,-0.29427167773246765,0.32992035150527954,-0.2795363962650299,0.16249795258045197,0.2856784760951996,0.24658043682575226,0.3296768367290497,-0.30473190546035767,0.3050445318222046,-0.23302166163921356,-0.35097408294677734,-0.26038673520088196,-0.25277456641197205,-0.293430894613266,-0.3043059706687927,-0.2633921205997467,0.20164906978607178,0.04945799335837364,-0.35018736124038696,-0.21734438836574554,0.052691854536533356,0.09822952747344971,-0.3367047607898712,0.149046391248703,-0.32390958070755005,0.360732764005661,-0.3139915466308594,0.23750418424606323,0.33482980728149414,-0.28805649280548096,-0.3476570248603821,-0.33211302757263184,-0.2557358741760254,-0.3521946370601654,-0.3155568540096283,-0.08768914639949799,-0.26218658685684204,-0.3012895882129669,-0.26792675256729126,-0.1795932501554489,-0.24612247943878174,-0.27927082777023315,-0.11797945201396942,0.25856590270996094,0.30926749110221863,-0.2407640963792801,-0.2719883918762207,-0.29246583580970764,-0.24373841285705566,-0.2665157616138458,-0.2836611568927765,0.3171162009239197,0.344121515750885,-0.2624892294406891,-0.2775532007217407,-0.3059895634651184,-0.2762865126132965,-0.17727433145046234,-0.23872576653957367,0.22137492895126343,0.3242974281311035,0.13755600154399872,-0.3270091712474823,-0.2513866424560547,-0.24091342091560364,-0.2719743549823761,-0.267916738986969,-0.09368988871574402,-0.009257650002837181,0.37386465072631836,-0.2946374714374542,-0.23399873077869415,-0.23694299161434174,-0.2617887258529663,0.27266883850097656,-0.2417181134223938,0.2697736620903015,-0.307506799697876,-0.07051168382167816,-0.22437530755996704,-0.33530277013778687,-0.21734438836574554,-0.3271715044975281,0.2563181519508362,0.0930715799331665,-0.1554209142923355,-0.29834333062171936,0.2987041175365448,-0.15270215272903442,-0.29834434390068054,0.35930272936820984,0.3594881296157837,0.13376395404338837,0.37922799587249756,-0.24426253139972687,-0.2676251232624054,0.21010901033878326,-0.04895692691206932,0.3594881296157837,-0.2596451938152313,-0.1695910096168518,-0.22270676493644714,-0.2840970754623413,-0.26367172598838806,-0.2399844378232956,-0.2688617408275604,0.3744438886642456,0.3620796501636505,0.31671229004859924,0.1778305023908615,-0.2501700818538666,-0.0871560126543045,0.2915819585323334,-0.24812772870063782,-0.34056395292282104,0.29800060391426086,-0.24713662266731262,-0.11665964871644974,0.18756717443466187,0.3594881296157837,-0.3361365497112274,0.32052943110466003,0.29014989733695984,0.30694860219955444,-0.23462606966495514,0.3049543499946594,-0.21734438836574554,-0.24984537065029144,-0.34747663140296936,0.0028189881704747677,0.14354372024536133,0.08883558958768845,0.1669463962316513,-0.24287357926368713,0.33215007185935974,-0.25632813572883606,0.20175930857658386,0.33534786105155945,-0.25631409883499146,0.16749657690525055,-0.36629459261894226,-0.3391178548336029,-0.24086731672286987,-0.13529419898986816,-0.36026477813720703,0.30201011896133423,-0.3348498046398163,-0.2733282148838043,-0.2603967785835266,-0.2679517865180969,-0.2389923334121704,0.044111642986536026,0.18369796872138977,-0.20800454914569855,-0.24393382668495178,0.02863479033112526,0.29824215173721313,-0.2782557010650635,-0.24268217384815216,-0.2836741805076599,-0.055000755935907364,0.2409464716911316,-0.32995346188545227,-0.2941604554653168,0.3145658075809479,-0.13659095764160156,-0.3277587890625,0.24601224064826965,-0.31567615270614624,-0.3075137734413147,0.31537145376205444,0.3594881296157837,0.2752603590488434,0.3388603627681732,-0.28971099853515625,-0.31880372762680054,0.37154772877693176,0.27834686636924744,-0.3225487172603607,0.3594881296157837,-0.21734438836574554,-0.2805004417896271,-0.2529650032520294,0.33950868248939514,-0.33729803562164307,0.24658043682575226,0.33110591769218445,-0.2656068205833435,-0.31535443663597107,-0.26654481887817383,-0.23416408896446228,-0.2658343017101288,-0.24119701981544495,0.2882879674434662,-0.07276546210050583,-0.2907482087612152,-0.2264396995306015,-0.36338138580322266,-0.26450949907302856,-0.3343677818775177,-0.24749338626861572,-0.006700221449136734,0.07658958435058594,0.12420064955949783,-0.264520525932312,0.2755278944969177,-0.26064229011535645,-0.2716957628726959,-0.2361082136631012,-0.2658753991127014,-0.28931117057800293,-0.013921551406383514,-0.0719737857580185,0.3275162875652313,-0.23822970688343048,-0.28699222207069397,-0.018217671662569046,-0.2376023828983307,-0.28971901535987854,-0.28888827562332153,0.33707958459854126,-0.16589416563510895,-0.3335169553756714,-0.29846858978271484,-0.2636536955833435,0.346455454826355,-0.1863565891981125,-0.2601412534713745,0.21893376111984253,0.37486475706100464,0.008661387488245964,-0.34012600779533386,0.2598235309123993,-0.2810826897621155,-0.2862526774406433,0.017177464440464973,-0.26555871963500977,0.3397882878780365,-0.2666320204734802,-0.26115238666534424,-0.2808632254600525,-0.25114914774894714,0.3589049279689789,0.3155548870563507,-0.17068833112716675,0.32873785495758057,0.24658043682575226,-0.30088770389556885,0.37486475706100464,0.3594881296157837,0.32364603877067566,0.07716881483793259,0.10697508603334427,-0.3116145133972168,0.03236370533704758,-0.32362502813339233,0.14372611045837402,0.22920455038547516,0.2865733802318573,0.026712708175182343,0.21191084384918213,0.3457629978656769,0.3488966226577759,-0.1387294977903366,0.08518283069133759,-0.2790122926235199,0.06631878763437271,-0.3198239505290985,0.09614910930395126,0.14580051600933075,0.33172717690467834,-0.2589547038078308,0.3594881296157837,-0.3399195671081543,0.21472783386707306,0.3197587728500366,-0.24411320686340332,-0.1868426352739334,-0.27946722507476807,-0.24023297429084778,-0.31459882855415344,0.13844390213489532,0.2003442943096161,-0.23117774724960327,0.29219725728034973,-0.23932905495166779,0.2915429174900055,0.3609592616558075,-0.047248296439647675,0.11853763461112976,-0.09909535944461823,-0.22657297551631927,0.3360503613948822,-0.12712787091732025,0.27115362882614136,0.17637743055820465,-0.22275285422801971,-0.2786354720592499,-0.2755218744277954,-0.2999858260154724,-0.2397880256175995,-0.1772843450307846,0.06778289377689362,-0.22789877653121948,-0.025093264877796173,-0.11245973408222198,-0.2417181134223938,-0.2907432019710541,0.32970693707466125,-0.272040456533432,-0.35472801327705383,-0.11538393050432205,-0.2755018472671509,-0.22172166407108307,-0.23839406669139862,0.24658043682575226,0.2882789373397827,0.1452132612466812,-0.25755271315574646,-0.0920373722910881,-0.3238885700702667,0.3228724002838135,-0.2438235878944397,-0.16366443037986755,0.29424765706062317,-0.3102877140045166,-0.23824675381183624,0.3725859224796295,-0.2496299147605896,-0.05241526663303375,-0.2856072783470154,-0.2898392677307129,0.23232319951057434,0.18785980343818665,0.24658043682575226,0.32148346304893494,-0.15234439074993134,0.2962619364261627,-0.2760249376296997,0.30752381682395935,-0.23763445019721985,-0.3014819920063019,0.16939058899879456,-0.2921622097492218,-0.32293352484703064,-0.3552190661430359,-0.3447709083557129,-0.26817628741264343,-0.2583133578300476,-0.2829847037792206,-0.15938334167003632,0.3338487148284912,-0.28782200813293457,-0.3086772859096527,0.059139542281627655,-0.2682875394821167,-0.10825179517269135,0.33110183477401733,-0.049252551048994064,-0.26012519001960754,-0.3070678114891052,-0.24196463823318481,-0.265994668006897,0.28632882237434387,-0.27820858359336853,0.3594881296157837,-0.24599018692970276,0.3065657913684845,-0.009367884136736393,0.07621980458498001,0.2938257157802582,0.28143545985221863,-0.2694880962371826,0.1077818050980568,-0.23390954732894897,-0.2840259373188019,-0.2379721701145172,-0.3639185428619385,-0.27906543016433716,0.32315200567245483,-0.24052459001541138,0.3170490562915802,0.3486781716346741,0.2554963529109955,0.3152482211589813,-0.08479700237512589,0.2198617309331894,-0.23459099233150482,-0.22914843261241913,-0.16452425718307495,0.3787500262260437,-0.27599987387657166,-0.3131487965583801,-0.27397358417510986,0.07709967344999313,-0.32025784254074097,0.27639973163604736,-0.08425486087799072,0.2042345404624939,0.3747064471244812,-0.24700233340263367,0.09819445013999939,-0.23409193754196167,-0.22738468647003174,-0.260633260011673,-0.24981029331684113,0.21528802812099457,-0.2681933045387268,0.2928396761417389,-0.22715520858764648,0.02146957814693451,0.12809793651103973,0.13724835216999054,-0.24474754929542542,-0.34780430793762207,0.3027988076210022,0.058669544756412506,0.3281135559082031,-0.2960444390773773,-0.25036749243736267,-0.14031687378883362,-0.004101706203073263,0.3594881296157837,-0.2621605396270752,-0.08314750343561172,-0.32180511951446533,0.24658043682575226,-0.1813730150461197,0.07004369050264359,0.34024327993392944,0.26250624656677246,-0.10377229005098343,-0.2704661190509796,-0.2350158840417862,0.027854131534695625,-0.2662481963634491,-0.3313153386116028,-0.3150237500667572,0.2984255254268646,-0.2519679069519043,-0.2129761129617691,-0.26964640617370605,0.3594881296157837,-0.23785090446472168,-0.17093385756015778,0.10120483487844467,0.16574785113334656,-0.20266422629356384,0.3183898627758026,-0.23730875551700592,0.3383151888847351,0.16423764824867249,-0.3379163146018982,-0.2639252543449402,-0.3535916209220886,-0.24757054448127747,-0.11306702345609665,0.3313845098018646,-0.31858429312705994,-0.050477150827646255,-0.299710214138031,-0.32726868987083435,-0.1456812620162964,0.11100664734840393,-0.2324514538049698,0.0689273253083229,-0.354132741689682,-0.17275872826576233,-0.2716697156429291,-0.2904185354709625,0.3238394558429718,0.012416359968483448,-0.23995135724544525,-0.24521654844284058,0.3448059856891632,-0.04985984042286873,-0.23774568736553192,-0.10861857980489731,0.2941223382949829,-0.29484590888023376,-0.26331496238708496,0.2837523818016052,-0.26306843757629395,-0.11855166405439377,-0.31863540410995483,-0.25113409757614136,-0.05252249166369438,-0.2428825944662094,-0.282982736825943,-0.2637859582901001,-0.2529900372028351,-0.04834061488509178,0.19701223075389862,0.10765153169631958,0.24124011397361755,-0.267171174287796,0.30135971307754517,-0.03918718546628952,-0.18991614878177643,-0.2781214118003845,-0.27398961782455444,0.20094357430934906,-0.24776996672153473,0.3594881296157837,0.3688660264015198,0.06152661144733429,0.13465887308120728,-0.2417181134223938,0.04946601390838623,0.30251115560531616,0.050369929522275925,-0.01935107633471489,-0.2644714117050171,0.10477441549301147,-0.05081687122583389,-0.1103823184967041,0.2564353942871094,-0.06157972291111946,-0.07264219969511032,-0.25549837946891785,0.28780293464660645,-0.3239867687225342,0.17513878643512726,0.3411071002483368,0.3780154585838318,0.263619601726532,0.006958772893995047,0.18267177045345306,0.23315295577049255,-0.1466422975063324,0.10992535948753357,0.12295801192522049,-0.24515841901302338,-0.27143120765686035,0.3594881296157837,-0.3050195276737213,-0.2688848078250885,0.2840700149536133,0.22714519500732422,-0.319847971200943,-0.27494966983795166,-0.21734438836574554,-0.35189399123191833,0.3538060784339905,0.03687528148293495,-0.1565072238445282,0.30350226163864136,-0.014633061364293098,-0.2634141743183136,0.2793109118938446,-0.2888782322406769,-0.2712969183921814,0.06209181249141693,0.3608369827270508,0.3398013114929199,0.24658043682575226,-0.2704451084136963,0.3110032379627228,-0.27642178535461426,-0.2598536014556885,-0.27447566390037537,0.2949370741844177,-0.33052465319633484,-0.3296958804130554,-0.2703068256378174,-0.24112285673618317,0.3470006287097931,-0.3148493468761444,0.09924869239330292,0.05800012871623039,0.3180331587791443,-0.2397950291633606,0.27302461862564087,0.35273078083992004,-0.3157963752746582,0.3377018868923187,-0.3563314378261566,0.21064817905426025,-0.3061809837818146,0.3740721046924591,-0.33489790558815,-0.24612247943878174,0.2898182272911072,0.31867846846580505,-0.20408223569393158,0.37366122007369995,-0.24896349012851715,-0.08497237414121628,0.10957662016153336,-0.26306045055389404,-0.26557478308677673,0.13254737854003906,0.2835879921913147,-0.029102778062224388,-0.2638500928878784,-0.22714318335056305,-0.28101256489753723,-0.31184497475624084,-0.005254149436950684,-0.26936081051826477,0.2497100979089737,-0.17920441925525665,-0.32803237438201904,0.3622760474681854,0.37486475706100464,-0.2640695571899414,0.361080527305603,-0.2038046419620514,-0.28492283821105957,-0.2683746814727783,-0.09578132629394531,-0.04607180133461952,-0.23938417434692383,0.161372572183609,0.30650562047958374,0.24658043682575226,0.2583363950252533,-0.01131401676684618,-0.2450762540102005,-0.23748914897441864,0.303469181060791,0.3777298927307129,-0.3041336238384247,0.3084868788719177,0.28176313638687134,0.3310076594352722,0.14900732040405273,-0.0788583979010582,0.22330904006958008,-0.27576035261154175,-0.04472995176911354,-0.29612860083580017,-0.06415719538927078,0.2910709083080292,-0.2559312880039215,0.01584363915026188,0.3026103675365448,-0.3472190499305725,-0.12478990107774734,0.3594881296157837,-0.25165823101997375,-0.3042649030685425,-0.10561719536781311,-0.2667061388492584,-0.2730456292629242,-0.21734438836574554,-0.28216901421546936,-0.32256779074668884,-0.23894523084163666,0.24658043682575226,0.3250540494918823,-0.2417181134223938,-0.3478143513202667,-0.3342265188694,0.37141144275665283,-0.08498841524124146,0.3397652208805084,-0.26753494143486023,0.2306155562400818,-0.16458037495613098,-0.36108648777008057,-0.1961624175310135,0.3218011260032654,0.03269140049815178,-0.23900334537029266,-0.24071700870990753,0.31037384271621704,-0.03342294692993164,-0.23984414339065552,0.08262839913368225,-0.34357932209968567,-0.26980775594711304,-0.14352868497371674,0.34776726365089417,0.15699227154254913,-0.32936516404151917,-0.17927956581115723,-0.3668667674064636,-0.23847323656082153,0.31988710165023804,0.22964750230312347,0.34775421023368835,0.23521031439304352,-0.23849327862262726,0.060805078595876694,-0.2942245900630951,0.013555782847106457,-0.2836311161518097,-0.20548519492149353,0.1909313201904297,-0.27615422010421753,-0.26568302512168884,-0.24612247943878174,0.364391565322876,-0.21972443163394928,0.25559258460998535,0.33364322781562805,0.3737293779850006,-0.23938417434692383,0.10984016954898834,-0.2699049413204193,-0.2417181134223938,0.004205930046737194,-0.26375389099121094,0.02125312015414238,-0.27677956223487854,-0.030595948919653893,-0.319995254278183,-0.25351616740226746,0.16866204142570496,0.36940115690231323,-0.2742481827735901,-0.2660347521305084,0.31827566027641296,-0.26528313755989075,0.2862165868282318,0.16192975640296936,-0.3322964012622833,-0.1999644935131073,0.3594881296157837,-0.26958227157592773,-0.27469614148139954,0.06869583576917648,0.27594882249832153,-0.2644203305244446,-0.26665106415748596,-0.2643812298774719,0.2841171622276306,0.3084688186645508,-0.26921048760414124,0.18852420151233673,0.024756556376814842,-0.17515681684017181,0.08360347151756287,-0.21419669687747955,-0.24890337884426117,-0.2626706063747406,0.0061029596254229546,0.25817304849624634,-0.36758729815483093,0.3550346791744232,-0.2791004776954651,0.13539141416549683,0.22952525317668915,0.3324577510356903,-0.21734438836574554,0.08421477675437927,-0.28303584456443787,-0.269867867231369,-0.21734438836574554,-0.24528969824314117,-0.29092660546302795,-0.2455272078514099,-0.162961944937706,-0.23876383900642395,-0.3163065016269684,0.16652752459049225,0.30984875559806824,0.3334428369998932,-0.27091607451438904,-0.290601909160614,-0.30574601888656616,0.30549347400665283,-0.10086511075496674,0.2270439863204956,0.2878851294517517,-0.302959144115448,0.36947834491729736,-0.04240000620484352,-0.28608331084251404,-0.19473940134048462,-0.3528740704059601,-0.26396334171295166,0.3601225018501282,0.3597436845302582,0.3124372661113739,-0.11890942603349686,-0.3300235867500305,-0.3712100088596344,0.2636927664279938,-0.21734438836574554,-0.30108413100242615,-0.15171605348587036,-0.29679304361343384,-0.2417181134223938,-0.21506354212760925,0.14226700365543365,-0.16642628610134125,-0.27756020426750183,0.132624551653862,-0.24440883100032806,0.15941140055656433,0.2985437214374542,0.21401332318782806,0.029730116948485374,-0.2428034245967865,-0.23878589272499084,0.3024430572986603,-0.34881049394607544,-0.2830268144607544,-0.2696554362773895,-0.32325318455696106,0.3077222406864166,0.3490529954433441,-0.23777174949645996,-0.13681241869926453,-0.2626706063747406,-0.2685651183128357,0.35085880756378174,0.3360423445701599,-0.266110897064209,-0.23909656703472137,-0.37315112352371216,-0.3212689757347107,0.337564617395401,-0.26255834102630615,0.36201149225234985,-0.20894554257392883,-0.2181490957736969,-0.21561269462108612,-0.3086371421813965,0.23455491662025452,-0.2623489201068878,0.32473334670066833,-0.2654384672641754,0.25089961290359497,-0.33486485481262207,-0.21734438836574554,-0.04511977732181549,-0.348391592502594,0.1412578672170639,0.24658043682575226,-0.3596835136413574,-0.2620603144168854,-0.018634555861353874,0.18543565273284912,0.2971157431602478,-0.20663464069366455,0.3355422914028168,-0.2970886826515198,-0.26109224557876587,0.2927895486354828,0.3577624261379242,0.327877014875412,0.32020270824432373,-0.2781544625759125,-0.23819765448570251,-0.29271936416625977,0.01643889956176281,-0.057122260332107544,0.30457958579063416,-0.2417181134223938,-0.24612247943878174,-0.2190389782190323,-0.2754647433757782,0.24658043682575226,0.10517025738954544,-0.2805776298046112,0.25089260935783386,-0.2695191502571106,0.12385492771863937,-0.35086381435394287,-0.31350651383399963,-0.35515695810317993,0.07955989241600037,-0.2711736559867859,-0.21734438836574554,-0.3412213623523712,0.04982477426528931,0.269925981760025,0.06597104668617249,0.011638709343969822,0.31855425238609314,0.35894298553466797,-0.23759737610816956,-0.2987311780452728,-0.1643027812242508,-0.22548766434192657,0.1827349066734314,-0.1077016294002533,-0.24435971677303314,-0.19410304725170135,0.2112704962491989,0.2770230770111084,-0.2439989596605301,0.3458421230316162,0.3157693147659302,-0.3218502104282379,-0.20039339363574982,-0.14418406784534454,0.3594881296157837,-0.2631746530532837,-0.26893994212150574,-0.27129489183425903,-0.24612247943878174,-0.25601649284362793,0.0009570321417413652,0.15741315484046936,0.3001611828804016,-0.32026585936546326,-0.26938486099243164,-0.32541781663894653,-0.33245477080345154,-0.2312198430299759,-0.28548404574394226,0.016858790069818497,-0.34122434258461,-0.2690771818161011,0.3064645826816559,-0.2853367328643799,-0.24459223449230194,-0.26268163323402405,-0.33294379711151123,-0.3465336561203003,-0.2620151937007904,0.05923474580049515,0.16608455777168274,0.24658043682575226,-0.3519170582294464,-0.267291396856308,-0.2744976878166199,-0.23516219854354858,-0.2948428988456726,-0.30857306718826294,-0.28665149211883545,-0.2732650935649872,-0.08896184712648392,0.18525926768779755,0.31356266140937805,-0.23998643457889557,0.3774031400680542,-0.16405726969242096,-0.30029547214508057,-0.3350973427295685,0.3017365634441376,0.3418797254562378,-0.26018330454826355,0.31873565912246704,0.32519033551216125,-0.3453912138938904,0.3526485860347748,-0.2477419078350067,-0.13568803668022156,-0.22718927264213562,0.25665083527565,-0.2558220624923706,-0.05309370532631874,-0.2380543351173401,0.30002087354660034,0.20889344811439514,0.24921202659606934,-0.26987090706825256,0.20863792300224304,-0.2850731611251831,-0.3169918954372406,0.34234073758125305,-0.1573520302772522,-0.23712235689163208,-0.31597575545310974,-0.25521978735923767,0.29221129417419434,0.22172166407108307,0.2802729606628418,-0.1294558048248291,-0.26277783513069153,-0.21734438836574554,0.1833091378211975,0.1549479365348816,-0.03348508104681969,-0.16956494748592377,0.3333105444908142,-0.1448424756526947,-0.22912238538265228,0.06791818141937256,-0.23367103934288025,-0.24477159976959229,0.18966762721538544,-0.32706430554389954,0.16798360645771027,-0.2629541754722595,0.32990434765815735,-0.26968950033187866,-0.30825337767601013,-0.26931774616241455,-0.24113088846206665,-0.0820932686328888,-0.1393999308347702,-0.36009544134140015,-0.24238254129886627,0.3780815899372101,-0.269227534532547,0.3591533899307251,-0.2399553805589676,-0.2653392553329468,-0.2803310751914978,0.3422495126724243,-0.29207098484039307,-0.26600968837738037,0.28145647048950195,0.03403425216674805,-0.2455201894044876,-0.24612247943878174,0.01029585674405098,0.08780038356781006,-0.2711505889892578,-0.23388950526714325,-0.3124362528324127,-0.2183966189622879,-0.1304749697446823,0.24658043682575226,-0.33612555265426636,-0.32538172602653503,0.3126116096973419,-0.048561085015535355,-0.2453678697347641,-0.28858762979507446,0.25641030073165894,-0.29639220237731934,-0.35404857993125916,-0.3361796736717224,-0.21734438836574554,-0.09476817399263382,-0.2659796476364136,0.348379522562027,-0.2553199827671051,-0.2637839615345001,-0.052385199815034866,0.32627561688423157,-0.17246711254119873,0.271909236907959,0.375829815864563,-0.19594696164131165,-0.3726300001144409,0.3239727318286896,0.2857014834880829,-0.33185648918151855,0.21356134116649628,-0.14652003347873688,-0.29539307951927185,-0.2778387665748596,-0.28824490308761597,-0.32084107398986816,-0.2806968688964844,-0.2883290648460388,0.3182586431503296,-0.24884626269340515,-0.30713197588920593,-0.29313725233078003,-0.304473340511322,-0.28871288895606995,-0.24827805161476135,0.2911781370639801,-0.2624911963939667,-0.25354623794555664,-0.04395630955696106,0.30812710523605347,0.006970801390707493,0.02678586356341839,0.07003267109394073,-0.1070893257856369,-0.30184173583984375,0.2554282546043396,-0.2302858531475067,0.14067964255809784,0.24658043682575226,0.15990445017814636,-0.2551456093788147,-0.09279397875070572,-0.348290354013443,-0.333457887172699,0.16481487452983856,-0.25251707434654236,-0.24826200306415558,-0.2417181134223938,0.32092827558517456,0.0034212640020996332,-0.27834686636924744,-0.06001740321516991,0.3180040419101715,-0.29237866401672363,-0.07441697269678116,-0.2432834357023239,-0.3213110864162445,0.19195449352264404,-0.0745091661810875,-0.26168450713157654,-0.3308142423629761,0.0011514467187225819,-0.27724453806877136,-0.23802727460861206,-0.26395633816719055,-0.31827566027641296,-0.26658791303634644,-0.24298781156539917,-0.1786321997642517,-0.2654154300689697,-0.08907008171081543,-0.2093193531036377,0.2889794707298279,0.31209051609039307,-0.23733481764793396,-0.23289740085601807,-0.2717859447002411,-0.27873870730400085,-0.23876886069774628,0.15033112466335297,-0.2773808240890503,0.34148889780044556,-0.23175397515296936,0.10867470502853394,0.3404637277126312,-0.3102074861526489,0.3594881296157837,-0.23612023890018463,-0.014619031921029091,-0.25119826197624207,-0.15027600526809692,0.3436124324798584,0.36663633584976196,0.3199572265148163,-0.015845635905861855,0.3020211160182953,-0.30271559953689575,0.3142591416835785,-0.26450350880622864,-0.2569745182991028,0.3271454870700836,-0.3308423161506653,-0.24966999888420105,-0.24120904505252838,0.22437329590320587,-0.3224334418773651,-0.25911906361579895,0.31869855523109436,-0.29106590151786804,-0.2379371076822281,0.07912095636129379,0.06909167766571045,-0.33793434500694275,0.29795753955841064,-0.31194519996643066,-0.29300999641418457,0.24658043682575226,-0.3286046087741852,-0.09101420640945435,-0.26669415831565857,-0.3012906014919281,-0.2799813449382782,0.003501434810459614,-0.24437876045703888,-0.23928596079349518,-0.2970215380191803,-0.30617794394493103,-0.1034325659275055,-0.11768782883882523,0.33381766080856323,0.3410349488258362,-0.019796019420027733,-0.3153734803199768,0.11007767170667648,-0.19950149953365326,-0.23807337880134583,-0.2778688967227936,-0.23934108018875122,-0.2851102352142334,-0.35122960805892944,-0.20373648405075073,0.01340345200151205,0.122075155377388,-0.2451854795217514,-0.1667710244655609,-0.011625676415860653,-0.26535728573799133,-0.2405877262353897,-0.28447288274765015,0.3156370222568512,-0.25462350249290466,-0.24255891144275665,0.050980228930711746,0.15315712988376617,0.2647079527378082,-0.0688701942563057,-0.3423868417739868,0.3280854821205139,0.30334195494651794,-0.27074572443962097,-0.24010469019412994,-0.26403748989105225,-0.024750540032982826,0.07163206487894058,0.31827566027641296,0.3338196277618408,0.2835519015789032,0.36598190665245056,0.24658043682575226,-0.2956165373325348,0.13280993700027466,-0.26987889409065247,0.039796486496925354,0.27977994084358215,-0.2603406310081482,-0.06322520971298218,-0.0026716680731624365,-0.31135493516921997,0.046120911836624146,-0.2530241310596466,-0.21734438836574554,-0.14030884206295013,-0.20763877034187317,-0.2963791489601135,-0.16388489305973053,0.11888737231492996,0.35507476329803467,0.2810215651988983,0.1965412199497223,-0.3247814178466797,0.2825598418712616,-0.27334025502204895,0.1044156551361084,-0.24676784873008728,0.37723883986473083,0.3482101857662201,0.25049877166748047,0.3283941149711609,0.2589988112449646,0.27156949043273926,0.2645295262336731,-0.3611927628517151,0.3787490129470825,0.3694041669368744,-0.26684144139289856,0.0697290226817131,-0.26837170124053955,0.2740668058395386,-0.2417181134223938,-0.28993749618530273,0.24658043682575226,-0.2633851170539856,-0.31330108642578125,-0.24612247943878174,-0.2792748510837555,-0.26072847843170166,-0.2417181134223938,-0.023384639993309975,0.046083830296993256,-0.23994536697864532,-0.14010341465473175,-0.26269665360450745,-0.1122262254357338,-0.3008827269077301,-0.029290175065398216,0.14487655460834503,-0.23480845987796783,-0.2388981282711029,-0.34916219115257263,0.29755863547325134,-0.267229288816452,-0.3253386616706848,0.3083235025405884,-0.24459722638130188,-0.2931252419948578,-0.23721155524253845,-0.042692627757787704,-0.2990317940711975,-0.16185759007930756,0.2833985686302185,-0.24504518508911133,-0.3121556341648102,0.2884032428264618,-0.3133101165294647,0.18226391077041626,-0.0855024978518486,0.2043568193912506,-0.3096994161605835,0.02994556725025177,-0.3535996377468109,0.3200083374977112,0.2702256441116333,-0.26439425349235535,0.22318878769874573,-0.26458364725112915,-0.26252126693725586,-0.21734438836574554,-0.24527065455913544,0.05431530252099037,-0.24108076095581055,-0.3471539616584778,-0.2787367105484009,-0.35081368684768677,-0.27290430665016174,-0.23453287780284882,-0.27701807022094727,-0.25971734523773193,-0.2498113065958023,0.3114301264286041,0.3372960090637207,-0.2633390426635742,0.11080321669578552,-0.2237810492515564,-0.31340330839157104,-0.2448938637971878,-0.2384040802717209,-0.2266741842031479,-0.2909947335720062,-0.24046145379543304,-0.09669426083564758,0.2837613821029663,-0.08187680691480637,-0.26513683795928955,-0.26087477803230286,-0.26504963636398315,0.3052810728549957,0.25632214546203613,-0.2399483621120453,-0.3109099864959717,0.24658043682575226,-0.3162924647331238,0.29341989755630493,-0.24268116056919098,0.31152433156967163,-0.3218432366847992,-0.22648879885673523,0.29425567388534546,-0.3599450886249542,0.3733084499835968,0.10824478417634964,-0.24612247943878174,0.35949912667274475,-0.28463825583457947,0.3054504096508026,-0.2529178857803345,-0.09574525058269501,0.11294977366924286,0.31829965114593506,0.05221784859895706,0.3594881296157837,0.28589487075805664,0.15975211560726166,0.3463742733001709,0.3731461465358734,-0.2649093568325043,-0.3065817952156067,-0.261505126953125,0.15369126200675964,-0.23816457390785217,0.3594881296157837,-0.269505113363266,-0.08559369295835495,-0.2835719883441925,-0.08624307811260223,-0.11440586298704147,-0.31356462836265564,-0.24800245463848114,-0.2790243327617645,-0.28129616379737854,0.011440289206802845,0.33123916387557983,-0.2621374726295471,-0.18727654218673706,-0.07643525302410126,-0.26345527172088623,-0.26265957951545715,0.1878477782011032,0.35758909583091736,0.32286638021469116,-0.35108131170272827,0.16649143397808075,-0.21458353102207184,0.2227959781885147,0.24683399498462677,-0.11553626507520676,-0.2431451380252838,-0.2591291069984436,-0.24279239773750305,-0.2804242968559265,-0.23894423246383667,-0.08597751706838608,0.37486475706100464,-0.22781962156295776,0.18931087851524353,-0.2640344798564911,0.0417165607213974,0.12744152545928955,-0.24491189420223236,0.3251853287220001,-0.18922066688537598,0.3157743513584137,0.34540024399757385,0.17220155894756317,-0.31347545981407166,-0.2364349067211151,-0.2623559236526489,-0.26084771752357483,-0.06228521838784218,0.300409734249115,-0.23829986155033112,-0.3136097192764282,0.136325404047966,-0.2851603627204895,-0.2400195151567459,-0.28893837332725525,-0.28012263774871826,-0.32519835233688354,0.31956836581230164,0.3504469394683838,-0.25280869007110596,0.3735189437866211,-0.2417181134223938,-0.2417181134223938,0.3330910801887512,0.353519469499588,-0.17274871468544006,0.31314072012901306,0.2876375913619995,0.17326480150222778,-0.18478025496006012,-0.2392178177833557,0.23485957086086273,-0.27458491921424866,-0.27382224798202515,-0.24872499704360962,-0.14800116419792175,0.32844820618629456,-0.3536938428878784,-0.039690252393484116,0.3778671324253082,-0.18970870971679688,-0.354265034198761,-0.12063609063625336,0.3572523891925812,0.30238690972328186,-0.31788280606269836,-0.2712177336215973,-0.30283185839653015,-0.21734438836574554,-0.14659619331359863,-0.11375147104263306,-0.2657000422477722,0.26568201184272766,0.32242849469184875,-0.23798519372940063,-0.23121081292629242,-0.2448006570339203,-0.14134906232357025,0.07634607702493668,0.20656049251556396,-0.21734438836574554,-0.26942893862724304,0.3594881296157837,-0.322950541973114,-0.2417181134223938,-0.3631899952888489,0.3335881531238556,0.2912081778049469,-0.34848374128341675,-0.1015155017375946,-0.262656569480896,-0.2694529891014099,-0.23791004717350006,0.3204823136329651,-0.3470958471298218,-0.25645437836647034,-0.254159539937973,-0.26417678594589233,-0.3192707896232605,-0.26327189803123474,-0.3482613265514374,0.3626718819141388,0.31578436493873596,-0.04207531362771988,-0.31634750962257385,-0.2949741780757904,0.0459565594792366,-0.26338014006614685,-0.2624591588973999,-0.2431020587682724,-0.24048550426959991,-0.25959405303001404,-0.32061460614204407,-0.32838812470436096,-0.1786983460187912,-0.242898628115654,0.31182795763015747,-0.2435389906167984,-0.10806139558553696,-0.13066236674785614,0.2541314959526062,0.2613217234611511,-0.30920740962028503,0.324801504611969,-0.3278990387916565,-0.25301408767700195,-0.31556084752082825,0.3736070990562439,-0.24272124469280243,-0.3748096823692322,0.37961286306381226,-0.2609509825706482,-0.21734438836574554,0.37860268354415894,-0.20197375118732452,0.35741373896598816,-0.23326517641544342,-0.23868468403816223,-0.047333478927612305,0.2918405532836914,-0.29315632581710815,0.24658043682575226,-0.07116205990314484,0.17114631831645966,0.31272587180137634,0.24658043682575226,-0.3270091712474823,0.13621817529201508,0.25151291489601135,-0.21734438836574554,-0.24253785610198975,-0.036199845373630524,0.101909339427948,0.3594881296157837,0.32486462593078613,0.26566097140312195,-0.279207706451416,-0.27161258459091187,0.22031669318675995,0.2744726836681366,-0.2691253125667572,-0.3143773376941681,0.3183096945285797,-0.29323145747184753,-0.2965465188026428,0.24658043682575226,-0.35388222336769104,-0.297945499420166,0.3034552037715912,0.28326234221458435,0.2448066771030426,0.29374659061431885,-0.19731184840202332,-0.2756040394306183,-0.3225817382335663,-0.23579354584217072,0.2944771647453308,-0.2869892120361328,0.2512704133987427,-0.2800605297088623,-0.3673357665538788,-0.3410569727420807,0.23791806399822235,-0.31864240765571594,-0.26435816287994385,-0.2991119623184204,-0.3210555613040924,0.15844033658504486,-0.25918519496917725,-0.2922563850879669,-0.3220787048339844,0.034867022186517715,-0.26320573687553406,-0.21734438836574554,-0.35680142045021057,0.03611766919493675,-0.2071988433599472,0.28590694069862366,-0.26928967237472534,0.041240546852350235,-0.32860955595970154,-0.18850015103816986,-0.001611419953405857,-0.27297645807266235,0.040975987911224365,0.3512346148490906,0.2837704122066498,-0.29917508363723755,-0.010228711180388927,0.059462226927280426,-0.10055144876241684,0.3594881296157837,0.1922270655632019,-0.23895125091075897,-0.22787773609161377,-0.250622034072876,-0.34617185592651367,-0.022651081904768944,-0.21514572203159332,-0.32216089963912964,-0.24163895845413208,-0.2467668354511261,-0.2534099221229553,0.3303161859512329,-0.1428612768650055,0.2870393395423889,-0.2677583694458008,0.35330402851104736,0.3716379404067993,-0.2691042423248291,-0.23889712989330292,-0.028012461960315704,0.28200265765190125,0.1039997786283493,-0.21734438836574554,-0.11688913404941559,0.108597531914711,-0.12123435735702515],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('361fdc6f-0e7e-43d8-ac40-5a44ce0d7ccc');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x,y,z=np.array(pre_enc_xyz[1].permute(1,0)).T\n",
        "#c = np.array(batch['category'][3]).T\n",
        "#c = np.array(pred_np[0]).T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, \n",
        "                                   mode='markers',\n",
        "                                   marker=dict(\n",
        "        size=30,\n",
        "        #color=c,                # set color to an array/list of desired values\n",
        "        colorscale='Viridis',   # choose a colorscale\n",
        "        opacity=1.0\n",
        "    ))])\n",
        "fig.update_traces(marker=dict(size=2,\n",
        "                              line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oLMz29B_lneq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "2110b2d7-c641-4a96-c4d2-1a02e4b29d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"0e273a2e-deec-4211-8115-3e9d59d702eb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0e273a2e-deec-4211-8115-3e9d59d702eb\")) {                    Plotly.newPlot(                        \"0e273a2e-deec-4211-8115-3e9d59d702eb\",                        [{\"marker\":{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"opacity\":1.0,\"size\":2,\"line\":{\"color\":\"DarkSlateGrey\",\"width\":2}},\"mode\":\"markers\",\"x\":[0.24030862748622894,-0.2590193450450897,-0.2746244966983795,-0.2748759984970093,0.2454184740781784,0.274836927652359,0.274836927652359,-0.26320624351501465,-0.18374355137348175,0.24584338068962097,-0.2746836245059967,0.251726895570755,0.2674863338470459,-0.22951070964336395,0.2593059539794922,-0.22053465247154236,0.24802900850772858,-0.012338696978986263,0.015095540322363377,0.036628250032663345,-0.2671877145767212,0.01870720647275448,-0.026090890169143677,-0.2700347602367401,0.09398701786994934,-0.0244063138961792,0.22870700061321259,-0.22874608635902405,-0.03631860390305519,0.2192068248987198,0.26627275347709656,-0.0012100731255486608,0.25136709213256836,-0.2432548850774765,-0.2748759984970093,-0.23813802003860474,0.2832898795604706,0.19965332746505737,-0.2671877145767212,0.2311832457780838,0.20104527473449707,-0.1476619690656662,-0.228851318359375,-0.1930292695760727,0.12941020727157593,-0.11942902952432632,0.10790656507015228,0.12564623355865479,-0.055668678134679794,0.08676969259977341,0.26714861392974854,-0.13195061683654785,-0.12814953923225403,0.11576424539089203,-0.017191998660564423,-0.2572145164012909,-0.28230077028274536,-0.03368300572037697,0.08247056603431702,-0.09254796802997589,-0.07861638814210892,-0.1467580497264862,0.1004527360200882,-0.23590828478336334,-0.28287699818611145,-0.07777661085128784,0.09888942539691925,0.22793936729431152,0.1993967741727829,0.26714861392974854,0.07980290055274963,0.10192485898733139,0.2794627547264099,-0.12737490236759186,-0.2496885508298874,0.2682008445262909,-0.22874608635902405,0.24350641667842865,0.2450035959482193,-0.05688726529479027,-0.2515154182910919,-0.014905145391821861,0.17080608010292053,0.23953698575496674,0.27401816844940186,-0.11558085680007935,0.025488603860139847,-0.16504888236522675,-0.0058759781531989574,0.26240652799606323,0.22870700061321259,-0.2592238187789917,-0.28754791617393494,0.29013538360595703,-0.2748759984970093,-0.22874608635902405,-0.26999062299728394,0.26307594776153564,0.05258813127875328,-0.2544967532157898,-0.002583989640697837,-0.09012983739376068,-0.0866534486413002,-0.2671877145767212,0.03931595757603645,0.1791798621416092,-0.18058085441589355,0.09600930660963058,-0.13295674324035645,-0.21451087296009064,-0.2804778814315796,-0.17957070469856262,0.12051532417535782,-0.24321380257606506,0.11579631268978119,0.06276272982358932,0.20391537249088287,-0.06776335090398788,0.16738882660865784,-0.05931040644645691,-0.24609792232513428,0.24966548383235931,0.1925853192806244,0.24681143462657928,-0.162262961268425,-0.08371321856975555,0.2903288006782532,0.16376513242721558,-1.9545792383723892e-05,-0.19000786542892456,0.14596736431121826,-0.24625827372074127,0.08420424163341522,-0.24799494445323944,-0.12653110921382904,0.2058795541524887,0.19808800518512726,-0.054486166685819626,-0.03692789375782013,-0.18971022963523865,0.04010763764381409,0.2700507640838623,-0.07649288326501846,0.08157466351985931,0.07466398924589157,0.28523901104927063,-0.14743147790431976,-0.09267523884773254,0.18843449652194977,0.050706133246421814,0.26059970259666443,0.10452939569950104,0.00406211894005537,-0.20249135792255402,-0.00018289254512637854,-0.04128314182162285,0.20963652431964874,-0.1785304844379425,0.13393481075763702,-0.05970524623990059,-0.19857604801654816,-0.19684436917304993,0.06662392616271973,-0.24759510159492493,-0.2533092200756073,0.012183358892798424,0.21683278679847717,0.24752695858478546,-0.10626107454299927,-0.016604753211140633,0.19686339795589447,0.19288796186447144,-0.2326764315366745,0.10296507179737091,-0.22438383102416992,0.08156363666057587,0.22870700061321259,-0.24649778008460999,0.22961893677711487,0.18265122175216675,0.2504100799560547,0.04276427626609802,0.26552414894104004,-0.22874608635902405,-0.2487625926733017,-0.2849504053592682,-0.10872030258178711,-0.07515103369951248,0.01947483792901039,0.1996513158082962,0.05313829705119133,-0.26220113039016724,-0.29025065898895264,-0.22932732105255127,-0.09876416623592377,-0.06617999076843262,0.23530100286006927,0.12611623108386993,0.07439642399549484,-0.2748759984970093,-0.25334930419921875,-0.24180079996585846,0.2727384567260742,-0.1072642058134079,0.23711785674095154,0.25506094098091125,-0.01980254240334034,-0.2905683219432831,-0.2734459936618805,0.20348045229911804,-0.08985525369644165,-0.2616930305957794,0.13167902827262878,-0.06618499755859375,-0.2087065428495407,0.2554207146167755,-0.23875835537910461,0.25710225105285645,-0.20664216578006744,0.26392677426338196,-0.22891244292259216,-0.2574891149997711,-0.2748759984970093,-0.21785196661949158,0.274836927652359,0.03161561116576195,0.209150493144989,0.04029102623462677,0.24772337079048157,-0.21737495064735413,0.24612095952033997,0.25104042887687683,0.2474958747625351,-0.14108599722385406,-0.11530426889657974,-0.23431290686130524,-0.2550429105758667,-0.08367713540792465,0.14889056980609894,0.26726284623146057,0.2658197581768036,-0.024041540920734406,-0.23962418735027313,0.2301410436630249,-0.004306646529585123,0.2675805389881134,0.274836927652359,0.17707538604736328,0.026442628353834152,0.034129947423934937,0.23449328541755676,-0.03040805459022522,0.2384095937013626,0.22100363671779633,0.19618697464466095,0.05273043364286423,-0.14998789131641388,-0.10830642282962799,-0.2748759984970093,0.1067982092499733,-0.11609896272420883,0.016874317079782486,-0.13405808806419373,-0.024981535971164703,-0.26185837388038635,0.2460598349571228,-0.08546794205904007,0.26714861392974854,-0.16616222262382507,-0.062006134539842606,-0.2735251486301422,0.2623434066772461,-0.1797330528497696,0.2253689169883728,0.26150161027908325,0.270699143409729,0.27181050181388855,-0.2621901035308838,0.25275102257728577,-0.2633334994316101,-0.2350284308195114,-0.22874608635902405,0.15699276328086853,0.24341821670532227,0.195118710398674,0.26117992401123047,0.09916701167821884,-0.2906906008720398,-0.23102793097496033,-0.03189621493220329,0.08750525116920471,0.2460598349571228,0.09188354760408401,-0.2053854912519455,0.22898559272289276,-0.13142649829387665,0.274836927652359,0.1486961543560028,0.14532901346683502,-0.20900116860866547,-0.008861315436661243,-0.23543429374694824,-0.12208767235279083,0.2460598349571228,-0.2550429105758667,-0.06215444579720497,0.23094473779201508,-0.1467871069908142,-0.23032544553279877,0.22870700061321259,0.24679338932037354,-0.17659136652946472,-0.26562535762786865,-0.26749834418296814,0.0590708926320076,0.029943060129880905,-0.2075130194425583,-0.26180729269981384,-0.24204231798648834,-0.2553325295448303,0.14870718121528625,-0.20840592682361603,0.22877314686775208,0.0008422837127000093,0.2764323055744171,-0.2160952389240265,0.027466801926493645,-0.2143104523420334,0.0440189391374588,-0.2469557374715805,0.1620514988899231,0.017689047381281853,-0.036640286445617676,0.058266181498765945,-0.24609792232513428,-0.05695641413331032,0.061918940395116806,0.2460598349571228,-0.24786368012428284,-0.1963954120874405,0.19950200617313385,0.05239572003483772,0.21802231669425964,0.02323281392455101,0.20207245647907257,0.27233460545539856,0.26729390025138855,0.01552946213632822,0.21896031498908997,-0.17254579067230225,0.22363223135471344,-0.19862915575504303,-0.2553134858608246,0.22090142965316772,-0.26659443974494934,0.2550860047340393,0.066851407289505,-0.28331291675567627,-0.033985648304224014,-0.08994143456220627,0.22329050302505493,-0.016778122633695602,0.274836927652359,-0.08999955654144287,0.16374309360980988,0.03892813250422478,0.267466276884079,-0.24609792232513428,0.2683311104774475,0.1305636614561081,0.281963050365448,0.24119751155376434,0.03439050167798996,-1.9545792383723892e-05,0.1042337641119957,-0.2578108012676239,-0.2671877145767212,-0.13514138758182526,0.23236075043678284,-0.054619450122117996,-0.26400694251060486,-0.2024272233247757,-0.2775056064128876,-0.26798540353775024,0.2433270364999771,-0.07937400043010712,-0.19387206435203552,0.13779902458190918,0.049822259694337845,-0.23168933391571045,0.0738823413848877,-0.0009936135029420257,0.26909974217414856,-0.07793594896793365,-0.15546151995658875,0.0948057472705841,-0.25316593050956726,0.1455935537815094,0.2627853453159332,-0.23955804109573364,0.2905522882938385,0.027869656682014465,-0.2345844805240631,-0.07040095329284668,-0.26992252469062805,-0.23409044742584229,0.14905792474746704,-0.1330329179763794,0.07399357110261917,0.13705243170261383,-0.2522880434989929,0.10967030376195908,0.28895488381385803,0.14217530190944672,-0.25350263714790344,0.14587515592575073,-0.2748759984970093,-0.01713888719677925,0.15453355014324188,0.27369049191474915,-0.21288742125034332,-0.036827683448791504,0.21074587106704712,-0.24068042635917664,0.17804445326328278,0.25523629784584045,-0.02388019859790802,-0.23058199882507324,-0.14357829093933105,-0.11937692016363144,-0.08539177477359772,0.26714861392974854,0.07960748672485352,-0.20140805840492249,0.23977850377559662,0.014238721691071987,-0.24807211756706238,-0.25403377413749695,-0.2304898053407669,-0.26616454124450684,0.08477946370840073,0.21350473165512085,-0.25694793462753296,-0.23888562619686127,-0.08398378640413284,-0.1972832977771759,0.09926922619342804,-0.06907914578914642,0.2511315941810608,-0.08764355629682541,-0.21331533789634705,0.27332472801208496,0.09865592420101166,0.2673790752887726,-0.26792725920677185,-0.2748759984970093,0.22998671233654022,0.22870700061321259,-0.2734109163284302,0.06298419833183289,0.11866138875484467,0.17698420584201813,0.03316088765859604,0.07270282506942749,-0.24609792232513428,-0.11223074793815613,0.07801510393619537,-0.05002770572900772,-0.10594841837882996,0.22344282269477844,-0.22874608635902405,0.051923718303442,-0.27886849641799927,0.034298308193683624,0.2340022474527359,0.22870700061321259,-0.08270206302404404,0.23488612473011017,0.028790613636374474,0.16214269399642944,-0.22874608635902405,-0.22874608635902405,-0.1684330552816391,-0.00793334562331438,0.1742544025182724,0.274836927652359,0.2324289083480835,0.10264238715171814,-0.28382501006126404,0.09558841586112976,0.2735642194747925,0.27361834049224854,-0.0333893857896328,0.04696018621325493,0.23416860401630402,0.26928213238716125,-0.21389155089855194,0.00439983606338501,-0.029585309326648712,0.07590662688016891,-0.2748759984970093,-0.289409875869751,0.016480479389429092,-0.03881390020251274,-0.2734670341014862,-0.2031698077917099,0.22870700061321259,-0.27152591943740845,0.274836927652359,-0.23026330769062042,-0.22874608635902405,-0.15717315673828125,0.25017958879470825,-0.13656240701675415,-0.2717333436012268,-0.2748759984970093,-0.28155818581581116,-0.2748759984970093,-0.24365974962711334,0.2381991446018219,-0.13643613457679749,0.22870700061321259,0.07487443834543228,0.24577723443508148,-0.19165335595607758,-0.09989956766366959,-0.0072899796068668365,-0.26087430119514465,-0.264534056186676,0.07551480084657669,-0.2748759984970093,0.0014986770693212748,-0.14054083824157715,-0.22874608635902405,0.13518346846103668,0.24366675317287445,-0.18042652308940887,0.19271259009838104,-0.030510272830724716,-0.04077807068824768,-0.019305486232042313,0.19644249975681305,0.13823093473911285,0.22870700061321259,0.2606668472290039,-0.08628267049789429,0.2643596827983856,-0.13362115621566772,-0.16719341278076172,0.0914255753159523,-0.16961155831813812,0.27108898758888245,0.2217331975698471,0.051661163568496704,-0.050416529178619385,-0.055038340389728546,-0.22604034841060638,0.22870700061321259,0.022783860564231873,-0.23303718864917755,0.22870700061321259,0.08064669370651245,-0.28395429253578186,-0.02551065944135189,-0.06596453487873077,0.2179020643234253,0.24459172785282135,0.21352876722812653,-0.2748759984970093,0.24413074553012848,0.13838525116443634,0.23771211504936218,0.274836927652359,-0.22794638574123383,0.28715208172798157,-0.023345062509179115,0.24279189109802246,0.012263529002666473,-0.06797180324792862,-0.22874608635902405,0.2246343493461609,0.267739862203598,-0.08300571143627167,-0.2369164377450943,-0.22882525622844696,0.23251508176326752,-0.07417396456003189,-0.13469043374061584,0.22870700061321259,-0.24725638329982758,0.2354302704334259,0.08954358100891113,0.07682958990335464,0.274836927652359,-0.029520170763134956,0.28893986344337463,-0.17062070965766907,0.26714861392974854,0.05278655141592026,-0.28153616189956665,-0.2172456830739975,-0.24207638204097748,-0.2641783058643341,0.274836927652359,-0.07340232282876968,-0.2671877145767212,-0.2748759984970093,0.274836927652359,-0.23075637221336365,0.289306640625,0.09891447424888611,0.21247054636478424,0.2336023896932602,0.274836927652359,-0.19946794211864471,0.02726737968623638,-0.07233506441116333,0.21081502735614777,0.274836927652359,-0.26437172293663025,0.25562813878059387,0.26714861392974854,-0.231183260679245,-0.13095450401306152,-0.1898956298828125,0.274836927652359,-0.22874608635902405,0.24617408215999603,0.274836927652359,0.2284294068813324,-0.2671877145767212,-0.2748759984970093,-0.27349308133125305,0.01739642396569252,-0.24746383726596832,-0.22874608635902405,-0.11290618032217026,-0.10692348331212997,0.06235887482762337,-0.2660492956638336,-0.26695922017097473,0.274836927652359,-0.20649786293506622,-0.22874608635902405,0.274836927652359,-0.019982924684882164,-0.22533786296844482,0.26714861392974854,-0.2748759984970093,0.14519771933555603,-0.22874608635902405,0.274836927652359,0.2694013714790344,0.22870700061321259,0.22870700061321259,-0.27894866466522217,0.26714861392974854,0.08627364039421082,-0.08756138384342194,-0.22874608635902405,0.24816931784152985,0.0993373692035675,-0.22874608635902405,-0.23552848398685455,-0.23957808315753937,-0.2226080596446991,-0.2748759984970093,0.23554952442646027,0.13775892555713654,-0.22767682373523712,-0.056405242532491684,-0.2748759984970093,-0.1945504993200302,0.04643506929278374,0.16757923364639282,-0.2748759984970093,0.2146661877632141,-0.22874608635902405,0.059269312769174576,0.09487289190292358,-0.0805695429444313,-0.15755096077919006,-0.08088821917772293,-0.059802453964948654,0.2306511402130127,0.24745279550552368,0.23005184531211853,0.2672748863697052,0.2460598349571228,-0.2696268856525421,-0.022285813465714455,-0.26979321241378784,-0.041311200708150864,-0.11175373941659927,0.17422935366630554,0.2663208544254303,0.2460598349571228,-0.23265036940574646,-0.27921122312545776,-0.22874608635902405,-0.24609792232513428,-0.17325930297374725,-0.22240763902664185,-0.04010363668203354,0.06888473033905029,0.21480047702789307,0.11881371587514877,0.18676497042179108,0.010670145973563194,-0.2748759984970093,0.2279694378376007,0.27177441120147705,0.1588386744260788,-0.0907401293516159,0.27674800157546997,-0.11871851980686188,-0.2504010796546936,-0.24654388427734375,0.16964763402938843,0.2297922968864441,0.035774439573287964,0.29081985354423523,0.22870700061321259,0.22870700061321259,-0.23558060824871063,-0.2799818515777588,0.26714861392974854,0.21972793340682983,-0.28949806094169617,-0.04717564955353737,0.22325442731380463,-0.23876334726810455,-0.09732811897993088,-0.16741789877414703,-0.014115468598902225,-0.2652726173400879,-0.2865878939628601,-0.0834326222538948,0.13718871772289276,0.034434594213962555,0.16700702905654907,-0.2748759984970093,0.2350233942270279,0.2701379656791687,0.19011907279491425,-0.1512024700641632,-0.08128806948661804,0.26842230558395386,-0.2671877145767212,-0.24609792232513428,-0.24344028532505035,-0.013900011777877808,0.22892345488071442,-0.2748759984970093,0.22870700061321259,0.13396187126636505,0.22989551723003387,-0.16659817099571228,0.09184647351503372,0.056051481515169144,0.22996367514133453,0.20041394233703613,-0.22874608635902405,0.14891061186790466,0.24274379014968872,-0.07228795439004898,-0.07066752016544342,-0.24609792232513428,0.2273360788822174,0.274836927652359,-0.037644416093826294,-0.2363341897726059,0.2296099066734314,-0.011802558787167072,0.274836927652359,-0.23674306273460388,-0.111569344997406,-0.24609792232513428,0.10477592051029205,-0.05945771932601929,-0.1838337481021881,-0.1302790641784668,-0.23269547522068024,-0.24305547773838043,0.28222861886024475,0.22870700061321259,0.26714861392974854,0.274836927652359,-0.02943098172545433,-0.22874608635902405,-0.26357904076576233,0.2097778171300888,-0.015567551366984844,-0.27327460050582886,0.2233947217464447,0.2579931616783142,0.22870700061321259,0.03304263949394226,-0.2671877145767212,0.274836927652359,0.04438471421599388,0.06917333602905273,-0.1423647105693817,0.22870700061321259,-0.25287729501724243,0.274836927652359,-0.1476840078830719,0.19443926215171814,-0.08788606524467468,0.18770495057106018,0.03734176605939865,-0.1614833027124405,-0.1293380707502365,-0.1501091718673706,0.05646035075187683,-0.23084554076194763,0.10299915075302124,0.0872797742486,0.13880915939807892,0.25171786546707153,0.22870700061321259,-0.2748759984970093,-0.01236374955624342,0.21241642534732819,-0.03010140359401703,-0.21910563111305237,-0.2514282464981079,-0.07784876227378845,0.2741945683956146,0.24926665425300598,0.26714861392974854,-0.2748759984970093,0.2700207233428955,0.2168307900428772,-0.12849026918411255,0.23631715774536133,0.274836927652359,-0.0666479840874672,0.26569753885269165,0.057380303740501404,0.24576520919799805,-0.11723737418651581,0.24701987206935883,0.15712504088878632,-0.22874608635902405,-0.2671877145767212,0.27334076166152954,-0.1824367791414261,0.22870700061321259,-0.04018080234527588,-0.24609792232513428,-0.2748759984970093,0.18424460291862488,0.02863428182899952,0.022519299760460854,0.045928992331027985,-0.09638110548257828,-0.1500881165266037,0.274836927652359,0.2585693895816803,-0.044934894889593124,-0.2057202160358429,-0.23300111293792725,-0.24252234399318695,0.2460598349571228,0.2460598349571228,-0.07212962210178375,0.025969624519348145,0.2313646376132965,0.22870700061321259,-0.13747534155845642,0.1424739509820938,-0.28178468346595764,0.2576363980770111,-0.16521623730659485,-0.24841885268688202,0.02626725658774376,-0.22874608635902405,0.26714861392974854,-0.2748759984970093,-0.2743208408355713,0.2737245559692383,-0.22086235880851746,0.22870700061321259,-0.2823067903518677,-0.20451365411281586,0.22550921142101288,0.018102925270795822,0.13555625081062317,0.19593141973018646,-0.2573728561401367,-0.1880386769771576,-0.2359473705291748,-0.005472120828926563,-0.21164779365062714,0.11558987200260162,0.274836927652359,0.039358045905828476,0.06327281147241592,0.1743856817483902,-0.2789817452430725,-0.13955575227737427,0.22870700061321259,0.10522586852312088,0.027656204998493195,-0.23021219670772552,-0.058646999299526215,-0.02497151494026184,0.06721718609333038,0.026837466284632683,0.015893233940005302,-0.10286185890436172,0.22870700061321259,0.274836927652359,0.011799543164670467,-0.2671877145767212,-0.017936579883098602,-0.231125146150589,0.0034037211444228888,-0.29084891080856323,0.035256337374448776,0.053521107882261276,0.2053704559803009,0.22870700061321259,-0.01677611656486988,-0.21590082347393036,-0.22874608635902405,0.18079128861427307,0.24114741384983063,-0.0513044148683548,-0.020889850333333015,0.10972742736339569,-0.10911913961172104,0.09225333482027054,0.05736827850341797,-0.28385308384895325,-0.10667996108531952,0.28397130966186523,0.26714861392974854,-0.2748759984970093,-0.260223925113678,0.04994051158428192,0.023449275642633438,0.07795197516679764,-0.2510644495487213,0.1715737134218216,-0.2313786745071411,-0.20421601831912994,0.24781957268714905,-0.12647098302841187,0.052357640117406845,-0.045874886214733124,0.19165533781051636,0.009293223731219769,-0.20260559022426605,0.16012340784072876,0.09699039161205292,-0.27760881185531616,0.2815852463245392,-0.2851698696613312,0.23424875736236572,-0.24754701554775238,-0.24609792232513428,0.05055481567978859,-0.21583768725395203,-0.1711057424545288,-0.12089213728904724,-0.20539051294326782,-0.2657396197319031,0.28240299224853516,-0.011299491859972477,-0.04061872884631157,-0.2839212119579315,0.26714861392974854,-0.14488306641578674,-0.03631860390305519,-0.22874608635902405,-0.22874608635902405,0.057831261307001114,0.2632693648338318,0.07585752755403519,0.006771871354430914,-0.2889368534088135,0.006073388271033764,0.05178442597389221,-0.003573089139536023,0.1725638061761856,-0.27310726046562195,0.14746354520320892,0.11402254551649094,0.24182985723018646,-0.08571546524763107,-0.10170440375804901,0.1565137505531311,0.24025149643421173,-0.24320478737354279,0.26714861392974854,-0.1066889837384224,0.28064224123954773,0.00710056908428669,-0.2748759984970093,-0.23894673585891724,-0.26407307386398315,-0.2748759984970093,-0.2748759984970093,-0.049214981496334076,-0.26597511768341064,-0.22874608635902405,-0.038469165563583374,0.05821707844734192,-0.23213627934455872,-0.03779273107647896,-0.19074241816997528,0.03137008845806122,0.2698122560977936,-0.22874608635902405,0.0740206316113472,-0.26848745346069336,0.26965993642807007,0.18552233278751373,-0.05554741993546486,0.13035421073436737,-0.23437905311584473,0.22248780727386475,0.2747798264026642,0.23758183419704437,-0.23476286232471466,0.23961515724658966,0.0943547934293747,-0.22386972606182098,0.1889415830373764,0.2207290679216385,0.2400420606136322,0.018082883208990097,0.025396408513188362,-0.2578278183937073,0.21755832433700562,0.18481482565402985,0.056707873940467834,-0.2555239498615265,-0.2748759984970093,0.03748106211423874,-0.19151605665683746,0.274836927652359,-0.22796142101287842,-0.03148433938622475,-0.2748759984970093,0.0013974623288959265,0.11638756096363068,-0.08570543676614761,-0.050211090594530106,-0.2569639980792999,-0.25584161281585693,0.24355551600456238,0.2460598349571228,-0.22303396463394165,0.1432395726442337,0.17953060567378998,-0.2511165738105774,-0.17199161648750305,0.04290156811475754,0.27637219429016113,0.274836927652359,0.07715427875518799,-0.21021775901317596,-0.189146026968956,0.22870700061321259,0.25299155712127686,0.09015186876058578,-0.22937943041324615,-0.2102448046207428,-0.1667424738407135,-0.20875665545463562,-0.02175368368625641,-0.17297670245170593,0.274836927652359,-0.2009190171957016,-0.20899516344070435,0.2630138099193573,-0.10579608380794525,-0.16029077768325806,0.14818206429481506,-0.22978027164936066,0.11111236363649368,-1.9545792383723892e-05,-0.22874608635902405,-0.19096289575099945,-0.06351533532142639,-0.04419131577014923,0.1903766244649887,0.2740362286567688,-0.2748759984970093,-0.07316382229328156,0.1994829624891281,-0.2739039361476898,0.14268839359283447,0.20624832808971405,-0.10859302431344986,0.2703503966331482,-0.2792071998119354,-0.09938347339630127,-0.2748759984970093,0.28514280915260315,0.0032093084882944822,-0.25438353419303894,-0.24609792232513428,0.028309592977166176,0.233076274394989,-0.15437822043895721,0.274836927652359,-0.20300345122814178,0.2460598349571228,0.26040929555892944,-0.2312474101781845,0.10507054626941681,0.2723957598209381,-0.2671877145767212,-0.052364662289619446,0.026282288134098053,-0.12554201483726501,0.06886468082666397,0.2702261209487915,0.13248874247074127,-0.23521080613136292,0.13549913465976715,-0.2426425963640213,0.2460598349571228,-0.1292869597673416,-0.06197807565331459,0.22870700061321259,0.27377966046333313,-0.05068910866975784,-0.21950146555900574,-0.13892842829227448,0.2651824355125427,-0.21929703652858734,0.07774753868579865,-0.2702201306819916,-0.09430068731307983,0.18028821051120758,0.28988587856292725,0.24989697337150574,0.07274191826581955,0.26714861392974854,-0.02730146050453186,-0.09996972233057022,-0.26335257291793823,0.08885512501001358,-0.23124338686466217,-0.054715655744075775,0.10495830327272415,0.22870700061321259,-0.22959588468074799,-0.15322677791118622,-0.26222214102745056,0.231460839509964,0.05061594396829605,0.23301613330841064,-0.18330763280391693,0.08223506063222885,-0.2696208655834198,-0.03761635720729828,-0.24609792232513428,0.0014605963369831443,-0.04803146421909332,-0.27282264828681946,0.0836590901017189,-0.2813377380371094,0.11903218179941177,0.16498172283172607,-0.1632320135831833,-0.18737025558948517,-0.13065186142921448,-0.24273578822612762,0.2503158748149872,-0.14456839859485626,0.19652067124843597,-0.24609792232513428,-0.05241677537560463,0.22935837507247925,-0.07104231417179108,-0.06520693004131317,-0.11592960357666016,0.2553405463695526,-0.22019194066524506,-0.2425123155117035,0.1621467024087906,-0.047093477100133896,0.08867473155260086,-0.21621748805046082,0.13541696965694427,-0.015567551366984844,-0.2748759984970093,0.10385295748710632,-0.25803425908088684,-0.22874608635902405,-0.22114194929599762,-0.26125410199165344,-0.037583284080028534,0.2719968855381012,-0.2817426025867462,-0.2204504758119583,0.21617038547992706,0.05965513363480568,-0.23501941561698914,-0.2748759984970093,0.2430073618888855,-0.2671877145767212,0.022201625630259514,0.2170492559671402,0.056424275040626526,0.25495171546936035,0.036625247448682785,-0.2640199661254883,-0.1584218144416809,-0.0896698608994484,0.274836927652359,0.23832741379737854,-0.22299186885356903,-0.19559872150421143,-0.1772327423095703,-0.271281361579895,-0.23949390649795532,-0.10576401650905609,-0.03072773478925228,-0.2748759984970093,-0.06415770202875137,0.2522610127925873,0.19945590198040009,0.2460598349571228,0.12011147290468216,0.26714861392974854,0.015463321469724178,0.004319665487855673,0.1451917141675949,0.28426793217658997,0.20895005762577057,-0.06906812638044357,0.03213471174240112,0.22870700061321259,0.0465322770178318,-0.19925348460674286,-0.21099340915679932,-0.247269406914711,0.17240747809410095,0.274836927652359,-0.2254410684108734,0.221070796251297,-0.10721810907125473,-0.11245622485876083,-0.06336401402950287,-0.2748759984970093,0.27359527349472046,-0.21416012942790985,-0.014068368822336197,-0.11859625577926636,-0.23536613583564758,-0.23526392877101898,0.14751464128494263,0.2549126148223877,-0.23143678903579712,-0.11812727153301239,0.12434445321559906,0.061524100601673126,-0.22874608635902405,-0.23653362691402435,-0.0976978987455368,-0.2639438211917877,0.1137920618057251,-0.2598952054977417,-0.2748759984970093,-0.0556827075779438,0.23699258267879486,0.001720147323794663,0.2460598349571228,0.1672024428844452,0.25286829471588135,-0.0510127954185009,0.11191607266664505,-0.03792601451277733,0.2359483689069748,0.19155412912368774,0.12375420331954956,-0.05291082710027695,0.22978328168392181,-0.2195405513048172,0.22527270019054413,0.23526892066001892,-0.20832374691963196,0.2746044397354126,-0.2476041316986084,0.24133780598640442,-0.11305349320173264,0.2821684777736664,-0.08131111413240433,0.2632022202014923,-0.18910594284534454,0.26714861392974854,0.22870700061321259,0.13025599718093872,-0.27498823404312134,0.2573087215423584,-0.20016241073608398,0.01552946213632822,-0.13718271255493164,-0.2748759984970093,0.06300925463438034,-0.22106076776981354,0.055338967591524124,-0.22874608635902405,0.022176573053002357,-0.01202202495187521,0.146700918674469,-0.2671877145767212,-0.02778749167919159,0.0799001082777977,-0.23968230187892914,-0.2671877145767212,-0.0742320865392685,-0.25325915217399597,-0.2796090841293335,-0.24846094846725464,0.28639546036720276,0.271689236164093,-0.09995468705892563,0.2223394811153412,-0.2671877145767212,0.02152518928050995,-0.02999217063188553,-0.2831736207008362,-0.2524193525314331,-0.24391929805278778,-0.08292854577302933,-0.23979955911636353,0.2666766047477722,0.06811509281396866,0.274836927652359,0.06464071571826935,0.25916263461112976,-0.2748759984970093,0.06360652297735214,-0.021322768181562424,0.26714861392974854,-0.2748759984970093,0.050273217260837555,0.22870700061321259,-0.06763006746768951,0.2514171898365021,0.07886090129613876,-0.2724127769470215,-0.24315668642520905,0.22870700061321259,-0.2490241378545761,0.2335863560438156,0.20780663192272186,0.09531883895397186,0.012270543724298477,-0.14746254682540894,0.023260874673724174,0.26714861392974854,-0.10363450646400452,0.09663262963294983,-0.18519966304302216,-0.1964014172554016,-0.2748759984970093,0.2515975832939148,-0.2654299736022949,-0.06873441487550735,-0.24751393496990204,-0.22874608635902405,0.25913459062576294,-0.21394968032836914,0.19814713299274445,-0.2155109941959381,0.04128413274884224,-0.03472020849585533,-0.24245017766952515,0.06824637204408646,-0.20238913595676422,0.26653629541397095,-0.11766228079795837,0.17826992273330688,0.20874463021755219,0.11134686321020126,0.100321464240551,0.2600395381450653,0.10312341153621674,0.24181783199310303,-0.056330081075429916,-1.9545792383723892e-05,0.08407697081565857,0.06833255290985107,0.2189352661371231,0.05875522270798683,-0.20604589581489563,0.03373410552740097,-0.2836456298828125,0.274836927652359,-0.23094776272773743,-0.2648557424545288,0.04622061178088188,-0.2679022252559662,-0.2748759984970093,-0.22874608635902405,0.23187772929668427,0.19212134182453156,0.1990841180086136,-0.16495968401432037,-0.013741674832999706,-0.16067257523536682,0.26571956276893616,-0.17605023086071014,0.22127622365951538,-0.2636892795562744,0.209617480635643,0.07296839356422424,-0.17600414156913757,0.07509791851043701,-0.0880664512515068,0.22870700061321259,0.00848149973899126,0.2670654356479645,-0.2671877145767212,0.0447765477001667,0.22020795941352844,-0.13411922752857208,-0.25224095582962036,-0.08403690159320831,0.22870700061321259,-0.2239338755607605,-0.28568094968795776,-0.25724056363105774,0.2618383467197418,0.2646142244338989,0.11971663683652878,0.229406476020813,0.025142868980765343,-0.013605386018753052,-0.22874608635902405,-0.199228435754776,0.08523844182491302,0.08369617164134979,-0.2487335205078125,-0.028001947328448296,-0.06676523387432098,0.0677623450756073,0.22001755237579346,0.26714861392974854,0.22870700061321259,-0.233047217130661,0.24588747322559357,-0.03271094337105751,0.017624910920858383,-0.06434109061956406,-0.22874608635902405,0.15565091371536255,-0.24240408837795258,-0.10145387053489685,0.08933313190937042,-0.057191912084817886,-0.0065163373947143555,-0.04024694114923477,-0.015606634318828583,0.274836927652359,-0.12860049307346344,-0.00566653348505497,0.22870700061321259,0.1542639583349228,0.10817613452672958,0.015401190146803856,-0.2639097273349762,-0.0016560197109356523,0.2779795825481415,-0.08082708716392517,-0.05137255787849426,-0.2671877145767212,-0.25641685724258423,0.2732255160808563,0.12274305522441864,0.22870700061321259,-0.09758666157722473,0.19617795944213867,-0.24859322607517242,0.244966521859169,-0.2577677071094513,0.2379365861415863,0.2324068546295166,0.10195893794298172,-0.20098115503787994,-0.22874608635902405,0.2509361803531647,0.008349219337105751,-0.02624722197651863,-0.09677894413471222,-0.2539806663990021,-0.016823217272758484,-0.02895597368478775,-0.22874608635902405,-0.24609792232513428,0.26714861392974854,0.2908168435096741,-0.17441175878047943,0.28351736068725586,0.016378263011574745,0.2475820779800415,0.18675394356250763,0.24060726165771484,0.08927500993013382,-0.2201298028230667,0.14358730614185333,0.274836927652359,0.19212134182453156,-0.11925866454839706,-0.035359565168619156,0.20794492959976196,-0.07937199622392654,-0.06842074543237686,-0.1464604139328003,-0.03915061429142952,0.04396382346749306,-0.2748759984970093,0.16744594275951385,-0.2229728400707245,0.22870700061321259,0.1378060281276703,-0.2054576426744461,-0.05738331750035286,-0.2671877145767212,0.11991104483604431,-0.2395830899477005,0.039784952998161316,-0.24609792232513428,0.2415131777524948,0.07166863977909088,-0.03895620256662369,0.20220473408699036,0.2747637629508972,0.016266025602817535,0.16490155458450317,-0.051708269864320755,-0.08093030750751495,0.24530723690986633,-0.027838600799441338,-0.2248508185148239,0.03885898366570473,0.24078765511512756,0.22870700061321259,-0.24845995008945465,-0.09052466601133347,0.26714861392974854,0.24293319880962372,0.2799517810344696,-0.2748759984970093,0.11354152113199234,0.21174098551273346,-0.23171940445899963,0.16021059453487396,-0.25915366411209106,-0.24609792232513428,0.22870700061321259,0.005247635766863823,-0.1751803755760193,0.26869288086891174,-0.2250622659921646,-0.1633703112602234,-0.2415492683649063,-0.04054056480526924,0.2007225900888443,-0.021480102092027664,-0.2748759984970093,-0.2671877145767212,-0.07743187993764877,-0.023307982832193375,-0.27434489130973816,0.23038354516029358,-0.2031257003545761,0.037180423736572266,-0.25520625710487366,-0.2438531517982483,-0.0006859604618512094,-0.09225234389305115,-0.18755966424942017,0.2716100811958313,0.25822165608406067,0.274836927652359,-0.2748759984970093,-0.06171451136469841,0.2582196593284607,-0.2612580955028534,0.0210040844976902,-0.08906857669353485,0.01552946213632822,0.2460598349571228,0.22870700061321259,0.02521001175045967,0.253740131855011,0.04352288693189621,0.2823147773742676,0.24840080738067627,-0.27896371483802795,0.2140979766845703,0.20978783071041107,0.0656568706035614,0.02231486700475216,0.26749536395072937,-0.009344340302050114,0.23398521542549133,0.21477040648460388,-0.2748759984970093,-0.2518611550331116,0.12178301811218262,0.26714861392974854,0.2460598349571228,0.274836927652359,0.2822917699813843,0.14878834784030914,-0.1066007986664772,0.20390334725379944,0.274836927652359,0.1974065601825714,0.01552946213632822,-0.06885567307472229,0.2042851448059082,-0.04339361935853958,0.05135250836610794,-0.08062364906072617,-0.22874608635902405,0.2210788130760193,-0.2613683342933655,0.22870700061321259,-0.0760459378361702,0.11618813872337341,0.2679022252559662,-0.26610037684440613,-0.26999062299728394,0.14254911243915558,0.21637383103370667,0.23016609251499176,-0.20010828971862793,0.2535116672515869,0.2460598349571228,-0.06464072316884995,0.12043917179107666,0.16785483062267303,0.05310121923685074,-0.16796807944774628,-0.0468599796295166,0.22870700061321259,-0.24609792232513428,0.2374916523694992,0.15130369365215302,-0.24609792232513428,0.1943701058626175,-0.22874608635902405,-0.24710404872894287,-0.2808897793292999,-0.07929583638906479,-0.1615344136953354,0.1896771341562271,0.12191930413246155,0.2493307888507843,0.274836927652359,0.00017787329852581024,-0.05989064276218414,-0.0892690122127533,-0.05205701291561127,0.075550876557827,0.054398976266384125,-0.10768108814954758,-0.21219795942306519,0.06437515467405319,-0.056172747164964676,0.1204070970416069,-0.16659115254878998,0.274836927652359,-0.25344952940940857,0.020236453041434288,0.28577715158462524,0.12407989799976349,0.22870700061321259,0.042907580733299255,0.23523084819316864,0.2563556730747223,0.25635868310928345,-0.25224798917770386,-0.22874608635902405,0.26502910256385803,-0.2113220989704132,0.2460598349571228,0.07641270756721497,0.2460598349571228,-0.09098264575004578,-0.03980099409818649,0.26206180453300476,0.26714861392974854,-0.24609792232513428,0.25007838010787964,0.10078243911266327,-0.2746916115283966,0.16005927324295044,-0.2421034425497055,0.05787034332752228,-0.2642444372177124,0.24905018508434296,0.0887158215045929,0.26857665181159973,0.014661619439721107,0.05592120438814163,0.16451071202754974,-0.24735058844089508,0.052941881120204926,-0.2517368793487549,-0.24966049194335938,-0.25703415274620056,0.2489159107208252,-0.24609792232513428,-0.16421310603618622,-0.07283110916614532,-0.21960768103599548,0.26128917932510376,0.21791109442710876,-0.22194766998291016,0.18985353410243988,0.20433124899864197,-0.029853878542780876,0.274836927652359,0.2322675585746765,0.014911149628460407,-0.16256259381771088,-1.9545792383723892e-05,0.2362319827079773,-0.2565370798110962,-0.239183247089386,0.274836927652359,0.274836927652359,0.26442381739616394,-0.054360903799533844,-0.2748759984970093,0.2509031295776367,0.22658750414848328,0.02511380799114704,0.12368806451559067,-0.25141221284866333,0.23598845303058624,0.019229315221309662,-0.010531861335039139,-0.2615106403827667,0.25651001930236816,-0.14346204698085785,-0.2671877145767212,0.22626681625843048,0.26714861392974854,-0.009662014432251453,0.07861939072608948,-0.06729135662317276,0.1379573494195938,-0.030072342604398727,-0.2291058450937271,0.06788961589336395,0.09042845666408539,-0.2584751844406128,0.25560107827186584,-0.14214524626731873,0.26605427265167236,-0.22874608635902405,0.24297527968883514,-0.2817075252532959,0.22870700061321259,-0.004455963149666786,-0.22247076034545898,-0.03807833790779114,0.0726146399974823,-0.015567551366984844,-0.2671877145767212,-0.01916218176484108,0.26714861392974854,-0.24609792232513428,-0.19253121316432953,-0.16052627563476562,0.08496686816215515,-0.23008893430233002,-0.18306010961532593,-0.22604836523532867,-0.024743029847741127,0.012191376648843288,0.25051429867744446,-0.2748759984970093,0.019730379804968834,-0.019392671063542366,-0.10589329898357391,-0.07343639433383942,0.0412941537797451,0.10331480950117111,-0.18710669875144958,0.2462722808122635,0.274836927652359,0.03386037424206734,0.2596396505832672,0.23737139999866486,-0.2671877145767212,0.27743643522262573,0.0841982364654541,0.2460598349571228,-0.03094719909131527,-0.10222651064395905,0.23733831942081451,-0.19875642657279968,-0.24662603437900543,-0.27264127135276794,0.2300167828798294,0.2460598349571228,0.2880770266056061,-0.24609792232513428,-0.14723004400730133,0.1720537394285202,-0.2726232409477234,-0.0428895503282547,0.15292011201381683,-0.09950272738933563,-0.2748759984970093,-0.24609792232513428,-0.11015834659337997,-0.2496003657579422,-0.2622893154621124,0.0699259340763092,-0.07664521038532257,-0.07276497036218643,0.07415090501308441,0.26397886872291565,-0.21164779365062714,-0.1075047180056572,0.2030545473098755,0.2460598349571228,-0.26413121819496155,-0.14390398561954498,-0.08210880309343338,-0.1412944495677948,0.2460598349571228,-0.22141051292419434,0.042007673531770706,0.08788806200027466,-0.2321954220533371,0.24436023831367493,0.03123380057513714,0.2655201554298401,0.24778048694133759,-0.2541840970516205,0.2040005475282669,-0.21989630162715912,-0.2536660134792328,0.1969205141067505,-0.06065426021814346,0.16696593165397644,0.2146160900592804,-0.2715319097042084,0.023183710873126984,-0.13716568052768707,0.2514803409576416,0.08757741004228592,0.26714861392974854,-0.062262676656246185,0.2460598349571228,-0.22874608635902405,0.23065613210201263,-0.02995409071445465,0.12624047696590424,0.24305245280265808,0.0887378677725792,-0.19866523146629333,0.09183143824338913,0.06544943153858185,-0.23436400294303894,-0.2671877145767212,-0.19881856441497803,0.24913637340068817,-0.22874608635902405,-0.06282787770032883,-0.2748759984970093,0.08273211866617203,0.2514171898365021,-0.1236049011349678,0.050671059638261795,0.23819012939929962,0.20227989554405212,0.12834495306015015,0.22986845672130585,-0.05642227828502655,0.24219362437725067,0.2460598349571228,0.0936262458562851,0.09771092236042023,-0.17412914335727692,-0.04569350183010101,0.22870700061321259,-0.21373723447322845,0.274809867143631,0.09432473033666611,0.26972708106040955,-0.2600976526737213,0.2552964389324188,0.04019983112812042,-0.10907505452632904,-0.05912401154637337,0.27777916193008423,0.274836927652359,0.008253015577793121,0.22870700061321259,-0.1167282983660698,0.2305779755115509,0.08731184154748917,0.2540868818759918,0.16907942295074463,-0.2501886188983917,-0.25530847907066345,-0.02158232033252716,0.274836927652359,0.10241289436817169,-0.04156874865293503,0.03708622232079506,-0.11465690284967422,0.07825662195682526,-0.052815619856119156,0.274836927652359,0.15375488996505737,-0.24609792232513428,-0.17875495553016663,0.23942676186561584,0.10099789500236511,-0.2722434401512146,0.07846105098724365,-0.17374834418296814,0.002674172632396221,0.24015630781650543,0.20243923366069794,0.06388010084629059,0.22870700061321259,-0.22874608635902405,-0.28351137042045593,-0.23046374320983887,-0.23308229446411133,-0.15268763899803162,-0.08699718117713928,-0.23337692022323608,0.02608587220311165,-0.2905873954296112,-0.07767640054225922,-0.2648847997188568,-0.24351945519447327,-0.036002933979034424,-0.2069077342748642,0.06128759682178497,0.026183076202869415,-0.14320750534534454,0.22351397573947906,0.26674774289131165,-0.16354267299175262,-0.08229520916938782,0.22346988320350647,0.025876427069306374,-0.07539454847574234,0.0593985877931118,0.26769477128982544,0.2662998139858246,0.21732082962989807,0.01858093962073326,0.23921430110931396,-0.2748759984970093,-0.02406959980726242,0.2658037543296814,-0.23537015914916992,0.14949184656143188,-0.06231779232621193,-0.22672781348228455,0.2542863190174103,-0.2748759984970093,-0.11456470936536789,-0.11748690158128738,0.23343104124069214,0.2114102840423584,-0.18016496300697327,-0.24609792232513428,-0.2558285892009735,0.016164811328053474,0.22870700061321259,-0.22944557666778564,-0.22874608635902405,-0.24473802745342255,-0.09189558029174805,-0.08463316410779953,0.0633108988404274,-0.2422858327627182,-0.16176390647888184,0.11530927568674088,-0.0914326012134552,0.02908223122358322,-0.26670968532562256,0.24174167215824127,0.15607081353664398,0.23791053891181946,-0.043557967990636826,0.010760338045656681,0.2670043110847473,0.11637253314256668,0.018104929476976395,-0.2748759984970093,-0.2671877145767212,-0.2671877145767212,-0.04345575347542763,-0.018098924309015274,-0.2797844409942627,0.23457244038581848,-0.27760082483291626,-0.21778181195259094,-0.2748759984970093,-0.28627723455429077,0.2179591804742813,0.24832163751125336,-0.07162655889987946,-0.28787660598754883,0.13852055370807648,-0.23895175755023956,0.057438429445028305,-0.24879464507102966,0.017814312130212784,0.2686888873577118,0.08796422928571701,-0.23269246518611908,0.19649261236190796,0.028225412592291832,-0.1352335810661316,-0.28194302320480347,-0.012442918494343758,0.26883718371391296,-0.2094060331583023,-0.08185426890850067,0.24951216578483582,0.10614783316850662,0.26115289330482483,0.21101443469524384,-0.22874608635902405,-0.06161830946803093,-0.031194724142551422,-0.115693099796772,0.049576740711927414,-0.21802634000778198,-0.06623110175132751,0.2460598349571228,-0.0047074975445866585],\"y\":[-0.15818028151988983,0.862852156162262,-0.8807020783424377,0.017567794770002365,-0.8878222107887268,-0.4425469636917114,0.4429227411746979,-0.5279371738433838,-0.07418297976255417,-0.8858780860900879,-0.44072309136390686,-0.0014395576436072588,0.8663305640220642,0.45273756980895996,-0.5175511837005615,-0.8859522342681885,-0.23309531807899475,-0.2186516523361206,0.2300678938627243,0.6669402718544006,-0.2055237889289856,-0.07267678529024124,-0.22884029150009155,-0.27038148045539856,-0.065889373421669,0.8822423219680786,-0.6724138855934143,-0.6682670712471008,0.00772489607334137,-0.22248178720474243,0.21631669998168945,0.4518887996673584,0.6477705836296082,0.2364073544740677,0.6619336605072021,-0.2250562608242035,-0.12567928433418274,-0.698139488697052,-0.23072628676891327,-0.34626156091690063,-0.07953032851219177,-0.06696765869855881,-0.7137135863304138,-0.11124465614557266,-0.23309531807899475,-0.1598518341779709,-0.0923284962773323,0.8083504438400269,-0.07583648711442947,-0.1175580620765686,-0.20490747690200806,-0.23298609256744385,-0.11365476995706558,-0.15874749422073364,-0.15934476256370544,-0.09748142957687378,-0.10843870043754578,-0.16010136902332306,0.09993865340948105,0.3273513913154602,0.5793603658676147,0.8012614250183105,0.35423848032951355,-0.4065525531768799,-0.1490529179573059,0.13315817713737488,-0.16159051656723022,-0.08704027533531189,-0.0699770525097847,-0.23006990551948547,0.5459133982658386,-0.23309531807899475,-0.12266389280557632,-0.22814881801605225,-0.5495681166648865,-0.7876555323600769,0.1257183849811554,-0.5508488416671753,0.7636045217514038,-0.06964635103940964,-0.7652309536933899,0.7698908448219299,-0.15773333609104156,0.3319201171398163,-0.3222305178642273,0.4540604054927826,-0.1602075845003128,-0.2240150421857834,-0.10112015902996063,0.1003575325012207,0.5480669140815735,0.7742370367050171,-0.173364520072937,-0.1474284678697586,-0.32644549012184143,0.5601506233215332,0.35394588112831116,-0.23309531807899475,-0.06594449281692505,-0.09316026419401169,-0.15743571519851685,-0.07418698817491531,-0.12981507182121277,-0.22161094844341278,-0.07862841337919235,-0.16391345858573914,-0.15833061933517456,-0.23309531807899475,-0.1622258871793747,-0.6194143891334534,-0.1291997730731964,-0.15985986590385437,-0.07304956763982773,-0.08041120320558548,0.4529159367084503,0.00017386703984811902,-0.7967308163642883,-0.0787997841835022,-0.07052120566368103,0.6770597696304321,-0.20113947987556458,-0.44488590955734253,-0.16093112528324127,-0.6134858131408691,-0.07912546396255493,-0.07032178342342377,-0.13932126760482788,-0.15737858414649963,0.34838607907295227,-0.15621310472488403,-0.06702277809381485,-0.8073663711547852,0.8845261931419373,-0.23309531807899475,-0.018317386507987976,-0.08010154217481613,-0.2898007333278656,-0.15821535885334015,-0.23309531807899475,-0.07365986704826355,-0.16209961473941803,-0.11176376044750214,0.2351967692375183,-0.15980374813079834,-0.15703485906124115,-0.1580369770526886,-0.07555990666151047,-0.16356171667575836,-0.1625174880027771,-0.0777796134352684,-0.2644649147987366,-0.0679006427526474,-0.06879553943872452,-0.1555446982383728,-0.14559657871723175,-0.07972173392772675,-0.07948824018239975,-0.0781874805688858,-0.1664588749408722,-0.15621310472488403,-0.16258364915847778,-0.27922824025154114,0.7477027177810669,-0.09014085680246353,-0.35186946392059326,0.14325359463691711,-0.1581091284751892,-0.23309531807899475,0.8729445934295654,0.5244778394699097,0.847166895866394,-0.22151975333690643,-0.8343557119369507,-0.15621310472488403,-0.16501981019973755,0.2762589454650879,-0.3786032199859619,-0.44735315442085266,0.15770027041435242,-0.09781514108181,-0.23309531807899475,-0.23309531807899475,-0.717003583908081,-0.3860490024089813,0.3027281165122986,-0.14965319633483887,-0.15840476751327515,-0.16175487637519836,-0.15972456336021423,-0.15934476256370544,-0.16921672224998474,-0.15621310472488403,-0.1623561531305313,0.7133508324623108,-0.23309531807899475,-0.159551203250885,0.38424918055534363,-0.08030998706817627,0.19005194306373596,0.5015822649002075,-0.09242971241474152,-0.31657353043556213,-0.6060961484909058,-0.15970852971076965,-0.3954770267009735,-0.15840476751327515,0.07503177225589752,-0.1677546203136444,-0.6056692600250244,-0.5576342344284058,0.0649503767490387,0.1902383416891098,-0.021843871101737022,-0.058273203670978546,0.8505871891975403,-0.5120053887367249,-0.03200043365359306,0.7961836457252502,-0.23309531807899475,0.6088670492172241,-0.08466523140668869,-0.5130716562271118,-0.7268123626708984,-0.0765409842133522,-0.06694361567497253,-0.2330331951379776,-0.081698939204216,0.841301441192627,-0.7522283792495728,-0.4931022822856903,0.4819926619529724,0.26270920038223267,-0.08562326431274414,-0.15621310472488403,-0.18227243423461914,-0.2261555939912796,-0.16813743114471436,0.7762282490730286,-0.16372105479240417,-0.8567101359367371,-0.33307555317878723,-0.09153782576322556,0.05724802613258362,-0.8128079175949097,-0.06429498642683029,-0.10567884147167206,-0.2643316388130188,-0.22941049933433533,-0.1572873890399933,0.5937469005584717,0.03860144689679146,0.28805097937583923,0.6963908076286316,-0.08490974456071854,-0.23309531807899475,-0.1606164574623108,-0.1638352870941162,0.3909373879432678,0.599084198474884,-0.1630556434392929,-0.08839114010334015,-0.044459883123636246,-0.0676240548491478,-0.15621310472488403,-0.11340925097465515,-0.2055598646402359,0.5143172740936279,-0.19278575479984283,-0.23032042384147644,-0.23110811412334442,-0.26268312335014343,-0.09802860021591187,-0.07446859031915665,-0.4590439796447754,-0.1024780422449112,-0.11046800762414932,0.7164433598518372,0.09258905053138733,-0.15621310472488403,0.4170658588409424,0.8037316799163818,-0.4674137532711029,-0.06726228445768356,-0.1589990258216858,-0.14275053143501282,0.29792293906211853,-0.07456979900598526,-0.16847915947437286,0.3790651857852936,0.19104906916618347,-0.06544943898916245,-0.16631655395030975,0.6443713903427124,-0.5592567324638367,-0.4755249619483948,-0.08384649455547333,-0.20671632885932922,-0.23309531807899475,-0.16116963326931,-0.27763986587524414,-0.08985424786806107,-0.29278305172920227,-0.15881364047527313,-0.16848015785217285,-0.6296451091766357,-0.15792375802993774,-0.6110756993293762,-0.16789692640304565,0.6206931471824646,-0.2812996506690979,0.07618321478366852,-0.16093212366104126,-0.861046314239502,0.8326420783996582,-0.15833361446857452,0.297553151845932,-0.7704119086265564,0.1570448875427246,-0.6503681540489197,-0.5873884558677673,-0.23024627566337585,-0.07891201972961426,-0.7407609820365906,-0.06673416495323181,-0.3855699896812439,-0.1656711995601654,-0.1609281301498413,-0.1597265750169754,0.054828889667987823,-0.15621310472488403,-0.18148276209831238,-0.08087217807769775,0.6164701581001282,-0.06984677910804749,-0.22629688680171967,0.8432495594024658,-0.23309531807899475,-0.20349249243736267,-0.1487332433462143,-0.21680773794651031,-0.6297673583030701,-0.08707133680582047,-0.23309531807899475,-0.1586933732032776,-0.0798049122095108,0.8344859480857849,0.5068835616111755,0.3994755148887634,-0.08432450890541077,-0.23309531807899475,-0.15621310472488403,-0.076890729367733,-0.6890702843666077,-0.4979325234889984,-0.8228392004966736,-0.8219302892684937,-0.21275314688682556,-0.1453240066766739,-0.15621310472488403,-0.0707346647977829,-0.18678000569343567,-0.08882105350494385,-0.6629127860069275,-0.07428519427776337,-0.07582646608352661,-0.09365732222795486,0.3816035985946655,-0.17185533046722412,-0.3898240327835083,-0.1578676402568817,-0.16229502856731415,-0.31567060947418213,-0.23309531807899475,-0.1567111760377884,-0.07029272615909576,-0.23309531807899475,-0.21522940695285797,-0.06769821792840958,-0.019337551668286324,0.7334885597229004,-0.45817112922668457,-0.6812296509742737,-0.362554132938385,-0.10001180320978165,-0.6696911454200745,-0.23309531807899475,-0.08370218425989151,-0.16000016033649445,-0.11367481201887131,0.6562185287475586,-0.07626239210367203,-0.0705893486738205,0.16099926829338074,-0.18248789012432098,0.8450784087181091,0.0454179085791111,0.24650777876377106,-0.06469984352588654,0.580432653427124,-0.08980013430118561,-0.14000670611858368,-0.15915235877037048,-0.16119569540023804,-0.11591657251119614,-0.7865381836891174,-0.1025572195649147,-0.1128520667552948,-0.07260362803936005,0.49804070591926575,0.8512194752693176,0.3026449382305145,-0.15621310472488403,-0.15094392001628876,-0.06403043121099472,0.7166948914527893,-0.16273295879364014,-0.37996211647987366,-0.20135995745658875,-0.08538475632667542,0.5288461446762085,-0.3519456088542938,-0.07066350430250168,-0.8477561473846436,-0.5647543668746948,0.7975585460662842,-0.5702500343322754,-0.23309531807899475,-0.17223411798477173,-0.08007949590682983,-0.16080786287784576,-0.06727631390094757,-0.19915226101875305,-0.15759003162384033,-0.08106658607721329,-0.5629124641418457,-0.21572846174240112,0.5090541243553162,-0.21276114881038666,-0.8853389024734497,-0.23309531807899475,-0.16982901096343994,-0.30322620272636414,-0.23309531807899475,-0.08007247745990753,0.6355897784233093,-0.15621310472488403,0.3984433114528656,-0.16066455841064453,-0.2762860059738159,-0.20031172037124634,-0.8209663033485413,-0.5065387487411499,-0.23309531807899475,0.203395277261734,0.7457516193389893,-0.043811507523059845,-0.427933931350708,0.43708935379981995,-0.5036075711250305,-0.0505247563123703,-0.1565718799829483,-0.1598949283361435,0.8855363130569458,-0.19207023084163666,-0.15786662697792053,-0.07694684714078903,0.7953779101371765,-0.06995700299739838,0.8304674029350281,-0.39516738057136536,-0.7449539303779602,-0.06766814738512039,-0.3169403076171875,0.5267215967178345,-0.7558830976486206,-0.8863189816474915,0.01016207318753004,0.7533737421035767,0.7167660593986511,-0.06911822408437729,-0.34190431237220764,0.4833765923976898,-0.1803453415632248,-0.16122575104236603,-0.18914802372455597,0.34478041529655457,-0.1635085940361023,-0.06904106587171555,-0.12257269769906998,-0.13956275582313538,0.007588607259094715,-0.7005235552787781,0.12265987694263458,0.34856441617012024,0.25344452261924744,-0.43971994519233704,-0.4415578544139862,-0.0036432372871786356,0.5680834054946899,0.5974818468093872,0.577228844165802,-0.1605052351951599,0.09677192568778992,-0.07161452621221542,-0.3993081748485565,-0.16364187002182007,0.8077201247215271,0.4060294032096863,0.12797215580940247,-0.001543777878396213,0.1900930404663086,-0.15829454362392426,-0.09732410311698914,-0.12286432087421417,-0.41358447074890137,0.1401289701461792,-0.10640738904476166,0.4686533808708191,-0.2682449519634247,-0.10924641042947769,-0.16432131826877594,0.6166184544563293,-0.16051927208900452,-0.04908469691872597,0.7997311353683472,-0.1890067309141159,0.845782995223999,-0.09882328659296036,-0.10173346102237701,-0.1619713306427002,-0.6608834266662598,-0.07942710816860199,-0.07003918290138245,-0.6048835515975952,-0.09823103249073029,-0.22062285244464874,-0.07214565575122833,-0.1458040028810501,0.24049903452396393,-0.1751282662153244,-0.1602506786584854,-0.16183705627918243,-0.2119935303926468,0.19858404994010925,-0.10371667891740799,-0.08925697952508926,0.6577247381210327,-0.06504758447408676,-0.12286331504583359,-0.19652970135211945,-0.17160479724407196,-0.32293200492858887,-0.351596862077713,-0.06583525985479355,0.3319842219352722,-0.15900202095508575,-0.31182044744491577,-0.35119199752807617,0.7891136407852173,-0.49393102526664734,0.1130194142460823,-0.0694669708609581,-0.119613416492939,-0.17920492589473724,0.8878793120384216,-0.7470443248748779,-0.08652517944574356,-0.5164167881011963,0.09202886372804642,-0.7095457315444946,-0.1893133819103241,0.4178745746612549,-0.8094387650489807,-0.16184405982494354,-0.15459366142749786,-0.15816625952720642,-0.09248983860015869,0.1878552883863449,-0.08837911486625671,-0.41921743750572205,-0.15796783566474915,-0.49039652943611145,0.28507766127586365,-0.0865442156791687,-0.858430802822113,0.6940077543258667,0.18123723566532135,-0.15871642529964447,0.8492934107780457,-0.7595208287239075,-0.6823049187660217,0.3157377541065216,0.14273951947689056,0.057208940386772156,-0.07088498026132584,-0.15082569420337677,-0.16364187002182007,-0.18426565825939178,0.22938543558120728,-0.17369422316551208,-0.23309531807899475,0.05258011817932129,0.34990429878234863,-0.3445218503475189,-0.07374705374240875,-0.19714400172233582,-0.8512715697288513,0.4139161705970764,-0.39118191599845886,-0.15008610486984253,0.8485388159751892,-0.07698893547058105,-0.8281444907188416,-0.7432703375816345,-0.727210283279419,-0.18465648591518402,-0.23309531807899475,-0.2573728859424591,-0.4018746316432953,0.5562312602996826,-0.11310159415006638,-0.18706761300563812,0.6034515500068665,-0.19380490481853485,-0.19253621995449066,0.5666343569755554,-0.2561121881008148,-0.5340260863304138,0.7632968425750732,-0.20351353287696838,-0.22090543806552887,0.26469239592552185,-0.5861978530883789,0.4911210834980011,-0.29746195673942566,0.27230754494667053,-0.14712682366371155,-0.07775656133890152,-0.07806020975112915,-0.34431442618370056,0.0010336936684325337,-0.8577232956886292,-0.10857899487018585,0.7563239932060242,0.3077227473258972,-0.15812015533447266,-0.08302976191043854,-0.18844853341579437,-0.19930359721183777,-0.06975558400154114,-0.18409529328346252,-0.10981061309576035,-0.2615978419780731,0.4702407419681549,-0.5205094814300537,-0.1066078171133995,-0.20220375061035156,-0.08713948726654053,-0.03088807314634323,-0.5270102620124817,-0.15621310472488403,0.0020037509966641665,0.393852561712265,0.6767882108688354,-0.5516505837440491,-0.15866431593894958,0.6849334836006165,-0.4332802891731262,-0.0641126036643982,-0.09547819197177887,-0.16066856682300568,0.21769261360168457,-0.16488952934741974,-0.1311028152704239,-0.07284814864397049,0.7924156785011292,-0.27331268787384033,0.3364056348800659,0.631013035774231,-0.15930669009685516,0.48224419355392456,-0.0909515768289566,-0.15794478356838226,0.08763452619314194,-0.26145756244659424,-0.47127294540405273,-0.873163104057312,-0.2466059923171997,-0.19987981021404266,-0.4901890754699707,-0.194026380777359,-0.4621756374835968,-0.139851376414299,-0.23066015541553497,-0.08000834286212921,-0.10490117967128754,-0.1571911871433258,0.5248656868934631,-0.1311238557100296,-0.7851903438568115,-0.2025374472141266,-0.07366788387298584,-0.6134828329086304,-0.06763407588005066,-0.1580289751291275,-0.435192346572876,-0.15621310472488403,-0.16322901844978333,-0.06371275335550308,-0.5430322885513306,-0.16031582653522491,-0.6069850325584412,-0.15885071456432343,-0.0737440437078476,-0.2957322895526886,-0.23309531807899475,-0.18238165974617004,0.8663305640220642,-0.15698575973510742,-0.6623806357383728,-0.19801786541938782,-0.1420370191335678,0.3396936058998108,-0.15348030626773834,0.8059894442558289,-0.1744207739830017,-0.17101754248142242,-0.23309531807899475,-0.17208781838417053,0.04678281024098396,-0.568953275680542,-0.2668319344520569,-0.1577383428812027,-0.16238521039485931,-0.15977166593074799,-0.06369972229003906,-0.15750786662101746,-0.15928763151168823,-0.1573515236377716,-0.15826848149299622,-0.15621310472488403,-0.2657656669616699,-0.6199876070022583,0.7323872447013855,-0.1125464215874672,-0.1592014580965042,-0.1592675894498825,0.04272820055484772,-0.19438613951206207,-0.18749050796031952,-0.14185863733291626,0.4849890470504761,-0.5971952080726624,0.5375956892967224,-0.8535253405570984,-0.0642508938908577,0.15495343506336212,-0.23309531807899475,-0.03950035572052002,-0.1745099574327469,-0.7886075377464294,-0.15859316289424896,0.09043146669864655,-0.1619112193584442,0.8608719706535339,-0.15621310472488403,-0.19297514855861664,-0.1904638111591339,-0.3164653182029724,0.6938043236732483,-0.03177294880151749,-0.358884334564209,0.37086477875709534,-0.07576433569192886,-0.6544778347015381,-0.7068530321121216,-0.16300953924655914,-0.19760097563266754,-0.20226988196372986,-0.23309531807899475,-0.067507803440094,-0.23309531807899475,0.1311318725347519,-0.6894370317459106,-0.16337832808494568,0.6553968191146851,-0.2008959800004959,-0.5436335802078247,0.710206151008606,-0.8169257044792175,-0.7291744351387024,-0.18466050922870636,0.40541812777519226,0.6257187724113464,-0.08574753254652023,-0.15621310472488403,0.5092204809188843,0.7589446306228638,-0.19437512755393982,0.2686798572540283,-0.08045127987861633,-0.23309531807899475,-0.15871743857860565,0.30022183060646057,0.4537016451358795,-0.03075278364121914,-0.1469123661518097,-0.15616200864315033,0.36462852358818054,-0.183448925614357,0.270668089389801,-0.15621310472488403,-0.07607699930667877,-0.16117866337299347,-0.15982478857040405,-0.15621310472488403,-0.23309531807899475,-0.06777537614107132,-0.23309531807899475,-0.20752404630184174,0.03194931894540787,0.8313693404197693,0.3735053837299347,-0.07685565203428268,-0.13538891077041626,-0.856378436088562,-0.8600462675094604,-0.15621310472488403,-0.112283855676651,-0.23309531807899475,-0.23303619027137756,0.04610336571931839,-0.08693605661392212,-0.1640838235616684,-0.16142918169498444,-0.05677201971411705,0.6293585300445557,0.5464244484901428,-0.23309531807899475,0.6824792623519897,-0.15621310472488403,-0.07157544046640396,-0.0943908765912056,-0.12442964315414429,0.22224728763103485,-0.1970728486776352,0.1647322028875351,-0.08678774535655975,0.07255451381206512,-0.23309531807899475,-0.15892888605594635,-0.1257254034280777,-0.25521525740623474,-0.15621310472488403,-0.0684548169374466,-0.06423386186361313,-0.1609060913324356,-0.20908837020397186,-0.7723209857940674,-0.47513914108276367,-0.1581742763519287,-0.3137866258621216,-0.42188310623168945,-0.45862507820129395,-0.23044569790363312,-0.22924816608428955,-0.21903447806835175,0.026400543749332428,-0.08790110051631927,0.7263924479484558,-0.19648660719394684,-0.1633402407169342,-0.12481345981359482,-0.8573133945465088,-0.23309531807899475,-0.08861061185598373,-0.2242836058139801,-0.06617899239063263,-0.20106132328510284,0.3238028585910797,-0.8879965543746948,0.7924988269805908,-0.5285845994949341,0.23447924852371216,-0.14098580181598663,-0.21009249985218048,-0.18548424541950226,-0.09331057965755463,-0.07540056109428406,-0.07821353524923325,-0.16553589701652527,-0.1591223031282425,-0.6483598351478577,-0.23309531807899475,-0.5278089642524719,-0.15709398686885834,-0.8877741098403931,-0.07825662940740585,0.8497353196144104,-0.07206347584724426,-0.1295495182275772,-0.23309531807899475,-0.792267382144928,-0.16059942543506622,0.5604171752929688,-0.6559980511665344,-0.07623133063316345,-0.06680531799793243,-0.23309531807899475,-0.1574838012456894,0.8522516489028931,-0.07583347707986832,-0.13097955286502838,0.4627588391304016,-0.23309531807899475,-0.23050682246685028,0.16098524630069733,0.33491647243499756,-0.16210061311721802,-0.16584154963493347,-0.005505189765244722,-0.1608709990978241,-0.656960129737854,-0.5795086622238159,-0.04652126133441925,-0.6805661916732788,-0.48935332894325256,-0.07410881668329239,0.5507205724716187,0.15484119951725006,-0.15888679027557373,-0.1765492707490921,0.41409754753112793,-0.07098118215799332,0.3171968460083008,-0.11467794328927994,-0.1622479259967804,-0.12720555067062378,-0.19450139999389648,-0.48373639583587646,-0.31211405992507935,-0.15964139997959137,-0.08466923981904984,0.36372262239456177,-0.6331345438957214,-0.07108940929174423,-0.3849516808986664,-0.19374176859855652,-0.2971843481063843,-0.07910742610692978,-0.15705791115760803,-0.10205814987421036,-0.07923770695924759,-0.16118668019771576,-0.16205450892448425,-0.16083091497421265,-0.1066308543086052,-0.1375073939561844,-0.16377116739749908,-0.17269007861614227,-0.2443411946296692,-0.23309531807899475,-0.17509619891643524,0.16728061437606812,-0.753510057926178,0.8308292031288147,-0.06623710691928864,-0.23309531807899475,-0.14631108939647675,-0.12400072813034058,-0.06871336698532104,-0.0706915631890297,-0.11969759315252304,-0.17860564589500427,-0.07276797294616699,-0.15911327302455902,0.16364388167858124,-0.04950860142707825,-0.2013198733329773,-0.47282421588897705,0.23679718375205994,-0.23309531807899475,-0.17384956777095795,0.8113979697227478,0.5536919236183167,-0.16456083953380585,0.8433327078819275,-0.4247491657733917,-0.07247135043144226,-0.16252350807189941,-0.8538260459899902,-0.15789368748664856,-0.05167720466852188,-0.23309531807899475,-0.7088763117790222,0.5905070304870605,-0.19658181071281433,-0.10519982874393463,-0.3475031852722168,-0.17582876980304718,0.05955692380666733,-0.7273485660552979,-0.014379027299582958,-0.07820351421833038,0.4359709918498993,-0.1625455617904663,0.6305781006813049,-0.7144131064414978,0.26140642166137695,-0.14582706987857819,-0.15621310472488403,0.6504061818122864,-0.0773286521434784,-0.06563083082437515,-0.40285366773605347,0.009828360751271248,-0.23309531807899475,-0.28136980533599854,-0.5727112293243408,-0.15731646120548248,0.8624853491783142,-0.06456255167722702,-0.4994317293167114,-0.12055943161249161,0.8663305640220642,-0.2071843147277832,-0.5584339499473572,-0.6298365592956543,-0.08427239954471588,-0.12305472791194916,-0.23309531807899475,-0.1627860814332962,-0.09404513984918594,0.07316380739212036,-0.1396559625864029,-0.09684709459543228,-0.7201592326164246,-0.07519212365150452,0.1142369955778122,-0.17602717876434326,0.3900494873523712,0.6348090767860413,-0.16076777875423431,-0.4788690507411957,-0.23309531807899475,-0.08526450395584106,0.7220152020454407,-0.10397221893072128,-0.1334768384695053,-0.08571746200323105,-0.10401029884815216,-0.08707234263420105,0.2051880806684494,0.5731080770492554,-0.18476873636245728,-0.08266898989677429,-0.09140053391456604,-0.07376309484243393,-0.6208494901657104,-0.17469936609268188,-0.23309531807899475,-0.11814931035041809,0.24240608513355255,0.03197036683559418,-0.798965573310852,-0.14467863738536835,-0.642855167388916,-0.3662189245223999,-0.15874548256397247,-0.5898877382278442,-0.07893005758523941,-0.07130687683820724,-0.07910843938589096,0.33926570415496826,-0.07565409690141678,0.49878832697868347,-0.1599450409412384,-0.6488578915596008,0.47444668412208557,-0.15895693004131317,-0.08080704510211945,0.8200823664665222,-0.1595892757177353,-0.19280077517032623,-0.19395524263381958,-0.5719786882400513,-0.0726577416062355,0.19910617172718048,0.308789998292923,-0.19252519309520721,-0.29730862379074097,-0.7578081488609314,0.10828937590122223,-0.1862889677286148,-0.448158860206604,-0.13586491346359253,0.8228522539138794,0.7978331446647644,-0.16966065764427185,-0.10576602071523666,-0.1646369993686676,-0.23470273613929749,-0.1303582340478897,-0.09081128239631653,0.7771923542022705,-0.19551654160022736,-0.08560923486948013,-0.08865570276975632,-0.07144816964864731,0.09512142837047577,-0.5919541120529175,-0.1938009113073349,-0.3526601493358612,-0.16336829960346222,0.815946638584137,-0.5674901604652405,-0.2127060443162918,-0.16357873380184174,-0.06644254922866821,0.8560868501663208,-0.07801411300897598,0.8260660767555237,-0.04362811520695686,-0.8868581056594849,-0.15664604306221008,-0.513806164264679,-0.17030301690101624,-0.044708412140607834,-0.07056128978729248,0.5867961645126343,-0.1720036417245865,0.5993808507919312,-0.07622732222080231,-0.1629624366760254,-0.023543480783700943,-0.14152494072914124,-0.17187334597110748,-0.33204135298728943,-0.23309531807899475,-0.15621310472488403,-0.14785335958003998,-0.38020461797714233,-0.09140553325414658,-0.21753229200839996,0.5079377293586731,-0.11159038543701172,-0.16277404129505157,0.47034797072410583,-0.08514424413442612,0.3498291075229645,-0.07221881300210953,-0.7152037620544434,0.41109615564346313,-0.06528007984161377,-0.38790997862815857,-0.09682804346084595,0.19773626327514648,-0.7754175662994385,-0.13460823893547058,-0.07598180323839188,-0.10271754860877991,0.7501329183578491,-0.2319268435239792,-0.2038542479276657,-0.20898714661598206,0.12264686077833176,-0.15827548503875732,-0.11390931159257889,0.41875144839286804,-0.15827348828315735,-0.10195192694664001,-0.07632151991128922,-0.16487149894237518,-0.5992175340652466,-0.23309531807899475,-0.16010938584804535,-0.0783187597990036,-0.2162555754184723,-0.08705028891563416,-0.38317692279815674,0.052933864295482635,-0.07092105597257614,-0.07210957258939743,-0.8295745253562927,-0.38340941071510315,-0.11230089515447617,-0.15822939574718475,-0.16131094098091125,-0.1596975177526474,-0.17455604672431946,-0.07053223252296448,0.42534440755844116,-0.3001967668533325,-0.19810603559017181,-0.41408854722976685,0.44094452261924744,-0.2227092683315277,-0.15621310472488403,0.09070003032684326,-0.8256502151489258,-0.14534404873847961,-0.4637559652328491,-0.3809211254119873,0.271937757730484,-0.35417434573173523,-0.622298538684845,-0.4266511797904968,-0.21552503108978271,-0.06870835274457932,-0.7591330409049988,0.7273605465888977,-0.41410356760025024,-0.07638465613126755,-0.6916507482528687,-0.07739680260419846,0.8463972210884094,0.1928609013557434,-0.47399473190307617,-0.293881356716156,-0.2614695429801941,-0.15835265815258026,-0.6638677716255188,0.8335950374603271,-0.23309531807899475,-0.23134559392929077,0.6956391930580139,-0.15997309982776642,0.35742223262786865,-0.1643233299255371,-0.16235215961933136,-0.06492232531309128,-0.2224627435207367,0.35791027545928955,-0.15824943780899048,-0.2031838297843933,-0.1279410868883133,-0.5956799983978271,-0.07011333853006363,-0.15822339057922363,-0.023948339745402336,-0.060013897716999054,-0.29389238357543945,0.8274871110916138,-0.19500647485256195,-0.21025682985782623,0.6633597016334534,-0.33831867575645447,-0.15661296248435974,-0.006901150569319725,-0.06793471425771713,-0.01304719876497984,-0.16106440126895905,-0.4317760765552521,-0.4116723835468292,0.21998950839042664,-0.07631450891494751,-0.8878281712532043,0.836431086063385,-0.15621310472488403,0.5351384878158569,-0.45955005288124084,-0.16019758582115173,-0.15621310472488403,-0.16280010342597961,0.7250356078147888,-0.08640993386507034,0.03621738404035568,-0.10153504461050034,-0.1372869312763214,-0.23309531807899475,-0.8147761225700378,-0.18884237110614777,-0.09078822284936905,0.8255379796028137,-0.20277796685695648,-0.22314520180225372,-0.7918674945831299,0.018684159964323044,-0.1601504683494568,-0.16028174757957458,-0.4950033128261566,-0.08495484292507172,-0.23309531807899475,0.531809389591217,-0.4959523379802704,-0.08141834288835526,-0.0948498547077179,-0.23309531807899475,-0.5034131407737732,-0.23378178477287292,-0.3433704078197479,-0.351625919342041,-0.07728656381368637,-0.1248365044593811,-0.08100045472383499,-0.13985738158226013,-0.22195768356323242,-0.21440865099430084,-0.40759074687957764,-0.18553034961223602,-0.16786184906959534,0.1303451955318451,-0.24672424793243408,0.4276132583618164,-0.1617077738046646,0.8655939698219299,0.7756731510162354,0.8570207953453064,-0.15769827365875244,0.48681390285491943,-0.2020273655653,0.7921731472015381,-0.15947002172470093,-0.1677776724100113,-0.06598958373069763,-0.07335522025823593,-0.23309531807899475,-0.20752905309200287,0.03339238464832306,0.37819933891296387,-0.12701714038848877,0.10346313565969467,-0.1559024602174759,-0.17434661090373993,-0.20232300460338593,-0.16466304659843445,-0.19216443598270416,-0.07362479716539383,-0.19602462649345398,-0.1178206130862236,-0.8248475193977356,-0.5909920334815979,-0.16054029762744904,-0.08270206302404404,-0.20140604674816132,0.07910040766000748,-0.3705922067165375,0.5264861583709717,-0.23217937350273132,0.18240170180797577,-0.15914833545684814,-0.0688987672328949,-0.15720520913600922,0.7508955001831055,-0.2055748999118805,0.7739624977111816,0.7510347962379456,0.22858074307441711,0.573582112789154,-0.10172845423221588,0.6822087168693542,0.3959750831127167,-0.4178875982761383,0.2850506007671356,-0.20638862252235413,-0.013605386018753052,0.8728864789009094,0.8168926239013672,0.3311314284801483,-0.1669679433107376,-0.16161257028579712,-0.23086757957935333,0.8419438004493713,-0.16935300827026367,0.29357168078422546,0.29235410690307617,-0.38457587361335754,0.6145300269126892,-0.25754621624946594,0.5359121561050415,-0.10118129104375839,-0.08062165230512619,-0.7260868549346924,-0.12294749915599823,-0.16568322479724884,-0.16059041023254395,0.7502902746200562,0.8829989433288574,-0.1665831208229065,-0.23309531807899475,0.43664342164993286,-0.23309531807899475,-0.48696625232696533,0.8687717318534851,-0.0938657596707344,-0.43619945645332336,-0.0671791136264801,0.1317211240530014,-0.06873240321874619,-0.06963131576776505,-0.07457581162452698,-0.021679524332284927,-0.8154445290565491,-0.06645657867193222,-0.15621310472488403,0.25386741757392883,-0.1500149518251419,0.3859718441963196,0.5723294615745544,-0.25414401292800903,-0.16159753501415253,0.271411657333374,-0.7023985981941223,-0.2893346846103668,-0.5409367680549622,-0.07387632876634598,-0.23309531807899475,-0.06686444580554962,0.11224177479743958,-0.19422580301761627,-0.10268749296665192,-0.16326206922531128,-0.16191823780536652,-0.09537697583436966,-0.3307977318763733,-0.07037990540266037,-0.15621310472488403,-0.0683014914393425,-0.16103436052799225,-0.3221924602985382,-0.15917140245437622,-0.6348121166229248,-0.20058931410312653,-0.02778748981654644,-0.8172323703765869,-0.21085210144519806,-0.814911425113678,-0.2044154405593872,-0.06555767357349396,-0.08089923858642578,-0.17219604551792145,-0.795560359954834,0.35351693630218506,-0.14043161273002625,-0.20285913348197937,-0.26646214723587036,-0.12133106589317322,-0.13557830452919006,0.3653580844402313,-0.09228441119194031,0.6645672917366028,-0.1471729278564453,-0.24115943908691406,-0.15731744468212128,-0.22258000075817108,-0.19899293780326843,-0.16115762293338776,-0.18339480459690094,-0.29693683981895447,0.19111721217632294,-0.5185742974281311,-0.15621310472488403,-0.08683384209871292,-0.21731781959533691,-0.8560296893119812,0.8373790979385376,-0.7858767509460449,-0.07265373319387436,-0.21992436051368713,0.6444776058197021,-0.23309531807899475,0.6894891262054443,0.8518628478050232,-0.7458678483963013,-0.19076445698738098,-0.08252468705177307,-0.4552970230579376,-0.23309531807899475,0.3858025074005127,0.11077465116977692,-0.15621310472488403,-0.0868067815899849,-0.16988714039325714,-0.07240018993616104,-0.16951735317707062,-0.17590491473674774,-0.8537458181381226,-0.8818936347961426,-0.23309531807899475,0.8359851837158203,-0.0672081708908081,-0.07822655886411667,-0.7538698315620422,0.6301562190055847,0.26832109689712524,-0.8617197871208191,0.21752125024795532,0.4766252636909485,-0.6521508693695068,-0.01816706918179989,0.4540874660015106,0.5026485323905945,0.7360941171646118,0.4858308434486389,0.4282536208629608,-0.1616496592760086,-0.0037324237637221813,0.04800039902329445,-0.19722817838191986,-0.21916474401950836,-0.14178046584129333,-0.08180715888738632,-0.12634770572185516,-0.16032986342906952,0.41228270530700684,-0.06962429732084274,0.18356618285179138,-0.15713006258010864,-0.20132187008857727,-0.15755595266819,0.40771299600601196,-0.07714126259088516,0.8250649571418762,0.8753026127815247,-0.08125700056552887,0.3565143346786499,0.31030622124671936,-0.09840940684080124,-0.08537573367357254,-0.07167765498161316,-0.5776036381721497,-0.07053723186254501,-0.15621310472488403,-0.55024653673172,-0.06392320245504379,-0.1835351139307022,0.7069331407546997,-0.21584570407867432,-0.16121172904968262,-0.5368180871009827,0.7416608929634094,-0.21311290562152863,0.007003366015851498,0.06014217063784599,-0.07985702902078629,0.8040122985839844,-0.5319036245346069,0.21825379133224487,-0.07746093720197678,-0.13689309358596802,-0.07153134793043137,-0.09366734325885773,-0.07682458311319351,-0.432233065366745,0.023839103057980537,-0.2515895962715149,0.09957487136125565,0.020114198327064514,-0.059154074639081955,-0.17305687069892883,-0.646833598613739,-0.12034296989440918,-0.03786187246441841,-0.15800991654396057,-0.46256041526794434,0.7757683396339417,-0.18891853094100952,-0.08957866579294205,-0.1791858822107315,0.007243874948471785,-0.16091008484363556,-0.16664326190948486,-0.3606831431388855,-0.0851161852478981,-0.15985684096813202,-0.8491671085357666,0.5733235478401184,-0.7681651711463928,0.14815600216388702,0.3758513629436493,-0.1932918131351471,0.8630215525627136,0.036760538816452026,-0.2994632124900818,-0.0847083255648613,0.8235377073287964,0.09296685457229614,-0.565427839756012,-0.32763200998306274,-0.0155635392293334,-0.07443951815366745,-0.08121791481971741,0.689093291759491,-0.5499539971351624,0.5580060482025146,0.4630143940448761,-0.1897583305835724,0.6071333289146423,-0.16100329160690308,0.19890174269676208,0.6618664860725403,0.4640055000782013,-0.17479054629802704,-0.181086927652359,0.1261472851037979,-0.3822559714317322,0.5988687872886658,-0.1252453774213791,-0.09598926454782486,-0.3463316857814789,-0.5230969190597534,-0.08200658112764359,0.6567075848579407,-0.06434008479118347,-0.11028261482715607,-0.09339877218008041,-0.17569947242736816,-0.0891307070851326,-0.44031018018722534,-0.8879594802856445,-0.08417919278144836,-0.23025628924369812,-0.23007290065288544,0.013165445066988468,-0.12544779479503632,-0.06783550977706909,-0.13588595390319824,-0.1364852488040924,0.5375837087631226,-0.26280540227890015,0.3728038966655731,0.2107769399881363,-0.16024768352508545,-0.15983881056308746,-0.07033480703830719,-0.15833662450313568,0.7869119644165039,-0.8783180117607117,-0.5011563897132874,-0.3358955383300781,0.15141792595386505,-0.08407998830080032,-0.17388564348220825,-0.15621310472488403,-0.37293216586112976,-0.08591988682746887,-0.6795410513877869,-0.42535245418548584,-0.1826612651348114,-0.7552908658981323,-0.17357195913791656,0.2539956867694855,-0.07342036068439484,-0.15621310472488403,0.026732247322797775,-0.15912029147148132,-0.1576792299747467,0.680711567401886,-0.2260984629392624,0.44483277201652527,-0.2131730318069458,-0.17608530819416046,-0.1645057052373886,-0.642057478427887,-0.23309531807899475,-0.11090894043445587,-0.09726697951555252,-0.16516511142253876,-0.15757200121879578,0.834563136100769,-0.24947910010814667,-0.1984718143939972,-0.15784558653831482,0.27492812275886536,-0.1656200885772705,-0.23309531807899475,-0.06711196899414062,-0.08737397938966751,-0.1633913666009903,-0.23309531807899475,-0.15621310472488403,0.8893283605575562,-0.159829780459404,-0.11477013677358627,-0.41622206568717957,-0.0941794291138649,-0.18805070221424103,-0.15679535269737244,-0.11246825009584427,0.48228129744529724,-0.20048509538173676,-0.5364643335342407,0.8151869773864746,-0.18723395466804504,0.6420564651489258,0.29869556427001953,-0.11056921631097794,-0.16094717383384705,-0.2009330540895462,-0.1208149716258049,-0.2025945782661438,-0.014379027299582958,-0.08282332122325897,-0.43648308515548706,-0.22289668023586273,-0.23273354768753052,-0.30946144461631775,-0.06593246012926102,-0.7777214646339417,-0.07290225476026535,0.561578631401062,0.7128387689590454,-0.4738544225692749,-0.4111352562904358,0.5025633573532104,-0.2882203459739685,-0.15921248495578766,0.6985032558441162,-0.16331319510936737,-0.6764224171638489,0.13265912234783173,0.49115511775016785,-0.736368715763092,-0.5242864489555359,-0.15621310472488403,-0.19835758209228516,-0.0709821805357933,0.7637448310852051,-0.16212067008018494,0.7472718358039856,-0.7095367312431335,-0.2863062918186188,-0.10259128361940384,-0.1573224663734436,-0.1124371886253357,-0.16861042380332947,-0.2116868793964386,-0.21823276579380035,-0.08393968641757965,-0.13216406106948853,0.7266841530799866,-0.04884118586778641,0.7384711503982544,0.3191460072994232,0.06747272610664368,0.031849104911088943,0.12115269154310226,0.24199523031711578,-0.6057053208351135,-0.5613651871681213,-0.047787945717573166,-0.06812311708927155,0.022179584950208664,0.5253857970237732,0.251522421836853,-0.040024466812610626,-0.1299213021993637,-0.7295191287994385,-0.13428056240081787,-0.1870044767856598,-0.6297032833099365,-0.17738205194473267,0.480637788772583,0.3422720730304718,0.17043429613113403,-0.15621310472488403,-0.0703779011964798,0.25390350818634033,0.6278863549232483,-0.17908266186714172,0.5265582799911499,-0.5825852155685425,-0.17303180694580078,-0.28279784321784973,0.6947864294052124,-0.1329597383737564,-0.14711681008338928,0.8167954087257385,-0.1718783676624298,-0.16360579431056976,-0.15972557663917542,-0.09989657253026962,0.4595610797405243,-0.2303965836763382,0.2647415101528168,-0.21740101277828217,-0.2161303013563156,-0.23309531807899475,-0.16101230680942535,-0.19287092983722687,-0.7753373980522156,-0.07256955653429031,-0.854308009147644,-0.11974169313907623,0.16800916194915771,-0.8068603277206421,-0.3507671058177948,-0.15823540091514587,0.06733443588018417,-0.18659061193466187,0.5131558179855347,-0.1603098213672638,0.4320867657661438,-0.15919344127178192,-0.23309531807899475,-0.7157468795776367,0.3057916462421417,-0.09000356495380402,0.18705858290195465,-0.20150426030158997,-0.37747883796691895,0.014664629474282265,-0.17992344498634338,-0.15915335714817047,0.4681242108345032,0.516061007976532,-0.16443657875061035,-0.4800976514816284,-0.8841633796691895,-0.783193051815033,-0.2201458364725113,-0.15310853719711304,-0.20176981389522552,-0.0716526061296463,-0.16263073682785034,0.3124106824398041,0.06414065510034561,-0.1749669313430786,0.38758426904678345,-0.1957530528306961,-0.22476965188980103,-0.1404937505722046,0.16756922006607056,-0.15621310472488403,-0.08392065018415451,-0.158678337931633,-0.07149627804756165,0.25184011459350586,-0.5174359083175659,-0.15856610238552094,-0.15621310472488403,-0.08587279170751572,-0.1863841563463211,0.8079847097396851,-0.23309531807899475,0.10245900601148605,-0.04045337811112404,-0.21328526735305786,-0.8219162821769714,-0.03455786034464836,0.7821037769317627,-0.1643614023923874,-0.24017333984375,0.16047315299510956,0.6830103993415833,-0.11602079123258591,0.584801971912384,-0.6598742604255676,-0.25301459431648254,-0.4847475588321686,-0.11883176118135452,-0.13318723440170288,-0.10067421197891235,-0.4129721522331238,-0.169284850358963,-0.1615404188632965,-0.08346568793058395,-0.6385771036148071,-0.06477801501750946,-0.20280902087688446,-0.08267199993133545,-0.21770966053009033,-0.4481738805770874,-0.5949354767799377,0.31687217950820923,-0.06936274468898773,-0.8284160494804382,0.8502093553543091,-0.07898717373609543,-0.14195285737514496,-0.1624142825603485,-0.7860932350158691,-0.19911418855190277,-0.15621310472488403,-0.45120730996131897,-0.6795660853385925,0.2890681326389313,-0.5518459677696228,-0.21699915826320648,-0.09825307130813599,-0.0684548169374466,-0.14182457327842712,-0.08464118093252182,-0.6049476861953735,-0.01808389276266098,0.05831829458475113,0.2498999983072281,-0.16404172778129578,-0.17530864477157593,-0.1573004275560379,-0.16418102383613586,-0.2063325047492981,-0.1944572925567627,-0.7546945810317993,-0.09430168569087982,-0.8879594802856445,-0.23309531807899475,-0.10602858662605286,0.7216805219650269,-0.15621310472488403,0.05217825993895531,-0.20675040781497955,-0.01413851510733366,-0.1674860417842865,0.7972278594970703,0.3976696729660034,-0.25900134444236755,-0.09128929674625397,0.5867279767990112,-0.04430856183171272,-0.7385814189910889,-0.1653004139661789,-0.5962021350860596,-0.0946383997797966,0.793593168258667,-0.6216741800308228,-0.15892186760902405,-0.049920473247766495,-0.002056867117062211,-0.1803663820028305,0.12768754363059998,0.22931328415870667,-0.7654153108596802,0.7992762327194214,-0.16825568675994873,-0.17682887613773346,-0.683977484703064,0.026172056794166565,-0.09935341775417328,-0.1615915298461914,-0.07398856431245804,-0.15621310472488403,0.7890365123748779,-0.23309531807899475,0.8491991758346558,0.25935307145118713,-0.8098607063293457,-0.1234375387430191,-0.3259384334087372,0.8663305640220642,-0.1541757881641388,0.07763530313968658,-0.19315554201602936,-0.14481191337108612,-0.16263775527477264,-0.15621310472488403,0.22468245029449463,-0.23309531807899475,-0.07411583513021469,-0.7249564528465271,0.6708225011825562,-0.15733148157596588,-0.07052922248840332,-0.15847191214561462,-0.30808451771736145,-0.21420320868492126,0.8390436768531799,-0.1590651571750641,-0.06930362433195114,-0.21029290556907654,0.08961774408817291,-0.10825831443071365,-0.10450734943151474,-0.20881778001785278,-0.15621310472488403,-0.2680475115776062,0.5049253702163696,0.29452770948410034,-0.15621310472488403,-0.08628065884113312,-0.20109137892723083,0.6477094292640686,-0.707230806350708,0.3213426470756531,0.5572283864021301,-0.21554507315158844,0.8510871529579163,0.7228139042854309,-0.5490971207618713,-0.24799194931983948,-0.21371117234230042,-0.26496195793151855,-0.16186010837554932,0.41963228583335876,-0.6319229602813721,-0.1643233299255371,-0.516862690448761,0.461855947971344,-0.23309531807899475,0.5917366743087769,-0.8333595395088196,0.7977710366249084,-0.06539633125066757,0.8057358860969543,-0.10071129351854324,0.7361181378364563,-0.5891571640968323,-0.06757896393537521,0.49186766147613525,0.6656375527381897,0.7786263823509216,-0.15621310472488403,-0.040844209492206573,-0.17591693997383118,-0.8556950092315674,-0.2088819146156311,-0.20077672600746155,-0.23309531807899475,0.24380604922771454,-0.10844270139932632,0.6272830963134766,-0.13232842087745667,-0.08243249356746674,-0.5189761519432068,-0.15324783325195312,-0.35430261492729187,-0.6983489990234375,0.5472021102905273,-0.1727893054485321,-0.07295536994934082,-0.16026070713996887,-0.16834485530853271,-0.23309531807899475,-0.0686873123049736,-0.05615871399641037,0.0731748417019844,-0.7522293925285339,-0.16139110922813416,-0.08736797422170639,0.8222560286521912,-0.13965797424316406,0.13433466851711273,-0.46024954319000244,-0.47024574875831604,-0.07021354883909225,-0.7248021364212036,-0.17744016647338867,-0.4702407419681549,-0.17052750289440155,0.4294711947441101,0.07856327295303345,-0.1583275943994522,-0.16401167213916779,-0.15720321238040924,-0.21197950839996338,-0.0746910572052002,-0.18120115995407104,-0.16027873754501343],\"z\":[0.3789254128932953,-0.3787590563297272,-0.37115588784217834,-0.26957327127456665,0.3201526403427124,-0.27252453565597534,-0.2602825164794922,0.3083505630493164,0.18600787222385406,-0.37926408648490906,-0.254714697599411,-0.22331205010414124,-0.3762447237968445,-0.25839653611183167,0.29983648657798767,0.34908100962638855,0.03833036869764328,-0.27014046907424927,-0.26849594712257385,-0.29298293590545654,-0.07096264511346817,-0.016790639609098434,0.3594881296157837,0.3183528184890747,0.21012908220291138,-0.35860925912857056,-0.2885324954986572,-0.2894805073738098,-0.2374931424856186,-0.27036193013191223,-0.28293561935424805,-0.2616424262523651,-0.33622974157333374,-0.2859610617160797,-0.2973431944847107,-0.2825157344341278,0.19410806894302368,0.30211734771728516,0.11823098361492157,0.37002047896385193,-0.05853024870157242,-0.10582965612411499,0.28009358048439026,0.3656732738018036,0.257802277803421,0.046147968620061874,-0.23418213427066803,-0.3191986382007599,0.28528860211372375,0.37272021174430847,-0.12368255853652954,0.24658043682575226,-0.2646067142486572,0.06712249666452408,0.16288478672504425,0.04221261292695999,-0.18796201050281525,-0.13446544110774994,-0.24711757898330688,-0.2699620723724365,-0.2878350019454956,-0.31885188817977905,-0.2523547112941742,0.3703712224960327,0.2520290017127991,-0.2619951665401459,-0.11398597061634064,0.07864496111869812,0.28799039125442505,0.26863226294517517,-0.29669684171676636,-0.22151723504066467,-0.22092197835445404,-0.21734438836574554,-0.3169918954372406,-0.3061348795890808,-0.2350369393825531,-0.31638461351394653,-0.30259835720062256,0.09304352104663849,-0.35404959321022034,-0.32090824842453003,0.17166340351104736,-0.2409094125032425,-0.24295775592327118,-0.28402093052864075,0.27117764949798584,0.3594881296157837,-0.26436522603034973,-0.2789261043071747,-0.28525254130363464,-0.3049834370613098,0.3741542398929596,-0.02087230607867241,-0.2520189583301544,-0.2903413474559784,-0.2919357419013977,0.14738187193870544,0.09151928126811981,-0.06707137823104858,0.047810494899749756,-0.013604877516627312,0.37486475706100464,-0.17796076834201813,-0.14763841032981873,-0.1851891130208969,-0.1500294804573059,0.35908329486846924,0.1585686206817627,0.3266564607620239,0.13884976506233215,-0.038644034415483475,0.01007639616727829,-0.27867957949638367,-0.2740948796272278,-0.2600129544734955,0.3171432614326477,-0.1806735396385193,-0.15149961411952972,-0.3140276074409485,0.026640553027391434,0.3654899001121521,0.0012165838852524757,0.28653424978256226,0.08032151311635971,0.1865890920162201,0.2940211594104767,0.31027165055274963,-0.2522173821926117,0.27693086862564087,0.12490013986825943,0.29212111234664917,-0.35999220609664917,0.22255845367908478,-0.2627718150615692,-0.2639613449573517,0.29536303877830505,-0.04039975628256798,0.24881017208099365,-0.19074591994285583,-0.19188334047794342,-0.12409242242574692,-0.2541474997997284,-0.02295873686671257,0.16498222947120667,0.08888769149780273,0.2724123001098633,0.2896779179573059,-0.09551376104354858,0.29254603385925293,0.3527568280696869,-0.07675192505121231,0.19674064218997955,-0.24168004095554352,0.3787219524383545,-0.08562877029180527,0.19889721274375916,-0.011991453357040882,-0.2671240270137787,-0.22863034904003143,0.08539928495883942,0.27518215775489807,-0.3247573971748352,0.30128055810928345,0.3063403069972992,-0.2566027045249939,0.24658043682575226,-0.044446349143981934,-0.35551270842552185,-0.2955884635448456,-0.3274661600589752,0.3594881296157837,-0.31750601530075073,0.2499716579914093,0.18389137089252472,-0.24856264889240265,-0.285969078540802,0.30250415205955505,-0.23407189548015594,0.3602317273616791,-0.19567438960075378,0.2953740060329437,-0.3419288396835327,-0.28757548332214355,-0.23630663752555847,-0.017097292467951775,-0.09579635411500931,0.21437408030033112,-0.06889624893665314,0.10032597929239273,-0.2672834098339081,-0.1312836855649948,-0.2417181134223938,-0.3403855562210083,0.31974276900291443,0.10488565266132355,-0.2957167625427246,0.31279805302619934,-0.24347685277462006,-0.30369067192077637,0.209443598985672,0.3719435930252075,-0.27549582719802856,-0.1713046431541443,0.31118056178092957,-0.28103959560394287,-0.26309651136398315,0.07514653354883194,-0.27586662769317627,0.3250930905342102,-0.23980404436588287,-0.2303239405155182,-0.2628399729728699,-0.26354146003723145,-0.3281947076320648,-0.2565676271915436,-0.22581034898757935,-0.3648545444011688,-0.21932056546211243,-0.2754497230052948,0.11963897943496704,-0.256986528635025,-0.2997332811355591,-0.12470772862434387,-0.2665608525276184,-0.21734438836574554,0.011246878653764725,-0.32662540674209595,0.28561627864837646,0.34808194637298584,-0.3104640245437622,-0.23319703340530396,0.3379954993724823,0.339094877243042,-0.26803797483444214,0.3594881296157837,0.3132770359516144,-0.3319877088069916,0.377460241317749,-0.32042622566223145,0.3111194670200348,0.3439521789550781,-0.22637054324150085,-0.3574768602848053,0.04694465920329094,0.13081370294094086,-0.27881789207458496,-0.21734438836574554,0.11067794263362885,-0.28077906370162964,-0.27519917488098145,-0.24698631465435028,-0.29110997915267944,-0.19827090203762054,0.24779704213142395,0.3324717879295349,0.22364777326583862,-0.25581100583076477,-0.3200664520263672,-0.1937422901391983,0.3321961760520935,-0.23848827183246613,0.14277708530426025,0.3184640407562256,0.36655914783477783,0.2044159471988678,-0.2926121652126312,0.3240959942340851,-0.2708148956298828,-0.21734438836574554,-0.2385624349117279,0.249118834733963,-0.2626104950904846,0.30680227279663086,-0.05944819748401642,0.036271002143621445,-0.3426142930984497,-0.2788459360599518,-0.17319364845752716,-0.3007614314556122,-0.36298254132270813,-0.2913013696670532,0.23306076228618622,0.15640702843666077,-0.26575416326522827,-0.2899254262447357,-0.16746649146080017,0.18684464693069458,-0.24950766563415527,-0.2539490759372711,0.14773963391780853,-0.061098698526620865,-0.31168967485427856,0.3123480975627899,-0.2997473180294037,-0.2094866931438446,-0.24755249917507172,0.3178076446056366,-0.05135601758956909,0.3356515169143677,-0.2038046419620514,-0.28530561923980713,-0.015192248858511448,0.030395524576306343,-0.3291807770729065,0.02038828283548355,-0.3199862539768219,-0.21734438836574554,-0.28212690353393555,-0.24259799718856812,-0.2251439392566681,0.025035150349140167,0.31344541907310486,-0.3200574517250061,0.045249056071043015,-0.2723962366580963,0.32065367698669434,-0.2810235321521759,0.28243058919906616,0.29288774728775024,-0.27081090211868286,0.25662174820899963,-0.3002132773399353,0.13642962276935577,0.35591357946395874,0.3774372637271881,-0.13320676982402802,-0.09527625143527985,-0.23792606592178345,-0.20108987390995026,0.24658043682575226,-0.07808375358581543,-0.30760398507118225,0.032511018216609955,-0.12243691086769104,-0.3279711902141571,-0.26829251646995544,0.08964429050683975,-0.28074997663497925,0.24658043682575226,0.31803613901138306,-0.21453942358493805,0.3026414215564728,-0.005158948712050915,0.13720928132534027,-0.32306283712387085,-0.2618488371372223,-0.28114479780197144,-0.11777500808238983,0.3012595474720001,0.31156638264656067,-0.06845030933618546,-0.33947762846946716,0.3498275876045227,-0.3633353114128113,0.3059234321117401,0.24658043682575226,-0.07834531366825104,0.26122555136680603,-0.11567655950784683,-0.21734438836574554,-0.1385050117969513,-0.3241090178489685,0.044093601405620575,0.07079429179430008,0.35186898708343506,-0.24700534343719482,0.11606739461421967,-0.24612846970558167,0.1309630125761032,0.23797519505023956,-0.28923898935317993,0.35186898708343506,-0.250926673412323,0.06833105534315109,-0.02224121242761612,0.2840901017189026,0.21449333429336548,-0.27332621812820435,-0.3040213882923126,0.35498857498168945,0.3177325129508972,0.35601577162742615,-0.13287606835365295,0.2818743586540222,-0.27034488320350647,0.3146759867668152,0.011334063485264778,-0.2646077275276184,-0.3320408761501312,-0.02267613634467125,0.25208014249801636,-0.2785022258758545,0.3594881296157837,-0.3518368899822235,-0.2625263035297394,-0.23230011761188507,-0.03864102438092232,-0.3229205012321472,-0.009641464799642563,0.3650980591773987,0.21606966853141785,-0.042535290122032166,-0.26465079188346863,-0.3067782521247864,-0.22917649149894714,-0.26459166407585144,-0.04742066189646721,-0.27134501934051514,-0.35058820247650146,-0.2915489077568054,0.30871933698654175,-0.09206744283437729,0.17869234085083008,-0.2922924757003784,-0.14287230372428894,-0.2579425573348999,-0.21734438836574554,-0.20823904871940613,-0.3103969097137451,0.3416011333465576,-0.018186604604125023,0.33185744285583496,-0.2651338279247284,-0.33397796750068665,0.3166291415691376,0.3004177510738373,0.24658043682575226,0.03060196340084076,0.09919557720422745,0.2422863394021988,-0.008854794315993786,0.10571842640638351,0.03560759499669075,-0.26512083411216736,0.24658043682575226,-0.25908297300338745,-0.229171484708786,-0.3297620117664337,0.06304984539747238,0.374342679977417,0.3435192406177521,0.16937856376171112,-0.1745305061340332,-0.29427167773246765,0.32992035150527954,-0.2795363962650299,0.16249795258045197,0.2856784760951996,0.24658043682575226,0.3296768367290497,-0.30473190546035767,0.3050445318222046,-0.23302166163921356,-0.35097408294677734,-0.26038673520088196,-0.25277456641197205,-0.293430894613266,-0.3043059706687927,-0.2633921205997467,0.20164906978607178,0.04945799335837364,-0.35018736124038696,-0.21734438836574554,0.052691854536533356,0.09822952747344971,-0.3367047607898712,0.149046391248703,-0.32390958070755005,0.360732764005661,-0.3139915466308594,0.23750418424606323,0.33482980728149414,-0.28805649280548096,-0.3476570248603821,-0.33211302757263184,-0.2557358741760254,-0.3521946370601654,-0.3155568540096283,-0.08768914639949799,-0.26218658685684204,-0.3012895882129669,-0.26792675256729126,-0.1795932501554489,-0.24612247943878174,-0.27927082777023315,-0.11797945201396942,0.25856590270996094,0.30926749110221863,-0.2407640963792801,-0.2719883918762207,-0.29246583580970764,-0.24373841285705566,-0.2665157616138458,-0.2836611568927765,0.3171162009239197,0.344121515750885,-0.2624892294406891,-0.2775532007217407,-0.3059895634651184,-0.2762865126132965,-0.17727433145046234,-0.23872576653957367,0.22137492895126343,0.3242974281311035,0.13755600154399872,-0.3270091712474823,-0.2513866424560547,-0.24091342091560364,-0.2719743549823761,-0.267916738986969,-0.09368988871574402,-0.009257650002837181,0.37386465072631836,-0.2946374714374542,-0.23399873077869415,-0.23694299161434174,-0.2617887258529663,0.27266883850097656,-0.2417181134223938,0.2697736620903015,-0.307506799697876,-0.07051168382167816,-0.22437530755996704,-0.33530277013778687,-0.21734438836574554,-0.3271715044975281,0.2563181519508362,0.0930715799331665,-0.1554209142923355,-0.29834333062171936,0.2987041175365448,-0.15270215272903442,-0.29834434390068054,0.35930272936820984,0.3594881296157837,0.13376395404338837,0.37922799587249756,-0.24426253139972687,-0.2676251232624054,0.21010901033878326,-0.04895692691206932,0.3594881296157837,-0.2596451938152313,-0.1695910096168518,-0.22270676493644714,-0.2840970754623413,-0.26367172598838806,-0.2399844378232956,-0.2688617408275604,0.3744438886642456,0.3620796501636505,0.31671229004859924,0.1778305023908615,-0.2501700818538666,-0.0871560126543045,0.2915819585323334,-0.24812772870063782,-0.34056395292282104,0.29800060391426086,-0.24713662266731262,-0.11665964871644974,0.18756717443466187,0.3594881296157837,-0.3361365497112274,0.32052943110466003,0.29014989733695984,0.30694860219955444,-0.23462606966495514,0.3049543499946594,-0.21734438836574554,-0.24984537065029144,-0.34747663140296936,0.0028189881704747677,0.14354372024536133,0.08883558958768845,0.1669463962316513,-0.24287357926368713,0.33215007185935974,-0.25632813572883606,0.20175930857658386,0.33534786105155945,-0.25631409883499146,0.16749657690525055,-0.36629459261894226,-0.3391178548336029,-0.24086731672286987,-0.13529419898986816,-0.36026477813720703,0.30201011896133423,-0.3348498046398163,-0.2733282148838043,-0.2603967785835266,-0.2679517865180969,-0.2389923334121704,0.044111642986536026,0.18369796872138977,-0.20800454914569855,-0.24393382668495178,0.02863479033112526,0.29824215173721313,-0.2782557010650635,-0.24268217384815216,-0.2836741805076599,-0.055000755935907364,0.2409464716911316,-0.32995346188545227,-0.2941604554653168,0.3145658075809479,-0.13659095764160156,-0.3277587890625,0.24601224064826965,-0.31567615270614624,-0.3075137734413147,0.31537145376205444,0.3594881296157837,0.2752603590488434,0.3388603627681732,-0.28971099853515625,-0.31880372762680054,0.37154772877693176,0.27834686636924744,-0.3225487172603607,0.3594881296157837,-0.21734438836574554,-0.2805004417896271,-0.2529650032520294,0.33950868248939514,-0.33729803562164307,0.24658043682575226,0.33110591769218445,-0.2656068205833435,-0.31535443663597107,-0.26654481887817383,-0.23416408896446228,-0.2658343017101288,-0.24119701981544495,0.2882879674434662,-0.07276546210050583,-0.2907482087612152,-0.2264396995306015,-0.36338138580322266,-0.26450949907302856,-0.3343677818775177,-0.24749338626861572,-0.006700221449136734,0.07658958435058594,0.12420064955949783,-0.264520525932312,0.2755278944969177,-0.26064229011535645,-0.2716957628726959,-0.2361082136631012,-0.2658753991127014,-0.28931117057800293,-0.013921551406383514,-0.0719737857580185,0.3275162875652313,-0.23822970688343048,-0.28699222207069397,-0.018217671662569046,-0.2376023828983307,-0.28971901535987854,-0.28888827562332153,0.33707958459854126,-0.16589416563510895,-0.3335169553756714,-0.29846858978271484,-0.2636536955833435,0.346455454826355,-0.1863565891981125,-0.2601412534713745,0.21893376111984253,0.37486475706100464,0.008661387488245964,-0.34012600779533386,0.2598235309123993,-0.2810826897621155,-0.2862526774406433,0.017177464440464973,-0.26555871963500977,0.3397882878780365,-0.2666320204734802,-0.26115238666534424,-0.2808632254600525,-0.25114914774894714,0.3589049279689789,0.3155548870563507,-0.17068833112716675,0.32873785495758057,0.24658043682575226,-0.30088770389556885,0.37486475706100464,0.3594881296157837,0.32364603877067566,0.07716881483793259,0.10697508603334427,-0.3116145133972168,0.03236370533704758,-0.32362502813339233,0.14372611045837402,0.22920455038547516,0.2865733802318573,0.026712708175182343,0.21191084384918213,0.3457629978656769,0.3488966226577759,-0.1387294977903366,0.08518283069133759,-0.2790122926235199,0.06631878763437271,-0.3198239505290985,0.09614910930395126,0.14580051600933075,0.33172717690467834,-0.2589547038078308,0.3594881296157837,-0.3399195671081543,0.21472783386707306,0.3197587728500366,-0.24411320686340332,-0.1868426352739334,-0.27946722507476807,-0.24023297429084778,-0.31459882855415344,0.13844390213489532,0.2003442943096161,-0.23117774724960327,0.29219725728034973,-0.23932905495166779,0.2915429174900055,0.3609592616558075,-0.047248296439647675,0.11853763461112976,-0.09909535944461823,-0.22657297551631927,0.3360503613948822,-0.12712787091732025,0.27115362882614136,0.17637743055820465,-0.22275285422801971,-0.2786354720592499,-0.2755218744277954,-0.2999858260154724,-0.2397880256175995,-0.1772843450307846,0.06778289377689362,-0.22789877653121948,-0.025093264877796173,-0.11245973408222198,-0.2417181134223938,-0.2907432019710541,0.32970693707466125,-0.272040456533432,-0.35472801327705383,-0.11538393050432205,-0.2755018472671509,-0.22172166407108307,-0.23839406669139862,0.24658043682575226,0.2882789373397827,0.1452132612466812,-0.25755271315574646,-0.0920373722910881,-0.3238885700702667,0.3228724002838135,-0.2438235878944397,-0.16366443037986755,0.29424765706062317,-0.3102877140045166,-0.23824675381183624,0.3725859224796295,-0.2496299147605896,-0.05241526663303375,-0.2856072783470154,-0.2898392677307129,0.23232319951057434,0.18785980343818665,0.24658043682575226,0.32148346304893494,-0.15234439074993134,0.2962619364261627,-0.2760249376296997,0.30752381682395935,-0.23763445019721985,-0.3014819920063019,0.16939058899879456,-0.2921622097492218,-0.32293352484703064,-0.3552190661430359,-0.3447709083557129,-0.26817628741264343,-0.2583133578300476,-0.2829847037792206,-0.15938334167003632,0.3338487148284912,-0.28782200813293457,-0.3086772859096527,0.059139542281627655,-0.2682875394821167,-0.10825179517269135,0.33110183477401733,-0.049252551048994064,-0.26012519001960754,-0.3070678114891052,-0.24196463823318481,-0.265994668006897,0.28632882237434387,-0.27820858359336853,0.3594881296157837,-0.24599018692970276,0.3065657913684845,-0.009367884136736393,0.07621980458498001,0.2938257157802582,0.28143545985221863,-0.2694880962371826,0.1077818050980568,-0.23390954732894897,-0.2840259373188019,-0.2379721701145172,-0.3639185428619385,-0.27906543016433716,0.32315200567245483,-0.24052459001541138,0.3170490562915802,0.3486781716346741,0.2554963529109955,0.3152482211589813,-0.08479700237512589,0.2198617309331894,-0.23459099233150482,-0.22914843261241913,-0.16452425718307495,0.3787500262260437,-0.27599987387657166,-0.3131487965583801,-0.27397358417510986,0.07709967344999313,-0.32025784254074097,0.27639973163604736,-0.08425486087799072,0.2042345404624939,0.3747064471244812,-0.24700233340263367,0.09819445013999939,-0.23409193754196167,-0.22738468647003174,-0.260633260011673,-0.24981029331684113,0.21528802812099457,-0.2681933045387268,0.2928396761417389,-0.22715520858764648,0.02146957814693451,0.12809793651103973,0.13724835216999054,-0.24474754929542542,-0.34780430793762207,0.3027988076210022,0.058669544756412506,0.3281135559082031,-0.2960444390773773,-0.25036749243736267,-0.14031687378883362,-0.004101706203073263,0.3594881296157837,-0.2621605396270752,-0.08314750343561172,-0.32180511951446533,0.24658043682575226,-0.1813730150461197,0.07004369050264359,0.34024327993392944,0.26250624656677246,-0.10377229005098343,-0.2704661190509796,-0.2350158840417862,0.027854131534695625,-0.2662481963634491,-0.3313153386116028,-0.3150237500667572,0.2984255254268646,-0.2519679069519043,-0.2129761129617691,-0.26964640617370605,0.3594881296157837,-0.23785090446472168,-0.17093385756015778,0.10120483487844467,0.16574785113334656,-0.20266422629356384,0.3183898627758026,-0.23730875551700592,0.3383151888847351,0.16423764824867249,-0.3379163146018982,-0.2639252543449402,-0.3535916209220886,-0.24757054448127747,-0.11306702345609665,0.3313845098018646,-0.31858429312705994,-0.050477150827646255,-0.299710214138031,-0.32726868987083435,-0.1456812620162964,0.11100664734840393,-0.2324514538049698,0.0689273253083229,-0.354132741689682,-0.17275872826576233,-0.2716697156429291,-0.2904185354709625,0.3238394558429718,0.012416359968483448,-0.23995135724544525,-0.24521654844284058,0.3448059856891632,-0.04985984042286873,-0.23774568736553192,-0.10861857980489731,0.2941223382949829,-0.29484590888023376,-0.26331496238708496,0.2837523818016052,-0.26306843757629395,-0.11855166405439377,-0.31863540410995483,-0.25113409757614136,-0.05252249166369438,-0.2428825944662094,-0.282982736825943,-0.2637859582901001,-0.2529900372028351,-0.04834061488509178,0.19701223075389862,0.10765153169631958,0.24124011397361755,-0.267171174287796,0.30135971307754517,-0.03918718546628952,-0.18991614878177643,-0.2781214118003845,-0.27398961782455444,0.20094357430934906,-0.24776996672153473,0.3594881296157837,0.3688660264015198,0.06152661144733429,0.13465887308120728,-0.2417181134223938,0.04946601390838623,0.30251115560531616,0.050369929522275925,-0.01935107633471489,-0.2644714117050171,0.10477441549301147,-0.05081687122583389,-0.1103823184967041,0.2564353942871094,-0.06157972291111946,-0.07264219969511032,-0.25549837946891785,0.28780293464660645,-0.3239867687225342,0.17513878643512726,0.3411071002483368,0.3780154585838318,0.263619601726532,0.006958772893995047,0.18267177045345306,0.23315295577049255,-0.1466422975063324,0.10992535948753357,0.12295801192522049,-0.24515841901302338,-0.27143120765686035,0.3594881296157837,-0.3050195276737213,-0.2688848078250885,0.2840700149536133,0.22714519500732422,-0.319847971200943,-0.27494966983795166,-0.21734438836574554,-0.35189399123191833,0.3538060784339905,0.03687528148293495,-0.1565072238445282,0.30350226163864136,-0.014633061364293098,-0.2634141743183136,0.2793109118938446,-0.2888782322406769,-0.2712969183921814,0.06209181249141693,0.3608369827270508,0.3398013114929199,0.24658043682575226,-0.2704451084136963,0.3110032379627228,-0.27642178535461426,-0.2598536014556885,-0.27447566390037537,0.2949370741844177,-0.33052465319633484,-0.3296958804130554,-0.2703068256378174,-0.24112285673618317,0.3470006287097931,-0.3148493468761444,0.09924869239330292,0.05800012871623039,0.3180331587791443,-0.2397950291633606,0.27302461862564087,0.35273078083992004,-0.3157963752746582,0.3377018868923187,-0.3563314378261566,0.21064817905426025,-0.3061809837818146,0.3740721046924591,-0.33489790558815,-0.24612247943878174,0.2898182272911072,0.31867846846580505,-0.20408223569393158,0.37366122007369995,-0.24896349012851715,-0.08497237414121628,0.10957662016153336,-0.26306045055389404,-0.26557478308677673,0.13254737854003906,0.2835879921913147,-0.029102778062224388,-0.2638500928878784,-0.22714318335056305,-0.28101256489753723,-0.31184497475624084,-0.005254149436950684,-0.26936081051826477,0.2497100979089737,-0.17920441925525665,-0.32803237438201904,0.3622760474681854,0.37486475706100464,-0.2640695571899414,0.361080527305603,-0.2038046419620514,-0.28492283821105957,-0.2683746814727783,-0.09578132629394531,-0.04607180133461952,-0.23938417434692383,0.161372572183609,0.30650562047958374,0.24658043682575226,0.2583363950252533,-0.01131401676684618,-0.2450762540102005,-0.23748914897441864,0.303469181060791,0.3777298927307129,-0.3041336238384247,0.3084868788719177,0.28176313638687134,0.3310076594352722,0.14900732040405273,-0.0788583979010582,0.22330904006958008,-0.27576035261154175,-0.04472995176911354,-0.29612860083580017,-0.06415719538927078,0.2910709083080292,-0.2559312880039215,0.01584363915026188,0.3026103675365448,-0.3472190499305725,-0.12478990107774734,0.3594881296157837,-0.25165823101997375,-0.3042649030685425,-0.10561719536781311,-0.2667061388492584,-0.2730456292629242,-0.21734438836574554,-0.28216901421546936,-0.32256779074668884,-0.23894523084163666,0.24658043682575226,0.3250540494918823,-0.2417181134223938,-0.3478143513202667,-0.3342265188694,0.37141144275665283,-0.08498841524124146,0.3397652208805084,-0.26753494143486023,0.2306155562400818,-0.16458037495613098,-0.36108648777008057,-0.1961624175310135,0.3218011260032654,0.03269140049815178,-0.23900334537029266,-0.24071700870990753,0.31037384271621704,-0.03342294692993164,-0.23984414339065552,0.08262839913368225,-0.34357932209968567,-0.26980775594711304,-0.14352868497371674,0.34776726365089417,0.15699227154254913,-0.32936516404151917,-0.17927956581115723,-0.3668667674064636,-0.23847323656082153,0.31988710165023804,0.22964750230312347,0.34775421023368835,0.23521031439304352,-0.23849327862262726,0.060805078595876694,-0.2942245900630951,0.013555782847106457,-0.2836311161518097,-0.20548519492149353,0.1909313201904297,-0.27615422010421753,-0.26568302512168884,-0.24612247943878174,0.364391565322876,-0.21972443163394928,0.25559258460998535,0.33364322781562805,0.3737293779850006,-0.23938417434692383,0.10984016954898834,-0.2699049413204193,-0.2417181134223938,0.004205930046737194,-0.26375389099121094,0.02125312015414238,-0.27677956223487854,-0.030595948919653893,-0.319995254278183,-0.25351616740226746,0.16866204142570496,0.36940115690231323,-0.2742481827735901,-0.2660347521305084,0.31827566027641296,-0.26528313755989075,0.2862165868282318,0.16192975640296936,-0.3322964012622833,-0.1999644935131073,0.3594881296157837,-0.26958227157592773,-0.27469614148139954,0.06869583576917648,0.27594882249832153,-0.2644203305244446,-0.26665106415748596,-0.2643812298774719,0.2841171622276306,0.3084688186645508,-0.26921048760414124,0.18852420151233673,0.024756556376814842,-0.17515681684017181,0.08360347151756287,-0.21419669687747955,-0.24890337884426117,-0.2626706063747406,0.0061029596254229546,0.25817304849624634,-0.36758729815483093,0.3550346791744232,-0.2791004776954651,0.13539141416549683,0.22952525317668915,0.3324577510356903,-0.21734438836574554,0.08421477675437927,-0.28303584456443787,-0.269867867231369,-0.21734438836574554,-0.24528969824314117,-0.29092660546302795,-0.2455272078514099,-0.162961944937706,-0.23876383900642395,-0.3163065016269684,0.16652752459049225,0.30984875559806824,0.3334428369998932,-0.27091607451438904,-0.290601909160614,-0.30574601888656616,0.30549347400665283,-0.10086511075496674,0.2270439863204956,0.2878851294517517,-0.302959144115448,0.36947834491729736,-0.04240000620484352,-0.28608331084251404,-0.19473940134048462,-0.3528740704059601,-0.26396334171295166,0.3601225018501282,0.3597436845302582,0.3124372661113739,-0.11890942603349686,-0.3300235867500305,-0.3712100088596344,0.2636927664279938,-0.21734438836574554,-0.30108413100242615,-0.15171605348587036,-0.29679304361343384,-0.2417181134223938,-0.21506354212760925,0.14226700365543365,-0.16642628610134125,-0.27756020426750183,0.132624551653862,-0.24440883100032806,0.15941140055656433,0.2985437214374542,0.21401332318782806,0.029730116948485374,-0.2428034245967865,-0.23878589272499084,0.3024430572986603,-0.34881049394607544,-0.2830268144607544,-0.2696554362773895,-0.32325318455696106,0.3077222406864166,0.3490529954433441,-0.23777174949645996,-0.13681241869926453,-0.2626706063747406,-0.2685651183128357,0.35085880756378174,0.3360423445701599,-0.266110897064209,-0.23909656703472137,-0.37315112352371216,-0.3212689757347107,0.337564617395401,-0.26255834102630615,0.36201149225234985,-0.20894554257392883,-0.2181490957736969,-0.21561269462108612,-0.3086371421813965,0.23455491662025452,-0.2623489201068878,0.32473334670066833,-0.2654384672641754,0.25089961290359497,-0.33486485481262207,-0.21734438836574554,-0.04511977732181549,-0.348391592502594,0.1412578672170639,0.24658043682575226,-0.3596835136413574,-0.2620603144168854,-0.018634555861353874,0.18543565273284912,0.2971157431602478,-0.20663464069366455,0.3355422914028168,-0.2970886826515198,-0.26109224557876587,0.2927895486354828,0.3577624261379242,0.327877014875412,0.32020270824432373,-0.2781544625759125,-0.23819765448570251,-0.29271936416625977,0.01643889956176281,-0.057122260332107544,0.30457958579063416,-0.2417181134223938,-0.24612247943878174,-0.2190389782190323,-0.2754647433757782,0.24658043682575226,0.10517025738954544,-0.2805776298046112,0.25089260935783386,-0.2695191502571106,0.12385492771863937,-0.35086381435394287,-0.31350651383399963,-0.35515695810317993,0.07955989241600037,-0.2711736559867859,-0.21734438836574554,-0.3412213623523712,0.04982477426528931,0.269925981760025,0.06597104668617249,0.011638709343969822,0.31855425238609314,0.35894298553466797,-0.23759737610816956,-0.2987311780452728,-0.1643027812242508,-0.22548766434192657,0.1827349066734314,-0.1077016294002533,-0.24435971677303314,-0.19410304725170135,0.2112704962491989,0.2770230770111084,-0.2439989596605301,0.3458421230316162,0.3157693147659302,-0.3218502104282379,-0.20039339363574982,-0.14418406784534454,0.3594881296157837,-0.2631746530532837,-0.26893994212150574,-0.27129489183425903,-0.24612247943878174,-0.25601649284362793,0.0009570321417413652,0.15741315484046936,0.3001611828804016,-0.32026585936546326,-0.26938486099243164,-0.32541781663894653,-0.33245477080345154,-0.2312198430299759,-0.28548404574394226,0.016858790069818497,-0.34122434258461,-0.2690771818161011,0.3064645826816559,-0.2853367328643799,-0.24459223449230194,-0.26268163323402405,-0.33294379711151123,-0.3465336561203003,-0.2620151937007904,0.05923474580049515,0.16608455777168274,0.24658043682575226,-0.3519170582294464,-0.267291396856308,-0.2744976878166199,-0.23516219854354858,-0.2948428988456726,-0.30857306718826294,-0.28665149211883545,-0.2732650935649872,-0.08896184712648392,0.18525926768779755,0.31356266140937805,-0.23998643457889557,0.3774031400680542,-0.16405726969242096,-0.30029547214508057,-0.3350973427295685,0.3017365634441376,0.3418797254562378,-0.26018330454826355,0.31873565912246704,0.32519033551216125,-0.3453912138938904,0.3526485860347748,-0.2477419078350067,-0.13568803668022156,-0.22718927264213562,0.25665083527565,-0.2558220624923706,-0.05309370532631874,-0.2380543351173401,0.30002087354660034,0.20889344811439514,0.24921202659606934,-0.26987090706825256,0.20863792300224304,-0.2850731611251831,-0.3169918954372406,0.34234073758125305,-0.1573520302772522,-0.23712235689163208,-0.31597575545310974,-0.25521978735923767,0.29221129417419434,0.22172166407108307,0.2802729606628418,-0.1294558048248291,-0.26277783513069153,-0.21734438836574554,0.1833091378211975,0.1549479365348816,-0.03348508104681969,-0.16956494748592377,0.3333105444908142,-0.1448424756526947,-0.22912238538265228,0.06791818141937256,-0.23367103934288025,-0.24477159976959229,0.18966762721538544,-0.32706430554389954,0.16798360645771027,-0.2629541754722595,0.32990434765815735,-0.26968950033187866,-0.30825337767601013,-0.26931774616241455,-0.24113088846206665,-0.0820932686328888,-0.1393999308347702,-0.36009544134140015,-0.24238254129886627,0.3780815899372101,-0.269227534532547,0.3591533899307251,-0.2399553805589676,-0.2653392553329468,-0.2803310751914978,0.3422495126724243,-0.29207098484039307,-0.26600968837738037,0.28145647048950195,0.03403425216674805,-0.2455201894044876,-0.24612247943878174,0.01029585674405098,0.08780038356781006,-0.2711505889892578,-0.23388950526714325,-0.3124362528324127,-0.2183966189622879,-0.1304749697446823,0.24658043682575226,-0.33612555265426636,-0.32538172602653503,0.3126116096973419,-0.048561085015535355,-0.2453678697347641,-0.28858762979507446,0.25641030073165894,-0.29639220237731934,-0.35404857993125916,-0.3361796736717224,-0.21734438836574554,-0.09476817399263382,-0.2659796476364136,0.348379522562027,-0.2553199827671051,-0.2637839615345001,-0.052385199815034866,0.32627561688423157,-0.17246711254119873,0.271909236907959,0.375829815864563,-0.19594696164131165,-0.3726300001144409,0.3239727318286896,0.2857014834880829,-0.33185648918151855,0.21356134116649628,-0.14652003347873688,-0.29539307951927185,-0.2778387665748596,-0.28824490308761597,-0.32084107398986816,-0.2806968688964844,-0.2883290648460388,0.3182586431503296,-0.24884626269340515,-0.30713197588920593,-0.29313725233078003,-0.304473340511322,-0.28871288895606995,-0.24827805161476135,0.2911781370639801,-0.2624911963939667,-0.25354623794555664,-0.04395630955696106,0.30812710523605347,0.006970801390707493,0.02678586356341839,0.07003267109394073,-0.1070893257856369,-0.30184173583984375,0.2554282546043396,-0.2302858531475067,0.14067964255809784,0.24658043682575226,0.15990445017814636,-0.2551456093788147,-0.09279397875070572,-0.348290354013443,-0.333457887172699,0.16481487452983856,-0.25251707434654236,-0.24826200306415558,-0.2417181134223938,0.32092827558517456,0.0034212640020996332,-0.27834686636924744,-0.06001740321516991,0.3180040419101715,-0.29237866401672363,-0.07441697269678116,-0.2432834357023239,-0.3213110864162445,0.19195449352264404,-0.0745091661810875,-0.26168450713157654,-0.3308142423629761,0.0011514467187225819,-0.27724453806877136,-0.23802727460861206,-0.26395633816719055,-0.31827566027641296,-0.26658791303634644,-0.24298781156539917,-0.1786321997642517,-0.2654154300689697,-0.08907008171081543,-0.2093193531036377,0.2889794707298279,0.31209051609039307,-0.23733481764793396,-0.23289740085601807,-0.2717859447002411,-0.27873870730400085,-0.23876886069774628,0.15033112466335297,-0.2773808240890503,0.34148889780044556,-0.23175397515296936,0.10867470502853394,0.3404637277126312,-0.3102074861526489,0.3594881296157837,-0.23612023890018463,-0.014619031921029091,-0.25119826197624207,-0.15027600526809692,0.3436124324798584,0.36663633584976196,0.3199572265148163,-0.015845635905861855,0.3020211160182953,-0.30271559953689575,0.3142591416835785,-0.26450350880622864,-0.2569745182991028,0.3271454870700836,-0.3308423161506653,-0.24966999888420105,-0.24120904505252838,0.22437329590320587,-0.3224334418773651,-0.25911906361579895,0.31869855523109436,-0.29106590151786804,-0.2379371076822281,0.07912095636129379,0.06909167766571045,-0.33793434500694275,0.29795753955841064,-0.31194519996643066,-0.29300999641418457,0.24658043682575226,-0.3286046087741852,-0.09101420640945435,-0.26669415831565857,-0.3012906014919281,-0.2799813449382782,0.003501434810459614,-0.24437876045703888,-0.23928596079349518,-0.2970215380191803,-0.30617794394493103,-0.1034325659275055,-0.11768782883882523,0.33381766080856323,0.3410349488258362,-0.019796019420027733,-0.3153734803199768,0.11007767170667648,-0.19950149953365326,-0.23807337880134583,-0.2778688967227936,-0.23934108018875122,-0.2851102352142334,-0.35122960805892944,-0.20373648405075073,0.01340345200151205,0.122075155377388,-0.2451854795217514,-0.1667710244655609,-0.011625676415860653,-0.26535728573799133,-0.2405877262353897,-0.28447288274765015,0.3156370222568512,-0.25462350249290466,-0.24255891144275665,0.050980228930711746,0.15315712988376617,0.2647079527378082,-0.0688701942563057,-0.3423868417739868,0.3280854821205139,0.30334195494651794,-0.27074572443962097,-0.24010469019412994,-0.26403748989105225,-0.024750540032982826,0.07163206487894058,0.31827566027641296,0.3338196277618408,0.2835519015789032,0.36598190665245056,0.24658043682575226,-0.2956165373325348,0.13280993700027466,-0.26987889409065247,0.039796486496925354,0.27977994084358215,-0.2603406310081482,-0.06322520971298218,-0.0026716680731624365,-0.31135493516921997,0.046120911836624146,-0.2530241310596466,-0.21734438836574554,-0.14030884206295013,-0.20763877034187317,-0.2963791489601135,-0.16388489305973053,0.11888737231492996,0.35507476329803467,0.2810215651988983,0.1965412199497223,-0.3247814178466797,0.2825598418712616,-0.27334025502204895,0.1044156551361084,-0.24676784873008728,0.37723883986473083,0.3482101857662201,0.25049877166748047,0.3283941149711609,0.2589988112449646,0.27156949043273926,0.2645295262336731,-0.3611927628517151,0.3787490129470825,0.3694041669368744,-0.26684144139289856,0.0697290226817131,-0.26837170124053955,0.2740668058395386,-0.2417181134223938,-0.28993749618530273,0.24658043682575226,-0.2633851170539856,-0.31330108642578125,-0.24612247943878174,-0.2792748510837555,-0.26072847843170166,-0.2417181134223938,-0.023384639993309975,0.046083830296993256,-0.23994536697864532,-0.14010341465473175,-0.26269665360450745,-0.1122262254357338,-0.3008827269077301,-0.029290175065398216,0.14487655460834503,-0.23480845987796783,-0.2388981282711029,-0.34916219115257263,0.29755863547325134,-0.267229288816452,-0.3253386616706848,0.3083235025405884,-0.24459722638130188,-0.2931252419948578,-0.23721155524253845,-0.042692627757787704,-0.2990317940711975,-0.16185759007930756,0.2833985686302185,-0.24504518508911133,-0.3121556341648102,0.2884032428264618,-0.3133101165294647,0.18226391077041626,-0.0855024978518486,0.2043568193912506,-0.3096994161605835,0.02994556725025177,-0.3535996377468109,0.3200083374977112,0.2702256441116333,-0.26439425349235535,0.22318878769874573,-0.26458364725112915,-0.26252126693725586,-0.21734438836574554,-0.24527065455913544,0.05431530252099037,-0.24108076095581055,-0.3471539616584778,-0.2787367105484009,-0.35081368684768677,-0.27290430665016174,-0.23453287780284882,-0.27701807022094727,-0.25971734523773193,-0.2498113065958023,0.3114301264286041,0.3372960090637207,-0.2633390426635742,0.11080321669578552,-0.2237810492515564,-0.31340330839157104,-0.2448938637971878,-0.2384040802717209,-0.2266741842031479,-0.2909947335720062,-0.24046145379543304,-0.09669426083564758,0.2837613821029663,-0.08187680691480637,-0.26513683795928955,-0.26087477803230286,-0.26504963636398315,0.3052810728549957,0.25632214546203613,-0.2399483621120453,-0.3109099864959717,0.24658043682575226,-0.3162924647331238,0.29341989755630493,-0.24268116056919098,0.31152433156967163,-0.3218432366847992,-0.22648879885673523,0.29425567388534546,-0.3599450886249542,0.3733084499835968,0.10824478417634964,-0.24612247943878174,0.35949912667274475,-0.28463825583457947,0.3054504096508026,-0.2529178857803345,-0.09574525058269501,0.11294977366924286,0.31829965114593506,0.05221784859895706,0.3594881296157837,0.28589487075805664,0.15975211560726166,0.3463742733001709,0.3731461465358734,-0.2649093568325043,-0.3065817952156067,-0.261505126953125,0.15369126200675964,-0.23816457390785217,0.3594881296157837,-0.269505113363266,-0.08559369295835495,-0.2835719883441925,-0.08624307811260223,-0.11440586298704147,-0.31356462836265564,-0.24800245463848114,-0.2790243327617645,-0.28129616379737854,0.011440289206802845,0.33123916387557983,-0.2621374726295471,-0.18727654218673706,-0.07643525302410126,-0.26345527172088623,-0.26265957951545715,0.1878477782011032,0.35758909583091736,0.32286638021469116,-0.35108131170272827,0.16649143397808075,-0.21458353102207184,0.2227959781885147,0.24683399498462677,-0.11553626507520676,-0.2431451380252838,-0.2591291069984436,-0.24279239773750305,-0.2804242968559265,-0.23894423246383667,-0.08597751706838608,0.37486475706100464,-0.22781962156295776,0.18931087851524353,-0.2640344798564911,0.0417165607213974,0.12744152545928955,-0.24491189420223236,0.3251853287220001,-0.18922066688537598,0.3157743513584137,0.34540024399757385,0.17220155894756317,-0.31347545981407166,-0.2364349067211151,-0.2623559236526489,-0.26084771752357483,-0.06228521838784218,0.300409734249115,-0.23829986155033112,-0.3136097192764282,0.136325404047966,-0.2851603627204895,-0.2400195151567459,-0.28893837332725525,-0.28012263774871826,-0.32519835233688354,0.31956836581230164,0.3504469394683838,-0.25280869007110596,0.3735189437866211,-0.2417181134223938,-0.2417181134223938,0.3330910801887512,0.353519469499588,-0.17274871468544006,0.31314072012901306,0.2876375913619995,0.17326480150222778,-0.18478025496006012,-0.2392178177833557,0.23485957086086273,-0.27458491921424866,-0.27382224798202515,-0.24872499704360962,-0.14800116419792175,0.32844820618629456,-0.3536938428878784,-0.039690252393484116,0.3778671324253082,-0.18970870971679688,-0.354265034198761,-0.12063609063625336,0.3572523891925812,0.30238690972328186,-0.31788280606269836,-0.2712177336215973,-0.30283185839653015,-0.21734438836574554,-0.14659619331359863,-0.11375147104263306,-0.2657000422477722,0.26568201184272766,0.32242849469184875,-0.23798519372940063,-0.23121081292629242,-0.2448006570339203,-0.14134906232357025,0.07634607702493668,0.20656049251556396,-0.21734438836574554,-0.26942893862724304,0.3594881296157837,-0.322950541973114,-0.2417181134223938,-0.3631899952888489,0.3335881531238556,0.2912081778049469,-0.34848374128341675,-0.1015155017375946,-0.262656569480896,-0.2694529891014099,-0.23791004717350006,0.3204823136329651,-0.3470958471298218,-0.25645437836647034,-0.254159539937973,-0.26417678594589233,-0.3192707896232605,-0.26327189803123474,-0.3482613265514374,0.3626718819141388,0.31578436493873596,-0.04207531362771988,-0.31634750962257385,-0.2949741780757904,0.0459565594792366,-0.26338014006614685,-0.2624591588973999,-0.2431020587682724,-0.24048550426959991,-0.25959405303001404,-0.32061460614204407,-0.32838812470436096,-0.1786983460187912,-0.242898628115654,0.31182795763015747,-0.2435389906167984,-0.10806139558553696,-0.13066236674785614,0.2541314959526062,0.2613217234611511,-0.30920740962028503,0.324801504611969,-0.3278990387916565,-0.25301408767700195,-0.31556084752082825,0.3736070990562439,-0.24272124469280243,-0.3748096823692322,0.37961286306381226,-0.2609509825706482,-0.21734438836574554,0.37860268354415894,-0.20197375118732452,0.35741373896598816,-0.23326517641544342,-0.23868468403816223,-0.047333478927612305,0.2918405532836914,-0.29315632581710815,0.24658043682575226,-0.07116205990314484,0.17114631831645966,0.31272587180137634,0.24658043682575226,-0.3270091712474823,0.13621817529201508,0.25151291489601135,-0.21734438836574554,-0.24253785610198975,-0.036199845373630524,0.101909339427948,0.3594881296157837,0.32486462593078613,0.26566097140312195,-0.279207706451416,-0.27161258459091187,0.22031669318675995,0.2744726836681366,-0.2691253125667572,-0.3143773376941681,0.3183096945285797,-0.29323145747184753,-0.2965465188026428,0.24658043682575226,-0.35388222336769104,-0.297945499420166,0.3034552037715912,0.28326234221458435,0.2448066771030426,0.29374659061431885,-0.19731184840202332,-0.2756040394306183,-0.3225817382335663,-0.23579354584217072,0.2944771647453308,-0.2869892120361328,0.2512704133987427,-0.2800605297088623,-0.3673357665538788,-0.3410569727420807,0.23791806399822235,-0.31864240765571594,-0.26435816287994385,-0.2991119623184204,-0.3210555613040924,0.15844033658504486,-0.25918519496917725,-0.2922563850879669,-0.3220787048339844,0.034867022186517715,-0.26320573687553406,-0.21734438836574554,-0.35680142045021057,0.03611766919493675,-0.2071988433599472,0.28590694069862366,-0.26928967237472534,0.041240546852350235,-0.32860955595970154,-0.18850015103816986,-0.001611419953405857,-0.27297645807266235,0.040975987911224365,0.3512346148490906,0.2837704122066498,-0.29917508363723755,-0.010228711180388927,0.059462226927280426,-0.10055144876241684,0.3594881296157837,0.1922270655632019,-0.23895125091075897,-0.22787773609161377,-0.250622034072876,-0.34617185592651367,-0.022651081904768944,-0.21514572203159332,-0.32216089963912964,-0.24163895845413208,-0.2467668354511261,-0.2534099221229553,0.3303161859512329,-0.1428612768650055,0.2870393395423889,-0.2677583694458008,0.35330402851104736,0.3716379404067993,-0.2691042423248291,-0.23889712989330292,-0.028012461960315704,0.28200265765190125,0.1039997786283493,-0.21734438836574554,-0.11688913404941559,0.108597531914711,-0.12123435735702515],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0e273a2e-deec-4211-8115-3e9d59d702eb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GAihIQ-8wFtN",
        "mYbsfYC35CZt",
        "GVGdMPy4DZBx",
        "pC7Q5LmzlY4g"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}