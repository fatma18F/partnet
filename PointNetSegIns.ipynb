{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatma18F/partnet/blob/main/PointNetSegIns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuEIVQsJGUbQ",
        "outputId": "bf24af93-dfc9-44ef-945a-7fe7edfb2df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pat = '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'\n",
        "!git clone https://{pat}@github.com/fatma18F/partnet.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG2FbKpuqD7O",
        "outputId": "a6dc7884-aac1-4f1c-d662-a71ab13f2331"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'partnet'...\n",
            "fatal: could not read Password for 'https://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@github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLGjzwFkGUbV",
        "outputId": "5b7e58e9-e92f-484e-bdef-7f50bbd7a4c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting path.py\n",
            "  Downloading path.py-12.5.0-py3-none-any.whl (2.3 kB)\n",
            "Collecting path\n",
            "  Downloading path-16.6.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: path, path.py\n",
            "Successfully installed path-16.6.0 path.py-12.5.0\n"
          ]
        }
      ],
      "source": [
        "root_dir = \"/content/gdrive/My Drive\"\n",
        "\n",
        "\n",
        "!pip install path.py;\n",
        "from path import Path\n",
        "import sys\n",
        "sys.path.append(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB7sxVUtGUbY"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import scipy.spatial.distance\n",
        "import math\n",
        "import random\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "#import utils\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data.dataset import random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAihIQ-8wFtN"
      },
      "source": [
        "## partnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VANXkwmrI3OY"
      },
      "outputs": [],
      "source": [
        "def load_json(fn):\n",
        "    with open(fn, 'r') as fin:\n",
        "        return json.load(fin)\n",
        "\n",
        "def load_h5(fn):\n",
        "    with h5py.File(fn, 'r') as fin:\n",
        "        pts = fin['pts'][:]\n",
        "        gt_label = fin['gt_label'][:]\n",
        "        gt_mask = fin['gt_mask'][:]\n",
        "        gt_valid = fin['gt_valid'][:]\n",
        "        gt_other_mask = fin['gt_other_mask'][:]\n",
        "        return pts, gt_label, gt_mask, gt_valid, gt_other_mask\n",
        "\n",
        "def load_data(fn):\n",
        "    cur_json_fn = fn.replace('.h5', '.json')\n",
        "    record = load_json(cur_json_fn)\n",
        "    pts, gt_label, gt_mask, gt_valid, gt_other_mask = load_h5(fn)\n",
        "    return pts, gt_label, gt_mask, gt_valid, gt_other_mask, record "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EF5Z6-BJNjx",
        "outputId": "f1666602-9a01-45c0-9c9c-195ab578f0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading data from  /content/gdrive/MyDrive/ins_seg_h5_for_detection/Chair-1/train-00.h5\n"
          ]
        }
      ],
      "source": [
        "path='/content/gdrive/MyDrive/ins_seg_h5_for_detection/Chair-1/train-00.h5'\n",
        "cur_h5_fn = os.path.join(path)\n",
        "print('Reading data from ', cur_h5_fn)\n",
        "pts, gt_label, gt_mask, gt_valid, gt_other_mask, record = load_data(cur_h5_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhJZwMNUCySJ",
        "outputId": "80661f4c-1a7b-4206-d131-a919b4f00dce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3213"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x=gt_mask[3][0]\n",
        "np.count_nonzero(x == True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZzPMWCe-4UN",
        "outputId": "2928fb1c-c19d-4d9e-8b05-ae90a769bf20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train List:  ['train-03.h5', 'train-01.h5', 'train-04.h5', 'train-00.h5', 'train-02.h5']\n",
            "val List:  ['val-00.h5']\n"
          ]
        }
      ],
      "source": [
        "category='Chair'\n",
        "level_id=1\n",
        "data_in_dir = f'{root_dir}/ins_seg_h5_for_detection/%s-%d/' % (category, level_id)\n",
        "train_h5_fn_list = []\n",
        "for item in os.listdir(data_in_dir):\n",
        "    if item.endswith('.h5') and item.startswith('train-'):\n",
        "        train_h5_fn_list.append(item)\n",
        "val_h5_fn_list = []\n",
        "for item in os.listdir(data_in_dir):\n",
        "    if item.endswith('.h5') and item.startswith('val-'):\n",
        "        val_h5_fn_list.append(item)\n",
        "print('train List: ', train_h5_fn_list)\n",
        "print('val List: ', val_h5_fn_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "laAdCJWzH1Vg",
        "outputId": "57edc4e7-865a-45ce-b286-b0ce321b0819"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pts_total=[]\\nfor item in train_h5_fn_list:\\n         path=\\'/content/gdrive/MyDrive/ins_seg_h5_for_detection/Chair-1/train-00.h5\\'\\n         cur_h5_fn = os.path.join(path)\\n         pts, gt_label, gt_mask, gt_valid, gt_other_mask, _ = load_data(cur_h5_fn)\\n         i=0\\n         pts_total=pts\\n         gt_label_total=gt_label\\n         for item in train_h5_fn_list:\\n            if i>0 : \\n                cur_h5_fn = os.path.join(data_in_dir, item)\\n         \\n                pts, gt_label, gt_mask, gt_valid, gt_other_mask, _ = load_data(cur_h5_fn)\\n                #print(\"shape of pts\" ,i, \":\" ,pts.shape)\\n                pts_total=np.concatenate((pts_total, pts))\\n                gt_label_total=np.concatenate((gt_label_total, gt_label))\\n            i+=1\\nprint(\"shape of pts: \" ,pts_total.shape)\\nprint(\"shape of gt_label: \" ,gt_label_total.shape)'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''pts_total=[]\n",
        "for item in train_h5_fn_list:\n",
        "         path='/content/gdrive/MyDrive/ins_seg_h5_for_detection/Chair-1/train-00.h5'\n",
        "         cur_h5_fn = os.path.join(path)\n",
        "         pts, gt_label, gt_mask, gt_valid, gt_other_mask, _ = load_data(cur_h5_fn)\n",
        "         i=0\n",
        "         pts_total=pts\n",
        "         gt_label_total=gt_label\n",
        "         for item in train_h5_fn_list:\n",
        "            if i>0 : \n",
        "                cur_h5_fn = os.path.join(data_in_dir, item)\n",
        "         \n",
        "                pts, gt_label, gt_mask, gt_valid, gt_other_mask, _ = load_data(cur_h5_fn)\n",
        "                #print(\"shape of pts\" ,i, \":\" ,pts.shape)\n",
        "                pts_total=np.concatenate((pts_total, pts))\n",
        "                gt_label_total=np.concatenate((gt_label_total, gt_label))\n",
        "            i+=1\n",
        "print(\"shape of pts: \" ,pts_total.shape)\n",
        "print(\"shape of gt_label: \" ,gt_label_total.shape)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9iyuMbOx8EUD",
        "outputId": "446156b6-8555-4e0d-94cc-c6dbf5f0bb35"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"path='/kaggle/input/partnet/partnet_seg_exps-master/exps/utils/provider.py'\\nprovider_FILE = os.path.join(path)\\nos.system('cp %s %s' % ( provider_FILE, '/kaggle/working/')) # bkp of model def\\nimport provider\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''path='/kaggle/input/partnet/partnet_seg_exps-master/exps/utils/provider.py'\n",
        "provider_FILE = os.path.join(path)\n",
        "os.system('cp %s %s' % ( provider_FILE, '/kaggle/working/')) # bkp of model def\n",
        "import provider'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4HzGKUOSl7"
      },
      "outputs": [],
      "source": [
        "path='gdrive/MyDrive/ins_seg_h5_for_detection/provider.py'\n",
        "provider_FILE = os.path.join(path)\n",
        "os.system('cp %s %s' % ( provider_FILE, '.')) # bkp of model def\n",
        "import provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaYJplQBM0L9"
      },
      "outputs": [],
      "source": [
        "# shuffle data order\n",
        "n_shape = pts.shape[0]\n",
        "idx = np.arange(n_shape)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "pts = pts[idx, ...]\n",
        "gt_label = gt_label[idx, ...]\n",
        "gt_mask = gt_mask[idx, ...]\n",
        "gt_valid = gt_valid[idx, ...]\n",
        "gt_other_mask = gt_other_mask[idx, ...]\n",
        "\n",
        "# data augmentation to pts\n",
        "pts = provider.jitter_point_cloud(pts)\n",
        "pts = provider.shift_point_cloud(pts)\n",
        "pts = provider.random_scale_point_cloud(pts)\n",
        "pts = provider.rotate_perturbation_point_cloud(pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRwLowhQCIRi",
        "outputId": "4d8bf445-fac5-4d7f-daee-4662bfdadb86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1024, 10000, 3)\n",
            "(1024, 10000)\n",
            "(1024, 200, 10000)\n",
            "(1024, 200)\n"
          ]
        }
      ],
      "source": [
        "print(pts.shape)\n",
        "print(gt_label.shape)\n",
        "print(gt_mask.shape)\n",
        "print(gt_valid.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8BVA-MtSkPT"
      },
      "outputs": [],
      "source": [
        "class Data(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, pts,gt_label,gt_mask,gt_valid, transform=None):\n",
        "        \n",
        "        self.pts = pts\n",
        "        self.gt_label = gt_label\n",
        "        self.gt_mask = gt_mask\n",
        "        self.gt_valid=gt_valid\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        return pts.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #if not self.valid:\n",
        "          #  theta = random.random()*360\n",
        "         #   image2 = utils.RandRotation_z()(utils.RandomNoise()(image2))\n",
        "        \n",
        "        return {'image': np.array(pts[idx], dtype=\"float32\"), 'category': gt_label[idx].astype(int) , 'masks':gt_mask[idx], 'valid':np.array(gt_valid[idx])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TZCgrjql_Ri",
        "outputId": "2e020555-f6c5-43e3-e47d-a9d3abc71c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######### Dataset class created #########\n",
            "Number of images:  1024\n",
            "Sample image shape:  (10000, 3)\n"
          ]
        }
      ],
      "source": [
        "dset = Data(pts , gt_label, gt_mask, gt_valid)\n",
        "train_num = int(len(dset) * 0.95)\n",
        "val_num = int(len(dset) *0.05)\n",
        "if int(len(dset)) - train_num -  val_num >0 :\n",
        "    train_num = train_num + 1\n",
        "elif int(len(dset)) - train_num -  val_num < 0:\n",
        "    train_num = train_num -1\n",
        "#train_dataset, val_dataset = random_split(dset, [3000, 118])\n",
        "train_dataset, val_dataset = random_split(dset, [train_num, val_num])\n",
        "val_dataset.valid=True\n",
        "\n",
        "print('######### Dataset class created #########')\n",
        "print('Number of images: ', len(dset))\n",
        "print('Sample image shape: ', dset[0]['image'].shape)\n",
        "#print('Sample image points categories', dset[0]['category'], end='\\n\\n')\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=4,drop_last = True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=4,drop_last = True)\n",
        "\n",
        "#dataloader = torch.utils.data.DataLoader(dset, batch_size=4, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUqnmRIP-xl9",
        "outputId": "81037a86-d236-4e4e-f2c7-d2d51ed9f9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading from  /content/gdrive/MyDrive/ins_seg_h5_for_detection/Chair-1/chair-level1.txt\n",
            "Part Name List:  ['chair/chair_head', 'chair/chair_back', 'chair/chair_arm', 'chair/chair_base', 'chair/chair_seat']\n",
            "Semantic Labels:  5\n",
            "Number of Instances:  200\n"
          ]
        }
      ],
      "source": [
        "stat_in_fn = '/content/gdrive/MyDrive/ins_seg_h5_for_detection/Chair-1/chair-level1.txt'\n",
        "print('Reading from ', stat_in_fn)\n",
        "with open(stat_in_fn, 'r') as fin:\n",
        "    part_name_list = [item.rstrip().split()[1] for item in fin.readlines()]\n",
        "print('Part Name List: ', part_name_list)\n",
        "NUM_CLASSES = len(part_name_list)\n",
        "print('Semantic Labels: ', NUM_CLASSES)\n",
        "NUM_INS = 200\n",
        "print('Number of Instances: ', NUM_INS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KNH5UtbWWok"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlCGIGvV88f_",
        "outputId": "9876cd71-e3d8-4e93-9c98-79a98ce3148f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n=np.unique(gt_label.ravel()).size\n",
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOOrEYSnWV7f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Tnet(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "      super().__init__()\n",
        "      self.k=k\n",
        "      self.conv1 = nn.Conv1d(k,64,1)\n",
        "      self.conv2 = nn.Conv1d(64,128,1)\n",
        "      self.conv3 = nn.Conv1d(128,1024,1)\n",
        "      self.fc1 = nn.Linear(1024,512)\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      self.fc3 = nn.Linear(256,k*k)\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "       \n",
        "\n",
        "   def forward(self, input):\n",
        "      # input.shape == (bs,n,3)\n",
        "      bs = input.size(0)\n",
        "      xb = F.relu(self.bn1(self.conv1(input)))\n",
        "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "      flat = nn.Flatten(1)(pool)\n",
        "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "      \n",
        "      #initialize as identity\n",
        "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "      if xb.is_cuda:\n",
        "        init=init.cuda()\n",
        "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
        "      return matrix\n",
        "\n",
        "\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=128)\n",
        "        self.fc1 = nn.Conv1d(3,64,1)\n",
        "        self.fc2 = nn.Conv1d(64,128,1) \n",
        "        self.fc3 = nn.Conv1d(128,128,1)\n",
        "        self.fc4 = nn.Conv1d(128,512,1)\n",
        "        self.fc5 = nn.Conv1d(512,2048,1)\n",
        "\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(2048)\n",
        "\n",
        "   def forward(self, input):\n",
        "        n_pts = input.size()[2]\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
        "        outs = []\n",
        "        \n",
        "        out1 = F.relu(self.bn1(self.fc1(xb)))\n",
        "        outs.append(out1)\n",
        "        out2 = F.relu(self.bn2(self.fc2(out1)))\n",
        "        outs.append(out2)\n",
        "        out3 = F.relu(self.bn3(self.fc3(out2)))\n",
        "        outs.append(out3)\n",
        "        matrix128x128 = self.feature_transform(out3)\n",
        "        \n",
        "        out4 = torch.bmm(torch.transpose(out3,1,2), matrix128x128).transpose(1,2) \n",
        "        outs.append(out4)\n",
        "        out5 = F.relu(self.bn4(self.fc4(out4)))\n",
        "        outs.append(out5)\n",
        "       \n",
        "        xb = self.bn5(self.fc5(out5))\n",
        "        \n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        out6 = nn.Flatten(1)(xb).repeat(n_pts,1,1).transpose(0,2).transpose(0,1)#.repeat(1, 1, n_pts)\n",
        "        outs.append(out6)\n",
        "        \n",
        "        \n",
        "        return outs, matrix3x3, matrix128x128\n",
        "\n",
        "\n",
        "class PointNetSeg(nn.Module):\n",
        "    def __init__(self, classes = 6):\n",
        "        super().__init__()\n",
        "        self.transform = Transform()\n",
        "\n",
        "        self.fc1 = nn.Conv1d(3008,256,1) \n",
        "        self.fc2 = nn.Conv1d(256,256,1) \n",
        "        self.fc3 = nn.Conv1d(256,128,1) \n",
        "        self.fc4 = nn.Conv1d(128,6,1) \n",
        "        self.fc4_ins = nn.Conv1d(128,200,1) \n",
        "        \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        \n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.bn4 = nn.BatchNorm1d(6)\n",
        "        self.bn4_ins = nn.BatchNorm1d(200)\n",
        "        self.softmax=nn.Softmax(dim=1)\n",
        "        #self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        inputs, matrix3x3, matrix128x128 = self.transform(input)\n",
        "        stack = torch.cat(inputs,1)\n",
        "        \n",
        "        xb = F.relu(self.bn1(self.fc1(stack)))\n",
        "       \n",
        "        xb = F.relu(self.bn2(self.fc2(xb)))\n",
        "    \n",
        "        xb = F.relu(self.bn3(self.fc3(xb)))\n",
        "        \n",
        "        output = F.relu(self.bn4(self.fc4(xb)))\n",
        "        masks_output = F.relu(self.bn4_ins(self.fc4_ins(xb)))\n",
        "\n",
        "        #masks_output=torch.round(masks_output)\n",
        "        #masks_output= masks_output > 0\n",
        "        \n",
        "        masks_output=self.softmax(masks_output)\n",
        "        end_points = {}\n",
        "\n",
        "        #return self.logsoftmax(output), matrix3x3, matrix128x128\n",
        "        return output,masks_output,end_points, matrix3x3, matrix128x128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqym7KLcuDOG"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK_H7SBf9ri7",
        "outputId": "c295d3da-2a8b-49bd-92a4-d87dd39492bb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nm = nn.LogSoftmax(dim=1)\\ncriterion2 = nn.NLLLoss()\\nx = torch.randn(1, 5)\\ny = torch.empty(1, dtype=torch.long).random_(5)\\nloss2 = criterion2(m(x), y)\\nprint(loss2)'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#from partnet github\n",
        "#def get_seg_loss(seg_pred, seg_gt, end_points):\n",
        " #   per_point_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seg_pred, labels=seg_gt)\n",
        "#in pytorch :\n",
        "#import torch.nn.functional as F\n",
        "#loss = F.nll_loss(F.softmax(input), target)\n",
        "'''\n",
        "m = nn.LogSoftmax(dim=1)\n",
        "criterion2 = nn.NLLLoss()\n",
        "x = torch.randn(1, 5)\n",
        "y = torch.empty(1, dtype=torch.long).random_(5)\n",
        "loss2 = criterion2(m(x), y)\n",
        "print(loss2)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0jCFwCCg3Hp"
      },
      "source": [
        " A regularization loss (with weight 0.001) is added to the softmax  loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDb9rPb_wPWj"
      },
      "outputs": [],
      "source": [
        "def get_seg_loss (outputs, labels, m3x3, m128x128, end_points, alpha = 0.0001): \n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs=outputs.size(0)\n",
        "    #id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n",
        "    #id128x128 = torch.eye(128, requires_grad=True).repeat(bs,1,1)\n",
        "    #if outputs.is_cuda:\n",
        "     #   id3x3=id3x3.cuda()\n",
        "     #   id128x128=id128x128.cuda()\n",
        "    #diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n",
        "    #diff128x128 = id128x128-torch.bmm(m128x128,m128x128.transpose(1,2))\n",
        "    logsoftmax = nn.LogSoftmax(dim=1)\n",
        "    per_point_loss= criterion(logsoftmax(outputs), labels) #+ alpha * (torch.norm(diff3x3)+torch.norm(diff128x128)) / float(bs)\n",
        "    end_points['per_point_seg_loss'] = per_point_loss\n",
        "    per_shape_loss = torch.mean(per_point_loss, -1)\n",
        "    end_points['per_shape_seg_loss'] = per_shape_loss\n",
        "   # loss = torch.mean(per_shape_loss)\n",
        "    return per_shape_loss, end_points "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdtMCeSTE8Ok",
        "outputId": "451cfe9a-2ab8-4351-dc17-aa6222c0a692"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"from progressbar import ProgressBar\\n\\ngt_label2 = np.zeros((4, 10000), dtype=np.uint8)\\ngt_mask2 = np.zeros((4, 200, 10000), dtype=bool)\\nbar = ProgressBar()\\nfor i in bar(range(1)):\\n        cur_label = labels[i, :10000].cpu()\\n        cur_record = record[i]\\n        cur_tot = 0\\n        j=0\\n        for item in cur_record['ins_seg']:\\n            if item['part_name'] in part_name_list:\\n                print(item['part_name'])\\n                selected = np.isin(cur_label, item['leaf_id_list'])\\n                gt_label2[i, selected] = part_name_list.index(item['part_name']) + 1\\n                gt_mask2[i, cur_tot, selected] = True\\n                #gt_valid[i, cur_tot] = True\\n                cur_tot += 1\\n                #print(cur_tot)\\n                #print(gt_label2)          \\n                #print(gt_mask2)\\n        gt_other_mask[i, :] = (gt_label[i, :] == 0)\\n\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from progressbar import ProgressBar\n",
        "\n",
        "gt_label2 = np.zeros((4, 10000), dtype=np.uint8)\n",
        "gt_mask2 = np.zeros((4, 200, 10000), dtype=bool)\n",
        "bar = ProgressBar()\n",
        "for i in bar(range(1)):\n",
        "        cur_label = labels[i, :10000].cpu()\n",
        "        cur_record = record[i]\n",
        "        cur_tot = 0\n",
        "        j=0\n",
        "        for item in cur_record['ins_seg']:\n",
        "            if item['part_name'] in part_name_list:\n",
        "                print(item['part_name'])\n",
        "                selected = np.isin(cur_label, item['leaf_id_list'])\n",
        "                gt_label2[i, selected] = part_name_list.index(item['part_name']) + 1\n",
        "                gt_mask2[i, cur_tot, selected] = True\n",
        "                #gt_valid[i, cur_tot] = True\n",
        "                cur_tot += 1\n",
        "                #print(cur_tot)\n",
        "                #print(gt_label2)          \n",
        "                #print(gt_mask2)\n",
        "        gt_other_mask[i, :] = (gt_label[i, :] == 0)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEca_LuVtOj5",
        "outputId": "29713d6d-7634-4586-82b3-b0b7bcd829aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "torch.Size([4, 6, 10000])\n",
            "torch.Size([4, 200, 10000])\n",
            "torch.Size([4, 200, 10000])\n",
            "torch.Size([4, 200])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([0.0024, 0.0016, 0.0044,  ..., 0.0143, 0.0014, 0.0029],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "pointnet = PointNetSeg()\n",
        "pointnet.to(device);\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "            inputs,labels,gt_mask_pl,gt_valid_pl = data['image'].to(device),data['category'].to(device),data['masks'].to(device),data['valid'].to(device)\n",
        "            outputs ,mask_pred,end_points, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n",
        "            break \n",
        "\n",
        "print(outputs.shape)\n",
        "print(mask_pred.shape)\n",
        "print(gt_mask_pl.shape)\n",
        "print(gt_valid_pl.shape)\n",
        "mask_pred[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0qC6SGE28Wv"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "def hungarian_matching(pred_x, gt_x, curnmasks):\n",
        "    \"\"\" pred_x, gt_x: B x nmask x n_point\n",
        "        curnmasks: B\n",
        "        return matching_idx: B x nmask x 2 \"\"\"\n",
        "\n",
        "    pred_x=pred_x.detach().cpu().numpy()\n",
        "    gt_x=gt_x.detach().cpu().numpy()\n",
        "    curnmasks=curnmasks.detach().cpu().numpy()\n",
        "    curnmasks=np.sum(curnmasks,axis=-1) \n",
        "       \n",
        "    batch_size = gt_x.shape[0]\n",
        "    nmask = gt_x.shape[1]\n",
        "    matching_score = np.matmul(gt_x, np.transpose(pred_x, axes=[0, 2, 1])) \n",
        "    matching_score = 1 - np.divide(matching_score, np.maximum(np.expand_dims(np.sum(pred_x, 2), 1)+np.sum(gt_x, 2, keepdims=True) - matching_score, 1e-8))\n",
        "    matching_idx = np.zeros((batch_size, nmask, 2)).astype('int32')\n",
        "    curnmasks = curnmasks.astype('int32')\n",
        "    print(matching_score.shape)\n",
        "    for i, curnmask in enumerate(curnmasks):\n",
        "        row_ind, col_ind = linear_sum_assignment(matching_score[i, :curnmask, :])\n",
        "        matching_idx[i, :curnmask, 0] = row_ind\n",
        "        matching_idx[i, :curnmask, 1] = col_ind\n",
        "    return matching_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S_bFmd8UQL0",
        "outputId": "41bb6210-ed93-4b95-8bbe-24fde30295ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 200, 10000])\n",
            "torch.Size([4, 200, 10000])\n",
            "torch.Size([4, 200])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(mask_pred.shape)\n",
        "print(gt_mask_pl.shape)\n",
        "print(gt_valid_pl.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek3HrXNa83SP"
      },
      "outputs": [],
      "source": [
        "def iou(pred_x, gt_x, gt_conf, n_point, nmask, end_points):\n",
        "\n",
        "    matching_idx = hungarian_matching(mask_pred, gt_mask_pl, gt_valid_pl)\n",
        "    matching_idx = torch.tensor(matching_idx)\n",
        "    \n",
        "    #matching_idx.requires_grad = True\n",
        "    end_points['matching_idx'] = matching_idx\n",
        "    matching_idx_row = matching_idx[:, :, 0]\n",
        "    #idx = tf.where(tf.greater_equal(n, 0))\n",
        "    idx=(matching_idx_row >= 0).nonzero(as_tuple=False)\n",
        "    matching_idx_row=torch.cat( (torch.unsqueeze(idx[:, 0].int(),dim=-1) , torch.reshape( matching_idx_row, (-1, 1)) ),1)\n",
        "    gt_x_matched = torch.reshape(gt_x[list(matching_idx_row.T.long())], (-1, nmask, n_point))\n",
        "    \n",
        "    matching_idx_column = matching_idx[:, :, 1]\n",
        "    matching_idx_column=torch.cat((torch.unsqueeze(idx[:, 0].int(),dim=-1 ) , torch.reshape( matching_idx_column, (-1, 1)) ),1)\n",
        "    pred_x_matched = torch.reshape(gt_x[list(matching_idx_column.T.long())], (-1, nmask, n_point))\n",
        "    \n",
        "    # compute meaniou\n",
        "    matching_score = torch.sum(torch.multiply(gt_x_matched, pred_x_matched),2)\n",
        "    iou_all = torch.div(matching_score, torch.sum(gt_x_matched, 2) + torch.sum(pred_x_matched, 2) - matching_score + 1e-8)\n",
        "    end_points['per_shape_all_iou'] = iou_all\n",
        "    meaniou = torch.div(torch.sum(torch.multiply(iou_all, gt_conf), 1), torch.sum(gt_conf, -1) + 1e-8) # B\n",
        "\n",
        "    return meaniou, end_points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO0m-mSgBErL"
      },
      "outputs": [],
      "source": [
        "def get_ins_loss(mask_pred, mask_gt, gt_valid, end_points):\n",
        "\n",
        "     \"\"\" Input:      mask_pred   B x K x N\n",
        "                     mask_gt     B x K x N\n",
        "                     gt_valid    B x K\n",
        "     \"\"\"\n",
        "     mask_gt=mask_gt.float()\n",
        "     gt_valid=gt_valid.float()\n",
        "     _,num_ins , num_point= mask_pred.shape\n",
        "     meaniou, end_points = iou( mask_pred, gt_mask_pl, gt_valid_pl, num_point, num_ins, end_points)\n",
        "     end_points['per_shape_mean_iou'] = meaniou\n",
        "     loss = - torch.mean(meaniou)\n",
        "     return loss, end_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg9RjG7awgVK"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq9AUuN5WRxI",
        "outputId": "a18dd09c-3342-4718-c317-fd7cbe7cd0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqXW9-oJwEPm"
      },
      "outputs": [],
      "source": [
        "pointnet = PointNetSeg()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mA80v2ywHhw"
      },
      "outputs": [],
      "source": [
        "pointnet.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV09EA4_wJnR"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgaPisZFwVzh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_loader, val_loader=None,  epochs=5, save=True):\n",
        "    for epoch in tqdm(range(epochs)): \n",
        "        pointnet.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs,labels,gt_mask_pl,gt_valid_pl = data['image'].to(device),data['category'].to(device),data['masks'].to(device),data['valid'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs ,mask_pred,end_points, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n",
        "            seg_loss ,end_points = get_seg_loss(outputs, labels, m3x3, m64x64,end_points)\n",
        "            ins_loss, end_points = get_ins_loss(mask_pred, gt_mask_pl, gt_valid_pl, end_points)\n",
        "            loss=ins_loss+seg_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                        (epoch + 1, i + 1, running_loss / 10))\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        pointnet.eval()\n",
        "        correct = total = 0\n",
        "        mcorrect = mtotal = 0\n",
        "\n",
        "        # validation\n",
        "        if val_loader:\n",
        "            with torch.no_grad():\n",
        "                for data in val_loader:\n",
        "                    inputs,labels,gt_mask_pl,gt_valid_pl = data['image'].to(device),data['category'].to(device),data['masks'].to(device),data['valid'].to(device)\n",
        "                    outputs ,mask_pred,end_points, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0) * labels.size(1) ##\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    _, mpredicted = torch.max(mask_pred.data, 1)\n",
        "                    mtotal += gt_mask_pl.size(0) * gt_mask_pl.size(1) ##\n",
        "                    mcorrect += (mpredicted == gt_mask_pl).sum().item()\n",
        "            val_acc = 100 * correct / total\n",
        "            mval_acc = 100 * mcorrect / mtotal\n",
        "            print('Valid accuracy: label %d %%' % val_acc)\n",
        "            print('Valid accuracy: mask %d %%' % mval_acc)\n",
        "\n",
        "        # save the model\n",
        "        if save:\n",
        "            torch.save(pointnet.state_dict(), root_dir+\"/modelsSeg_partnet/total_loss\"+str(epoch)+\"_\"+str(val_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzR20yMouPz",
        "outputId": "d8e6c81e-c752-436d-acee-6abe1f833af6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    10] loss: 1.581\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    20] loss: 1.355\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    30] loss: 1.254\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    40] loss: 1.225\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    50] loss: 1.220\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    60] loss: 1.155\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    70] loss: 1.079\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    80] loss: 1.123\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,    90] loss: 1.153\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   100] loss: 1.086\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   110] loss: 1.144\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   120] loss: 1.077\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   130] loss: 1.066\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   140] loss: 1.060\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   150] loss: 1.084\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   160] loss: 1.066\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   170] loss: 1.115\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   180] loss: 1.026\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   190] loss: 1.039\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   200] loss: 0.907\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   210] loss: 1.011\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   220] loss: 0.946\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   230] loss: 0.982\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "[1,   240] loss: 0.961\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n",
            "(4, 200, 200)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [1:05:51<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-add75b13e19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-c83aa352c045>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, epochs, save)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mmtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgt_mask_pl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgt_mask_pl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mmcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgt_mask_pl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mmval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (200) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "train(pointnet, train_loader, val_loader,  save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DB3aVUlh0eO"
      },
      "source": [
        "Pointnet  authors, in their implementation, did not use feature matrix regularization for semantic segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGQ9uOtMZoOi"
      },
      "source": [
        "## test : ** loss += feature_transform_regularizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NqbwpM7yZv8N"
      },
      "outputs": [],
      "source": [
        "pointnet = PointNetSeg()\n",
        "#path='/content/gdrive/MyDrive/modelsSeg_partnet/newLoss14_83.58901960784314'\n",
        "path='/content/gdrive/MyDrive/modelsSeg_partnet/total9_76.95529411764706'\n",
        "pointnet.load_state_dict(torch.load(path))\n",
        "#pointnet.load_state_dict(torch.load(root_dir+\"/modelsSeg_partnet/14_83.21156862745099\",map_location=torch.device('cpu')))\n",
        "\n",
        "pointnet.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XRxYfs7ZZwmW"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(val_loader))\n",
        "pred = pointnet(batch['image'].transpose(1,2))\n",
        "pred_np = np.array(torch.argmax(pred[0],1));\n",
        "acc = (pred_np==np.array(batch['category']))\n",
        "\n",
        "resulting_acc = np.sum(acc, axis=1) / 10000\n",
        "resulting_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bl-Pe2tNZ9Hc"
      },
      "outputs": [],
      "source": [
        "x,y,z=np.array(batch['image'][1]).T\n",
        "c = np.array(batch['category'][1]).T\n",
        "#c = np.array(pred_np[0]).T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, \n",
        "                                   mode='markers',\n",
        "                                   marker=dict(\n",
        "        size=30,\n",
        "        color=c,                # set color to an array/list of desired values\n",
        "        colorscale='Viridis',   # choose a colorscale\n",
        "        opacity=1.0\n",
        "    ))])\n",
        "fig.update_traces(marker=dict(size=2,\n",
        "                              line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YhebSr_jZ93r"
      },
      "outputs": [],
      "source": [
        "x,y,z=np.array(batch['image'][1]).T\n",
        "c = pred_np[1].T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, \n",
        "                                   mode='markers',\n",
        "                                   marker=dict(\n",
        "        size=30,\n",
        "        color=c,                # set color to an array/list of desired values\n",
        "        colorscale='Viridis',   # choose a colorscale\n",
        "        opacity=1\n",
        "    ))])\n",
        "fig.update_traces(marker=dict(size=2,\n",
        "                              line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tiPQAmARqnNY"
      },
      "outputs": [],
      "source": [
        "x,y,z=pts[0].T\n",
        "c = gt_mask_pl[0].T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, \n",
        "                                   mode='markers',\n",
        "                                   marker=dict(\n",
        "        size=30,\n",
        "        color=c,                # set color to an array/list of desired values\n",
        "        colorscale='Viridis',   # choose a colorscale\n",
        "        opacity=1\n",
        "    ))])\n",
        "fig.update_traces(marker=dict(size=2,\n",
        "                              line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeUZqen5GlKr"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eFmYGZ5bMAgm"
      },
      "outputs": [],
      "source": [
        "pointnet = PointNetSeg()\n",
        "path='/content/gdrive/MyDrive/modelsSeg_partnet/newLoss14_83.58901960784314'\n",
        "pointnet.load_state_dict(torch.load(path))\n",
        "#pointnet.load_state_dict(torch.load(root_dir+\"/modelsSeg_partnet/14_83.21156862745099\",map_location=torch.device('cpu')))\n",
        "\n",
        "pointnet.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xsk9nSDAI3ba"
      },
      "outputs": [],
      "source": [
        "#pointnet = PointNetSeg()\n",
        "#pointnet.load_state_dict(torch.load(root_dir+\"/modelsSeg_partnet/14_83.21156862745099\",map_location=torch.device('cpu')))\n",
        "#pointnet.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zoE5fRX8GnWR"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(val_loader))\n",
        "pred = pointnet(batch['image'].transpose(1,2))\n",
        "pred_np = np.array(torch.argmax(pred[0],1));\n",
        "pred_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ISvwR4RL_eT"
      },
      "outputs": [],
      "source": [
        "pred_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "60Mr2Bp7O9xu"
      },
      "outputs": [],
      "source": [
        "acc = (pred_np==np.array(batch['category']))\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jc1jLPj-PBo-"
      },
      "outputs": [],
      "source": [
        "resulting_acc = np.sum(acc, axis=1) / 10000\n",
        "resulting_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N9bgpbtnHC2E"
      },
      "outputs": [],
      "source": [
        "x,y,z=pts[0].T\n",
        "c = labels[0].T\n",
        "#c = np.array(pred_np[0]).T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, \n",
        "                                   mode='markers',\n",
        "                                   marker=dict(\n",
        "        size=30,\n",
        "        color=c,                # set color to an array/list of desired values\n",
        "        colorscale='Viridis',   # choose a colorscale\n",
        "        opacity=1.0\n",
        "    ))])\n",
        "fig.update_traces(marker=dict(size=2,\n",
        "                              line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s2DtKYpoO2yi"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "x,y,z=np.array(batch['image'][1]).T\n",
        "c = pred_np[1].T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, \n",
        "                                   mode='markers',\n",
        "                                   marker=dict(\n",
        "        size=30,\n",
        "        color=c,                # set color to an array/list of desired values\n",
        "        colorscale='Viridis',   # choose a colorscale\n",
        "        opacity=1\n",
        "    ))])\n",
        "fig.update_traces(marker=dict(size=2,\n",
        "                              line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()\n",
        "plotly.offline.plot(fig, filename='/content/chairSeg.html')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hmjl6Lm4tiUF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4KNH5UtbWWok"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}